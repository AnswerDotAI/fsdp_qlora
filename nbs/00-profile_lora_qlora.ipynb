{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d30779b0-0df2-445a-829d-fc3b243c462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "import transformers\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "from transformers.integrations.bitsandbytes import replace_with_bnb_linear\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "\n",
    "from peft.tuners.lora.config import LoraConfig\n",
    "from peft.mapping import get_peft_model\n",
    "from peft.utils.peft_types import *\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import inspect\n",
    "from accelerate.utils import set_seed\n",
    "from functools import partial\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ade2d0-6c49-4f79-8a66-70a4edf9f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"profile_snapshots/\")\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a001c3-4941-44dc-97b0-dd9f67c5148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bda461b-c894-4c8b-8d43-a3023a9570bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def malloc_in_gb():\n",
    "    return torch.cuda.memory_allocated()/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e63dde-9528-4315-88df-4c7bea0db6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size_config(model_size):\n",
    "    if model_size == \"DEBUG\":\n",
    "        model_size_config = dict(hidden_size=128,\n",
    "                                num_hidden_layers=2,\n",
    "                                num_attention_heads=2,\n",
    "                                num_key_value_heads=2,\n",
    "                                intermediate_size=256)\n",
    "    elif model_size == \"60M\":\n",
    "        model_size_config = dict(hidden_size=512,\n",
    "                                num_hidden_layers=4,\n",
    "                                num_attention_heads=4,\n",
    "                                num_key_value_heads=4,\n",
    "                                intermediate_size=1024)\n",
    "    elif model_size == \"120M\":\n",
    "        model_size_config = dict(hidden_size=768,\n",
    "                                num_hidden_layers=12,\n",
    "                                num_attention_heads=12,\n",
    "                                num_key_value_heads=12,\n",
    "                                intermediate_size=1536)\n",
    "    elif model_size == \"290M\":\n",
    "        model_size_config = dict(hidden_size=1024,\n",
    "                                num_hidden_layers=12,\n",
    "                                num_attention_heads=16,\n",
    "                                num_key_value_heads=16,\n",
    "                                intermediate_size=4096)\n",
    "    elif model_size == \"1B\":\n",
    "        model_size_config = dict(hidden_size=2048,\n",
    "                                num_hidden_layers=24,\n",
    "                                num_attention_heads=16,\n",
    "                                num_key_value_heads=16,\n",
    "                                intermediate_size=4096)\n",
    "    elif model_size == \"7B\":\n",
    "        model_size_config = {}\n",
    "    return model_size_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bae5ba6-f4cb-44a7-9191-89bab9e930f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_size=\"1B\"):\n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04c9743-43b7-451f-90d9-ff7a3201f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a3990ca-6cd7-47de-813c-442802520487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory allocated: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d994c610-4ae6-4cef-ab29-32439904a4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "# create dummy inputs\n",
    "model = create_model(\"DEBUG\")\n",
    "vocab_size = model.model.embed_tokens.weight.size(0)\n",
    "inputs = [torch.randint(0, vocab_size, (1, sl)) for sl in [512,1024,2048,3072]]\n",
    "print(f\"Memory allocated: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "431913f6-e03f-4e4c-b0a4-c8943934e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_model(create_model_func, inference=False, save_filename=\"mem_profile.pickle\"):\n",
    "\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#demonstrating-speedups\n",
    "\n",
    "    https://pytorch.org/docs/stable/torch_cuda_memory.html\n",
    "\n",
    "    https://medium.com/pytorch/how-activation-checkpointing-enables-scaling-up-training-deep-learning-models-7a93ae01ff2d\n",
    "\n",
    "    https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    torch.cuda.memory._record_memory_history()\n",
    "    for x in inputs:\n",
    "        print(f\"Input Size:{tuple(x.size())}\")\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start.record()\n",
    "        if inference:\n",
    "            with torch.no_grad():\n",
    "                model = create_model_func()\n",
    "                model.to(\"cuda\", torch.bfloat16);\n",
    "                print(f\"Memory allocated [MODEL]: {malloc_in_gb():.3f} GB\")\n",
    "                output = model(x.to(\"cuda\"))\n",
    "                print(f\"Memory allocated [FWD]: {malloc_in_gb():.3f} GB\")\n",
    "        else:\n",
    "            model = create_model_func()\n",
    "            model.to(\"cuda\", torch.bfloat16);\n",
    "            print(f\"Memory allocated [MODEL): {malloc_in_gb():.3f} GB\")\n",
    "            output = model(x.to(\"cuda\"))\n",
    "            print(f\"Memory allocated [FWD]: {malloc_in_gb():.3f} GB\")            \n",
    "            output.logits.mean().backward()\n",
    "            print(f\"Memory allocated [BWD]: {malloc_in_gb():.3f} GB\")\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        secs = start.elapsed_time(end) / 1000\n",
    "        print(f\"Elapsed time: {secs:.3f}\\n\\n\")\n",
    "        output, model = None, None\n",
    "        free_memory()\n",
    "    torch.cuda.memory._dump_snapshot(save_filename)\n",
    "    print(f\"Memory allocated [finish]: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee22e7e6-4838-4bbf-bf0a-a7e29dc4e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 0.018 GB\n",
      "Memory allocated [FWD]: 0.093 GB\n",
      "Elapsed time: 0.562\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 0.027 GB\n",
      "Memory allocated [FWD]: 0.160 GB\n",
      "Elapsed time: 0.111\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 0.027 GB\n",
      "Memory allocated [FWD]: 0.291 GB\n",
      "Elapsed time: 0.096\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 0.027 GB\n",
      "Memory allocated [FWD]: 0.425 GB\n",
      "Elapsed time: 0.104\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "# warmup\n",
    "profile_model(partial(create_model, \"DEBUG\"), inference=True, save_filename=save_dir/\"debug-inference.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247f004-cfcb-4735-9341-f2461cdc473a",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d23e911a-0e15-4113-b129-83d5a214250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 2.311 GB\n",
      "Memory allocated [FWD]: 2.478 GB\n",
      "Elapsed time: 12.858\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 2.311 GB\n",
      "Memory allocated [FWD]: 2.645 GB\n",
      "Elapsed time: 12.719\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 2.311 GB\n",
      "Memory allocated [FWD]: 2.976 GB\n",
      "Elapsed time: 12.735\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 2.311 GB\n",
      "Memory allocated [FWD]: 3.322 GB\n",
      "Elapsed time: 12.682\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_model, \"1B\"), inference=True, save_filename=save_dir/\"base-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f06773f8-2eba-4a6f-8785-41564a072232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 2.311 GB\n",
      "Memory allocated [FWD]: 3.605 GB\n",
      "Memory allocated [BWD]: 4.764 GB\n",
      "Elapsed time: 11.823\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.320 GB\n",
      "Memory allocated [FWD]: 4.907 GB\n",
      "Memory allocated [BWD]: 4.930 GB\n",
      "Elapsed time: 12.106\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.320 GB\n",
      "Memory allocated [FWD]: 7.493 GB\n",
      "Memory allocated [BWD]: 5.260 GB\n",
      "Elapsed time: 12.611\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.320 GB\n",
      "Memory allocated [FWD]: 10.093 GB\n",
      "Memory allocated [BWD]: 5.606 GB\n",
      "Elapsed time: 13.033\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "# (1, 4096) OOMs with a 16GB GPU\n",
    "profile_model(partial(create_model, \"1B\"), inference=False, save_filename=save_dir/\"base-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb26df-a649-43da-bd14-f095e9913ab4",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d24d2f1-25bb-432c-be9f-401bd0ff561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_model(model_size=\"1B\", gc_enabled=False):\n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    if gc_enabled: model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8660235c-52b1-4417-815b-72f4cf2e5cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 2.489 GB\n",
      "Elapsed time: 12.622\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 2.657 GB\n",
      "Elapsed time: 12.293\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 2.988 GB\n",
      "Elapsed time: 12.341\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 3.334 GB\n",
      "Elapsed time: 12.339\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\"), inference=True, save_filename=save_dir/\"lora-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96344a01-3ed4-44b3-891e-d6e6df0ed8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 3.451 GB\n",
      "Memory allocated [BWD]: 2.492 GB\n",
      "Elapsed time: 11.359\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 4.580 GB\n",
      "Memory allocated [BWD]: 2.660 GB\n",
      "Elapsed time: 11.946\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 6.835 GB\n",
      "Memory allocated [BWD]: 2.991 GB\n",
      "Elapsed time: 12.710\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 9.105 GB\n",
      "Memory allocated [BWD]: 3.337 GB\n",
      "Elapsed time: 13.298\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\"), inference=False, save_filename=save_dir/\"lora-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b479a83-c80a-4c4a-bf4f-74392671921b",
   "metadata": {},
   "source": [
    "### LORA + Gradient Ckpt.\n",
    "\n",
    "Using default HF grad ckpt strategy which wraps each individual decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce7f0c9c-480e-45e9-a637-6a4647dfb9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 2.315 GB\n",
      "Memory allocated [FWD]: 2.439 GB\n",
      "Memory allocated [BWD]: 2.392 GB\n",
      "Elapsed time: 11.923\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 2.573 GB\n",
      "Memory allocated [BWD]: 2.458 GB\n",
      "Elapsed time: 12.374\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 2.820 GB\n",
      "Memory allocated [BWD]: 2.588 GB\n",
      "Elapsed time: 12.543\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 3.082 GB\n",
      "Memory allocated [BWD]: 2.733 GB\n",
      "Elapsed time: 13.120\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\", gc_enabled=True), inference=False, save_filename=save_dir/\"lora-gc-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e01883-2551-4784-93a3-e9ca651feb36",
   "metadata": {},
   "source": [
    "### QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe1306d6-051a-42cb-a313-21c4849d70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_bnb_4bit_linear(\n",
    "    model,\n",
    "    modules_to_not_convert=None,\n",
    "    current_key_name=None,\n",
    "    quantization_config=None,\n",
    "    has_been_replaced=False,\n",
    "    quant_storage=torch.uint8, \n",
    "    keep_trainable=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Private method that wraps the recursion for module replacement.\n",
    "\n",
    "    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if current_key_name is None:\n",
    "            current_key_name = []\n",
    "        current_key_name.append(name)\n",
    "\n",
    "        if (isinstance(module, nn.Linear) or isinstance(module, Conv1D)) and name not in modules_to_not_convert:\n",
    "            # Check if the current key is not in the `modules_to_not_convert`\n",
    "            if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n",
    "                # with init_empty_weights():\n",
    "                if isinstance(module, Conv1D):\n",
    "                    in_features, out_features = module.weight.shape\n",
    "                else:\n",
    "                    in_features = module.in_features\n",
    "                    out_features = module.out_features\n",
    "\n",
    "                    model._modules[name] = bnb.nn.Linear4bit(\n",
    "                        in_features,\n",
    "                        out_features,\n",
    "                        module.bias is not None,\n",
    "                        quantization_config.bnb_4bit_compute_dtype,\n",
    "                        compress_statistics=quantization_config.bnb_4bit_use_double_quant,\n",
    "                        quant_type=quantization_config.bnb_4bit_quant_type,\n",
    "                        quant_storage=quant_storage\n",
    "                    )\n",
    "                    has_been_replaced = True\n",
    "                # Store the module class in case we need to transpose the weight later\n",
    "                model._modules[name].source_cls = type(module)\n",
    "                # Force requires grad to False to avoid unexpected errors\n",
    "                if keep_trainable: \n",
    "                    model._modules[name].requires_grad_(True)\n",
    "                else:\n",
    "                    model._modules[name].requires_grad_(True)\n",
    "        if len(list(module.children())) > 0:\n",
    "            _, has_been_replaced = replace_with_bnb_4bit_linear(\n",
    "                module,\n",
    "                modules_to_not_convert,\n",
    "                current_key_name,\n",
    "                quantization_config,\n",
    "                has_been_replaced=has_been_replaced,\n",
    "            )\n",
    "        # Remove the last key for recursion\n",
    "        current_key_name.pop(-1)\n",
    "    return model, has_been_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5f71873-662f-40e8-be83-cc2ef63cd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qlora_model(model_size=\"1B\", with_lora=True, gc_enabled=False, keep_trainable=False):\n",
    "    \n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "    \n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    qconfig = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                       bnb_4bit_quant_type=\"nf4\",\n",
    "                       bnb_4bit_use_double_quant=False,\n",
    "                       bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    model, has_been_replaced = replace_with_bnb_4bit_linear(model,\n",
    "                                                            modules_to_not_convert=[\"lm_head\"], \n",
    "                                                            quantization_config=qconfig, \n",
    "                                                            keep_trainable=keep_trainable, \n",
    "                                                            quant_storage=torch.bfloat16)\n",
    "    assert has_been_replaced\n",
    "    if with_lora:\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    if gc_enabled: model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56720604-ea7f-40d8-a6da-b4df0b7cecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 0.859 GB\n",
      "Memory allocated [FWD]: 1.034 GB\n",
      "Elapsed time: 19.783\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 0.868 GB\n",
      "Memory allocated [FWD]: 1.201 GB\n",
      "Elapsed time: 17.461\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 0.868 GB\n",
      "Memory allocated [FWD]: 1.532 GB\n",
      "Elapsed time: 17.779\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 0.868 GB\n",
      "Memory allocated [FWD]: 1.878 GB\n",
      "Elapsed time: 17.819\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_qlora_model, \"1B\"), inference=True, save_filename=save_dir/\"qlora-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59def356-5ac1-48cc-89ea-d13cbc71c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 0.868 GB\n",
      "Memory allocated [FWD]: 2.195 GB\n",
      "Memory allocated [BWD]: 1.295 GB\n",
      "Elapsed time: 17.303\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 3.532 GB\n",
      "Memory allocated [BWD]: 1.712 GB\n",
      "Elapsed time: 17.051\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 6.185 GB\n",
      "Memory allocated [BWD]: 2.542 GB\n",
      "Elapsed time: 17.963\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 8.853 GB\n",
      "Memory allocated [BWD]: 3.387 GB\n",
      "Elapsed time: 18.167\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_qlora_model, \"1B\"), inference=False, save_filename=save_dir/\"qlora-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4724c-2e12-4835-9dc7-631f09ba5b22",
   "metadata": {},
   "source": [
    "### QLORA + Gradient Ckpt.\n",
    "\n",
    "Using default HF grad ckpt strategy which wraps each individual decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b76f28a8-edc5-4788-8125-69f3b7b63462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 1.250 GB\n",
      "Memory allocated [BWD]: 1.194 GB\n",
      "Elapsed time: 17.265\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 1.625 GB\n",
      "Memory allocated [BWD]: 1.511 GB\n",
      "Elapsed time: 16.252\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 2.371 GB\n",
      "Memory allocated [BWD]: 2.140 GB\n",
      "Elapsed time: 17.468\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 3.133 GB\n",
      "Memory allocated [BWD]: 2.783 GB\n",
      "Elapsed time: 18.704\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_qlora_model, \"1B\", gc_enabled=True), inference=False, save_filename=save_dir/\"qlora-gc-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc3bca-c43e-4d37-a764-5431e7b0e4c4",
   "metadata": {},
   "source": [
    "### NF4 Training?\n",
    "\n",
    "No gradients for Params4bit, even if there were we might need something like dequantize -> step -> quantize during step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "942508d2-b691-4362-b342-1074266b0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainable_params(model):\n",
    "    params_grad = []\n",
    "    params_no_grad = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            params_grad.append(n)\n",
    "        else:\n",
    "            params_no_grad.append(n)\n",
    "    return params_grad, params_no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ffe6921-38c9-4a40-80c0-6501bf48397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_qlora_model(\"DEBUG\", with_lora=False, keep_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "97351271-2006-4338-8fd8-f952057abeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['model.embed_tokens.weight',\n",
       "  'model.layers.0.self_attn.q_proj.weight',\n",
       "  'model.layers.0.self_attn.k_proj.weight',\n",
       "  'model.layers.0.self_attn.v_proj.weight',\n",
       "  'model.layers.0.self_attn.o_proj.weight',\n",
       "  'model.layers.0.mlp.gate_proj.weight',\n",
       "  'model.layers.0.mlp.up_proj.weight',\n",
       "  'model.layers.0.mlp.down_proj.weight',\n",
       "  'model.layers.0.input_layernorm.weight',\n",
       "  'model.layers.0.post_attention_layernorm.weight',\n",
       "  'model.layers.1.self_attn.q_proj.weight',\n",
       "  'model.layers.1.self_attn.k_proj.weight',\n",
       "  'model.layers.1.self_attn.v_proj.weight',\n",
       "  'model.layers.1.self_attn.o_proj.weight',\n",
       "  'model.layers.1.mlp.gate_proj.weight',\n",
       "  'model.layers.1.mlp.up_proj.weight',\n",
       "  'model.layers.1.mlp.down_proj.weight',\n",
       "  'model.layers.1.input_layernorm.weight',\n",
       "  'model.layers.1.post_attention_layernorm.weight',\n",
       "  'model.norm.weight',\n",
       "  'lm_head.weight'],\n",
       " [])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aa3e46f7-289b-4bb9-b521-905a080c559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda', torch.bfloat16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5069618f-3825-41f4-b32c-73fb26900807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = model((inputs[0]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b78152f7-29b6-4628-b7ad-47c7d1650bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.sum().backward();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6f6fdb2b-1126-4d84-8b14-9e8a0a36ee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_submodule('model.embed_tokens').weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7fa9cb15-78fc-417c-b9cf-ab8b83a3ac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[ 1.0012e-08],\n",
       "            [ 1.9399e+08],\n",
       "            [ 2.0491e-17],\n",
       "            ...,\n",
       "            [ 4.3539e-08],\n",
       "            [ 3.8794e-19],\n",
       "            [-9.6000e+01]], device='cuda:0', dtype=torch.bfloat16,\n",
       "           requires_grad=True))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_submodule('model.layers.0.self_attn.q_proj').weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ca63d610-b009-4bc4-9cc3-f52ce39e09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_submodule('model.layers.0.self_attn.q_proj').weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6aa659-54a3-4622-981e-e8c61e71ce0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27f8e9-5cd7-408e-bf89-d50ecf0ff806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a7dc7-e933-42c9-b341-2eed67868fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b8321-d994-4ce7-be6a-ca032d865b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60b942-9d0a-46fb-9666-150463323d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821da14b-65d8-44ee-875a-e31321e462e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
