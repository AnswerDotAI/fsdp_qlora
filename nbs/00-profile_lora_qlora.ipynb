{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30779b0-0df2-445a-829d-fc3b243c462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "import transformers\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "from transformers.integrations.bitsandbytes import replace_with_bnb_linear\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "\n",
    "from peft.tuners.lora.config import LoraConfig\n",
    "from peft.mapping import get_peft_model\n",
    "from peft.utils.peft_types import *\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import inspect\n",
    "from accelerate.utils import set_seed\n",
    "from functools import partial\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ade2d0-6c49-4f79-8a66-70a4edf9f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"profile_snapshots/\")\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a001c3-4941-44dc-97b0-dd9f67c5148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda461b-c894-4c8b-8d43-a3023a9570bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def malloc_in_gb():\n",
    "    return torch.cuda.memory_allocated()/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e63dde-9528-4315-88df-4c7bea0db6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size_config(model_size):\n",
    "    if model_size == \"DEBUG\":\n",
    "        model_size_config = dict(hidden_size=128,\n",
    "                                num_hidden_layers=2,\n",
    "                                num_attention_heads=2,\n",
    "                                num_key_value_heads=2,\n",
    "                                intermediate_size=256)\n",
    "    elif model_size == \"60M\":\n",
    "        model_size_config = dict(hidden_size=512,\n",
    "                                num_hidden_layers=4,\n",
    "                                num_attention_heads=4,\n",
    "                                num_key_value_heads=4,\n",
    "                                intermediate_size=1024)\n",
    "    elif model_size == \"120M\":\n",
    "        model_size_config = dict(hidden_size=768,\n",
    "                                num_hidden_layers=12,\n",
    "                                num_attention_heads=12,\n",
    "                                num_key_value_heads=12,\n",
    "                                intermediate_size=1536)\n",
    "    elif model_size == \"290M\":\n",
    "        model_size_config = dict(hidden_size=1024,\n",
    "                                num_hidden_layers=12,\n",
    "                                num_attention_heads=16,\n",
    "                                num_key_value_heads=16,\n",
    "                                intermediate_size=4096)\n",
    "    elif model_size == \"1B\":\n",
    "        model_size_config = dict(hidden_size=2048,\n",
    "                                num_hidden_layers=24,\n",
    "                                num_attention_heads=16,\n",
    "                                num_key_value_heads=16,\n",
    "                                intermediate_size=4096)\n",
    "    elif model_size == \"7B\":\n",
    "        model_size_config = {}\n",
    "    return model_size_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae5ba6-f4cb-44a7-9191-89bab9e930f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_size=\"1B\"):\n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c9743-43b7-451f-90d9-ff7a3201f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3990ca-6cd7-47de-813c-442802520487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory allocated: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994c610-4ae6-4cef-ab29-32439904a4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "# create dummy inputs\n",
    "model = create_model(\"DEBUG\")\n",
    "vocab_size = model.model.embed_tokens.weight.size(0)\n",
    "inputs = [torch.randint(0, vocab_size, (1, sl)) for sl in [512,1024,2048,3072]]\n",
    "print(f\"Memory allocated: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431913f6-e03f-4e4c-b0a4-c8943934e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_model(create_model_func, inference=False, save_filename=\"mem_profile.pickle\"):\n",
    "\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#demonstrating-speedups\n",
    "\n",
    "    https://pytorch.org/docs/stable/torch_cuda_memory.html\n",
    "\n",
    "    https://medium.com/pytorch/how-activation-checkpointing-enables-scaling-up-training-deep-learning-models-7a93ae01ff2d\n",
    "\n",
    "    https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    torch.cuda.memory._record_memory_history()\n",
    "    for x in inputs:\n",
    "        print(f\"Input Size:{tuple(x.size())}\")\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start.record()\n",
    "        if inference:\n",
    "            with torch.no_grad():\n",
    "                model = create_model_func()\n",
    "                model.to(\"cuda\", torch.bfloat16);\n",
    "                print(f\"Memory allocated [MODEL]: {malloc_in_gb():.3f} GB\")\n",
    "                output = model(x.to(\"cuda\"))\n",
    "                print(f\"Memory allocated [FWD]: {malloc_in_gb():.3f} GB\")\n",
    "        else:\n",
    "            model = create_model_func()\n",
    "            model.to(\"cuda\", torch.bfloat16);\n",
    "            print(f\"Memory allocated [MODEL): {malloc_in_gb():.3f} GB\")\n",
    "            output = model(x.to(\"cuda\"))\n",
    "            print(f\"Memory allocated [FWD]: {malloc_in_gb():.3f} GB\")            \n",
    "            output.logits.mean().backward()\n",
    "            print(f\"Memory allocated [BWD]: {malloc_in_gb():.3f} GB\")\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        secs = start.elapsed_time(end) / 1000\n",
    "        print(f\"Elapsed time: {secs:.3f}\\n\\n\")\n",
    "        output, model = None, None\n",
    "        free_memory()\n",
    "    torch.cuda.memory._dump_snapshot(save_filename)\n",
    "    print(f\"Memory allocated [finish]: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22e7e6-4838-4bbf-bf0a-a7e29dc4e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 0.018 GB\n",
      "Memory allocated [FWD]: 0.093 GB\n",
      "Elapsed time: 0.562\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 0.027 GB\n",
      "Memory allocated [FWD]: 0.160 GB\n",
      "Elapsed time: 0.111\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 0.027 GB\n",
      "Memory allocated [FWD]: 0.291 GB\n",
      "Elapsed time: 0.096\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 0.027 GB\n",
      "Memory allocated [FWD]: 0.425 GB\n",
      "Elapsed time: 0.104\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "# warmup\n",
    "profile_model(partial(create_model, \"DEBUG\"), inference=True, save_filename=save_dir/\"debug-inference.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247f004-cfcb-4735-9341-f2461cdc473a",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e911a-0e15-4113-b129-83d5a214250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 2.311 GB\n",
      "Memory allocated [FWD]: 2.478 GB\n",
      "Elapsed time: 12.858\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 2.311 GB\n",
      "Memory allocated [FWD]: 2.645 GB\n",
      "Elapsed time: 12.719\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 2.311 GB\n",
      "Memory allocated [FWD]: 2.976 GB\n",
      "Elapsed time: 12.735\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 2.311 GB\n",
      "Memory allocated [FWD]: 3.322 GB\n",
      "Elapsed time: 12.682\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_model, \"1B\"), inference=True, save_filename=save_dir/\"base-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06773f8-2eba-4a6f-8785-41564a072232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 2.311 GB\n",
      "Memory allocated [FWD]: 3.605 GB\n",
      "Memory allocated [BWD]: 4.764 GB\n",
      "Elapsed time: 11.823\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.320 GB\n",
      "Memory allocated [FWD]: 4.907 GB\n",
      "Memory allocated [BWD]: 4.930 GB\n",
      "Elapsed time: 12.106\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.320 GB\n",
      "Memory allocated [FWD]: 7.493 GB\n",
      "Memory allocated [BWD]: 5.260 GB\n",
      "Elapsed time: 12.611\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.320 GB\n",
      "Memory allocated [FWD]: 10.093 GB\n",
      "Memory allocated [BWD]: 5.606 GB\n",
      "Elapsed time: 13.033\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "# (1, 4096) OOMs with a 16GB GPU\n",
    "profile_model(partial(create_model, \"1B\"), inference=False, save_filename=save_dir/\"base-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb26df-a649-43da-bd14-f095e9913ab4",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24d2f1-25bb-432c-be9f-401bd0ff561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_model(model_size=\"1B\", gc_enabled=False):\n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    if gc_enabled: model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660235c-52b1-4417-815b-72f4cf2e5cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 2.489 GB\n",
      "Elapsed time: 12.622\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 2.657 GB\n",
      "Elapsed time: 12.293\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 2.988 GB\n",
      "Elapsed time: 12.341\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 3.334 GB\n",
      "Elapsed time: 12.339\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\"), inference=True, save_filename=save_dir/\"lora-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96344a01-3ed4-44b3-891e-d6e6df0ed8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 3.451 GB\n",
      "Memory allocated [BWD]: 2.492 GB\n",
      "Elapsed time: 11.359\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 4.580 GB\n",
      "Memory allocated [BWD]: 2.660 GB\n",
      "Elapsed time: 11.946\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 6.835 GB\n",
      "Memory allocated [BWD]: 2.991 GB\n",
      "Elapsed time: 12.710\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 9.105 GB\n",
      "Memory allocated [BWD]: 3.337 GB\n",
      "Elapsed time: 13.298\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\"), inference=False, save_filename=save_dir/\"lora-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b479a83-c80a-4c4a-bf4f-74392671921b",
   "metadata": {},
   "source": [
    "### LORA + Gradient Ckpt.\n",
    "\n",
    "Using default HF grad ckpt strategy which wraps each individual decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f0c9c-480e-45e9-a637-6a4647dfb9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 2.315 GB\n",
      "Memory allocated [FWD]: 2.439 GB\n",
      "Memory allocated [BWD]: 2.392 GB\n",
      "Elapsed time: 11.923\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 2.573 GB\n",
      "Memory allocated [BWD]: 2.458 GB\n",
      "Elapsed time: 12.374\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 2.820 GB\n",
      "Memory allocated [BWD]: 2.588 GB\n",
      "Elapsed time: 12.543\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 3.082 GB\n",
      "Memory allocated [BWD]: 2.733 GB\n",
      "Elapsed time: 13.120\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\", gc_enabled=True), inference=False, save_filename=save_dir/\"lora-gc-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e01883-2551-4784-93a3-e9ca651feb36",
   "metadata": {},
   "source": [
    "### QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1306d6-051a-42cb-a313-21c4849d70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_bnb_4bit_linear(\n",
    "    model,\n",
    "    modules_to_not_convert=None,\n",
    "    current_key_name=None,\n",
    "    quantization_config=None,\n",
    "    has_been_replaced=False,\n",
    "    quant_storage=torch.uint8, \n",
    "    keep_trainable=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Private method that wraps the recursion for module replacement.\n",
    "\n",
    "    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if current_key_name is None:\n",
    "            current_key_name = []\n",
    "        current_key_name.append(name)\n",
    "\n",
    "        if (isinstance(module, nn.Linear) or isinstance(module, Conv1D)) and name not in modules_to_not_convert:\n",
    "            # Check if the current key is not in the `modules_to_not_convert`\n",
    "            if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n",
    "                # with init_empty_weights():\n",
    "                if isinstance(module, Conv1D):\n",
    "                    in_features, out_features = module.weight.shape\n",
    "                else:\n",
    "                    in_features = module.in_features\n",
    "                    out_features = module.out_features\n",
    "\n",
    "                    model._modules[name] = bnb.nn.Linear4bit(\n",
    "                        in_features,\n",
    "                        out_features,\n",
    "                        module.bias is not None,\n",
    "                        quantization_config.bnb_4bit_compute_dtype,\n",
    "                        compress_statistics=quantization_config.bnb_4bit_use_double_quant,\n",
    "                        quant_type=quantization_config.bnb_4bit_quant_type,\n",
    "                        quant_storage=quant_storage\n",
    "                    )\n",
    "                    has_been_replaced = True\n",
    "                # Store the module class in case we need to transpose the weight later\n",
    "                model._modules[name].source_cls = type(module)\n",
    "                # Force requires grad to False to avoid unexpected errors\n",
    "                if keep_trainable: \n",
    "                    model._modules[name].requires_grad_(True)\n",
    "                else:\n",
    "                    model._modules[name].requires_grad_(True)\n",
    "        if len(list(module.children())) > 0:\n",
    "            _, has_been_replaced = replace_with_bnb_4bit_linear(\n",
    "                module,\n",
    "                modules_to_not_convert,\n",
    "                current_key_name,\n",
    "                quantization_config,\n",
    "                has_been_replaced=has_been_replaced,\n",
    "            )\n",
    "        # Remove the last key for recursion\n",
    "        current_key_name.pop(-1)\n",
    "    return model, has_been_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f71873-662f-40e8-be83-cc2ef63cd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qlora_model(model_size=\"1B\", with_lora=True, gc_enabled=False, keep_trainable=False):\n",
    "    \n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "    \n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    qconfig = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                       bnb_4bit_quant_type=\"nf4\",\n",
    "                       bnb_4bit_use_double_quant=False,\n",
    "                       bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    model, has_been_replaced = replace_with_bnb_4bit_linear(model,\n",
    "                                                            modules_to_not_convert=[\"lm_head\"], \n",
    "                                                            quantization_config=qconfig, \n",
    "                                                            keep_trainable=keep_trainable, \n",
    "                                                            quant_storage=torch.bfloat16)\n",
    "    assert has_been_replaced\n",
    "    if with_lora:\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    if gc_enabled: model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56720604-ea7f-40d8-a6da-b4df0b7cecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 0.859 GB\n",
      "Memory allocated [FWD]: 1.034 GB\n",
      "Elapsed time: 19.783\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 0.868 GB\n",
      "Memory allocated [FWD]: 1.201 GB\n",
      "Elapsed time: 17.461\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 0.868 GB\n",
      "Memory allocated [FWD]: 1.532 GB\n",
      "Elapsed time: 17.779\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 0.868 GB\n",
      "Memory allocated [FWD]: 1.878 GB\n",
      "Elapsed time: 17.819\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_qlora_model, \"1B\"), inference=True, save_filename=save_dir/\"qlora-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59def356-5ac1-48cc-89ea-d13cbc71c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 0.868 GB\n",
      "Memory allocated [FWD]: 2.195 GB\n",
      "Memory allocated [BWD]: 1.295 GB\n",
      "Elapsed time: 17.303\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 3.532 GB\n",
      "Memory allocated [BWD]: 1.712 GB\n",
      "Elapsed time: 17.051\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 6.185 GB\n",
      "Memory allocated [BWD]: 2.542 GB\n",
      "Elapsed time: 17.963\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 8.853 GB\n",
      "Memory allocated [BWD]: 3.387 GB\n",
      "Elapsed time: 18.167\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_qlora_model, \"1B\"), inference=False, save_filename=save_dir/\"qlora-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4724c-2e12-4835-9dc7-631f09ba5b22",
   "metadata": {},
   "source": [
    "### QLORA + Gradient Ckpt.\n",
    "\n",
    "Using default HF grad ckpt strategy which wraps each individual decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f28a8-edc5-4788-8125-69f3b7b63462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 1.250 GB\n",
      "Memory allocated [BWD]: 1.194 GB\n",
      "Elapsed time: 17.265\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 1.625 GB\n",
      "Memory allocated [BWD]: 1.511 GB\n",
      "Elapsed time: 16.252\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 2.371 GB\n",
      "Memory allocated [BWD]: 2.140 GB\n",
      "Elapsed time: 17.468\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 0.876 GB\n",
      "Memory allocated [FWD]: 3.133 GB\n",
      "Memory allocated [BWD]: 2.783 GB\n",
      "Elapsed time: 18.704\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_qlora_model, \"1B\", gc_enabled=True), inference=False, save_filename=save_dir/\"qlora-gc-training.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6aa659-54a3-4622-981e-e8c61e71ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.882 GB\n",
      "Memory allocated [FWD]: 1.260 GB\n",
      "Memory allocated [BWD]: 1.210 GB\n",
      "Max MemAlloc: 1.360229376\n",
      "Elapsed time: 0.195\n",
      "\n",
      "\n",
      "Memory allocated [MODEL): 0.891 GB\n",
      "Memory allocated [FWD]: 1.646 GB\n",
      "Memory allocated [BWD]: 1.532 GB\n",
      "Max MemAlloc: 1.844102144\n",
      "Elapsed time: 0.194\n",
      "\n",
      "\n",
      "Memory allocated [MODEL): 0.891 GB\n",
      "Memory allocated [FWD]: 2.397 GB\n",
      "Memory allocated [BWD]: 2.174 GB\n",
      "Max MemAlloc: 2.791502848\n",
      "Elapsed time: 0.231\n",
      "\n",
      "\n",
      "Memory allocated [MODEL): 0.891 GB\n",
      "Memory allocated [FWD]: 3.142 GB\n",
      "Memory allocated [BWD]: 2.784 GB\n",
      "Max MemAlloc: 3.733136384\n",
      "Elapsed time: 0.417\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in inputs:\n",
    "    set_seed(42)\n",
    "    model = create_qlora_model(\"1B\", gc_enabled=True)\n",
    "    model.to(\"cuda\", torch.bfloat16);\n",
    "    with torch.no_grad():\n",
    "        model(inputs[0].to(\"cuda\"))\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"Memory allocated [MODEL): {malloc_in_gb():.3f} GB\")\n",
    "    output = model(x.to(\"cuda\"))\n",
    "    print(f\"Memory allocated [FWD]: {malloc_in_gb():.3f} GB\")            \n",
    "    output.logits.mean().backward()\n",
    "    print(f\"Memory allocated [BWD]: {malloc_in_gb():.3f} GB\")\n",
    "    max_memory = torch.cuda.memory.max_memory_allocated()/1e9\n",
    "    print(f\"Max MemAlloc: {max_memory}\")\n",
    "    \n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    secs = start.elapsed_time(end) / 1000\n",
    "    print(f\"Elapsed time: {secs:.3f}\\n\\n\")\n",
    "\n",
    "    output, model = None, None\n",
    "    free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27f8e9-5cd7-408e-bf89-d50ecf0ff806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a7dc7-e933-42c9-b341-2eed67868fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b8321-d994-4ce7-be6a-ca032d865b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60b942-9d0a-46fb-9666-150463323d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821da14b-65d8-44ee-875a-e31321e462e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
