{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0290220-376a-443f-acf6-a1de15ecac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab3a0e-a61c-4853-bffb-401fe5d826da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.peft import PeftUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74506326-bf1d-41ac-b97b-bd85ce876683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "import transformers\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "from transformers.integrations.bitsandbytes import replace_with_bnb_linear\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "\n",
    "from peft.tuners.lora.config import LoraConfig\n",
    "from peft.mapping import get_peft_model\n",
    "from peft.utils.peft_types import *\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import inspect\n",
    "from accelerate.utils import set_seed\n",
    "from functools import partial\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ade2d0-6c49-4f79-8a66-70a4edf9f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"profile_snapshots/\")\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a001c3-4941-44dc-97b0-dd9f67c5148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda461b-c894-4c8b-8d43-a3023a9570bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def malloc_in_gb():\n",
    "    return torch.cuda.memory_allocated()/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e63dde-9528-4315-88df-4c7bea0db6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size_config(model_size):\n",
    "    if model_size == \"DEBUG\":\n",
    "        model_size_config = dict(hidden_size=128,\n",
    "                                num_hidden_layers=2,\n",
    "                                num_attention_heads=2,\n",
    "                                num_key_value_heads=2,\n",
    "                                intermediate_size=256)\n",
    "    elif model_size == \"60M\":\n",
    "        model_size_config = dict(hidden_size=512,\n",
    "                                num_hidden_layers=4,\n",
    "                                num_attention_heads=4,\n",
    "                                num_key_value_heads=4,\n",
    "                                intermediate_size=1024)\n",
    "    elif model_size == \"120M\":\n",
    "        model_size_config = dict(hidden_size=768,\n",
    "                                num_hidden_layers=12,\n",
    "                                num_attention_heads=12,\n",
    "                                num_key_value_heads=12,\n",
    "                                intermediate_size=1536)\n",
    "    elif model_size == \"290M\":\n",
    "        model_size_config = dict(hidden_size=1024,\n",
    "                                num_hidden_layers=12,\n",
    "                                num_attention_heads=16,\n",
    "                                num_key_value_heads=16,\n",
    "                                intermediate_size=4096)\n",
    "    elif model_size == \"1B\":\n",
    "        model_size_config = dict(hidden_size=2048,\n",
    "                                num_hidden_layers=24,\n",
    "                                num_attention_heads=16,\n",
    "                                num_key_value_heads=16,\n",
    "                                intermediate_size=4096)\n",
    "    elif model_size == \"7B\":\n",
    "        model_size_config = {}\n",
    "    return model_size_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae5ba6-f4cb-44a7-9191-89bab9e930f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_size=\"1B\"):\n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c9743-43b7-451f-90d9-ff7a3201f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3990ca-6cd7-47de-813c-442802520487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory allocated: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994c610-4ae6-4cef-ab29-32439904a4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "# create dummy inputs\n",
    "model = create_model(\"DEBUG\")\n",
    "vocab_size = model.model.embed_tokens.weight.size(0)\n",
    "inputs = [torch.randint(0, vocab_size, (1, sl)) for sl in [512,1024,2048,3072]]\n",
    "print(f\"Memory allocated: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431913f6-e03f-4e4c-b0a4-c8943934e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_model(create_model_func, inference=False, save_filename=\"mem_profile.pickle\"):\n",
    "\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#demonstrating-speedups\n",
    "\n",
    "    https://pytorch.org/docs/stable/torch_cuda_memory.html\n",
    "\n",
    "    https://medium.com/pytorch/how-activation-checkpointing-enables-scaling-up-training-deep-learning-models-7a93ae01ff2d\n",
    "\n",
    "    https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    torch.cuda.memory._record_memory_history()\n",
    "    for x in inputs:\n",
    "        print(f\"Input Size:{tuple(x.size())}\")\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start.record()\n",
    "        if inference:\n",
    "            with torch.no_grad():\n",
    "                model = create_model_func()\n",
    "                model.to(\"cuda\", torch.bfloat16);\n",
    "                print(f\"Memory allocated [MODEL]: {malloc_in_gb():.3f} GB\")\n",
    "                output = model(x.to(\"cuda\"))\n",
    "                print(f\"Memory allocated [FWD]: {malloc_in_gb():.3f} GB\")\n",
    "        else:\n",
    "            model = create_model_func()\n",
    "            model.to(\"cuda\", torch.bfloat16);\n",
    "            print(f\"Memory allocated [MODEL): {malloc_in_gb():.3f} GB\")\n",
    "            output = model(x.to(\"cuda\"))\n",
    "            print(f\"Memory allocated [FWD]: {malloc_in_gb():.3f} GB\")            \n",
    "            output.logits.mean().backward()\n",
    "            print(f\"Memory allocated [BWD]: {malloc_in_gb():.3f} GB\")\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        secs = start.elapsed_time(end) / 1000\n",
    "        print(f\"Elapsed time: {secs:.3f}\\n\\n\")\n",
    "        output, model = None, None\n",
    "        free_memory()\n",
    "    torch.cuda.memory._dump_snapshot(save_filename)\n",
    "    print(f\"Memory allocated [finish]: {malloc_in_gb():.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22e7e6-4838-4bbf-bf0a-a7e29dc4e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 0.051 GB\n",
      "Memory allocated [FWD]: 0.125 GB\n",
      "Elapsed time: 1.338\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 0.059 GB\n",
      "Memory allocated [FWD]: 0.193 GB\n",
      "Elapsed time: 0.142\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 0.059 GB\n",
      "Memory allocated [FWD]: 0.324 GB\n",
      "Elapsed time: 0.135\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 0.059 GB\n",
      "Memory allocated [FWD]: 0.425 GB\n",
      "Elapsed time: 0.201\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "# warmup\n",
    "profile_model(partial(create_model, \"DEBUG\"), inference=True, save_filename=save_dir/\"debug-inference.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247f004-cfcb-4735-9341-f2461cdc473a",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e911a-0e15-4113-b129-83d5a214250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 2.320 GB\n",
      "Memory allocated [FWD]: 2.492 GB\n",
      "Elapsed time: 15.401\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 2.320 GB\n",
      "Memory allocated [FWD]: 2.666 GB\n",
      "Elapsed time: 14.997\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 2.320 GB\n",
      "Memory allocated [FWD]: 3.010 GB\n",
      "Elapsed time: 14.370\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 2.320 GB\n",
      "Memory allocated [FWD]: 3.322 GB\n",
      "Elapsed time: 14.218\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_model, \"1B\"), inference=True, save_filename=save_dir/\"base-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06773f8-2eba-4a6f-8785-41564a072232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 2.320 GB\n",
      "Memory allocated [FWD]: 3.521 GB\n",
      "Memory allocated [BWD]: 4.779 GB\n",
      "Elapsed time: 13.765\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.328 GB\n",
      "Memory allocated [FWD]: 4.757 GB\n",
      "Memory allocated [BWD]: 4.952 GB\n",
      "Elapsed time: 13.277\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.328 GB\n",
      "Memory allocated [FWD]: 7.283 GB\n",
      "Memory allocated [BWD]: 5.294 GB\n",
      "Elapsed time: 13.706\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.328 GB\n",
      "Memory allocated [FWD]: 9.879 GB\n",
      "Memory allocated [BWD]: 5.606 GB\n",
      "Elapsed time: 14.203\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "# (1, 4096) OOMs with a 16GB GPU\n",
    "profile_model(partial(create_model, \"1B\"), inference=False, save_filename=save_dir/\"base-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb26df-a649-43da-bd14-f095e9913ab4",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24d2f1-25bb-432c-be9f-401bd0ff561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_model(model_size=\"1B\", gc_enabled=False):\n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    if gc_enabled: model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660235c-52b1-4417-815b-72f4cf2e5cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL]: 2.314 GB\n",
      "Memory allocated [FWD]: 2.495 GB\n",
      "Elapsed time: 17.398\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 2.669 GB\n",
      "Elapsed time: 15.746\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 3.013 GB\n",
      "Elapsed time: 15.481\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL]: 2.323 GB\n",
      "Memory allocated [FWD]: 3.325 GB\n",
      "Elapsed time: 15.432\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\"), inference=True, save_filename=save_dir/\"lora-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96344a01-3ed4-44b3-891e-d6e6df0ed8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n",
      "Memory allocated [MODEL): 2.323 GB\n",
      "Memory allocated [FWD]: 3.363 GB\n",
      "Memory allocated [BWD]: 2.507 GB\n",
      "Elapsed time: 16.125\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.331 GB\n",
      "Memory allocated [FWD]: 4.437 GB\n",
      "Memory allocated [BWD]: 2.681 GB\n",
      "Elapsed time: 15.417\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.331 GB\n",
      "Memory allocated [FWD]: 6.642 GB\n",
      "Memory allocated [BWD]: 3.025 GB\n",
      "Elapsed time: 15.374\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.331 GB\n",
      "Memory allocated [FWD]: 8.916 GB\n",
      "Memory allocated [BWD]: 3.337 GB\n",
      "Elapsed time: 15.821\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\"), inference=False, save_filename=save_dir/\"lora-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b479a83-c80a-4c4a-bf4f-74392671921b",
   "metadata": {},
   "source": [
    "### LoRA + Gradient Ckpt.\n",
    "\n",
    "Using default HF grad ckpt strategy which wraps each individual decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f0c9c-480e-45e9-a637-6a4647dfb9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 2.331 GB\n",
      "Memory allocated [FWD]: 2.466 GB\n",
      "Memory allocated [BWD]: 2.406 GB\n",
      "Elapsed time: 15.596\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n",
      "Memory allocated [MODEL): 2.331 GB\n",
      "Memory allocated [FWD]: 2.594 GB\n",
      "Memory allocated [BWD]: 2.479 GB\n",
      "Elapsed time: 14.345\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n",
      "Memory allocated [MODEL): 2.331 GB\n",
      "Memory allocated [FWD]: 2.845 GB\n",
      "Memory allocated [BWD]: 2.622 GB\n",
      "Elapsed time: 14.974\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n",
      "Memory allocated [MODEL): 2.331 GB\n",
      "Memory allocated [FWD]: 3.091 GB\n",
      "Memory allocated [BWD]: 2.733 GB\n",
      "Elapsed time: 15.887\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_lora_model, \"1B\", gc_enabled=True), inference=False, save_filename=save_dir/\"lora-gc-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e01883-2551-4784-93a3-e9ca651feb36",
   "metadata": {},
   "source": [
    "### HQQ LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1924d-624f-4065-a7fb-6e214d27041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import BaseQuantizeConfig, HQQLinear, HQQBackend\n",
    "from hqq.models.hf.llama import LlamaHQQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1306d6-051a-42cb-a313-21c4849d70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_hqq_linear(\n",
    "    model,\n",
    "    modules_to_not_convert=None,\n",
    "    current_key_name=None,\n",
    "    quantization_config=None,\n",
    "    has_been_replaced=False,\n",
    "    quant_storage=torch.uint8, \n",
    "    compute_dtype=torch.bfloat16,\n",
    "    keep_trainable=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Private method that wraps the recursion for module replacement.\n",
    "\n",
    "    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n",
    "    \"\"\"    \n",
    "    for name, module in model.named_children():\n",
    "        if current_key_name is None:\n",
    "            current_key_name = []\n",
    "        current_key_name.append(name)\n",
    "\n",
    "        if (isinstance(module, nn.Linear) or isinstance(module, Conv1D)) and name not in modules_to_not_convert:\n",
    "            # Check if the current key is not in the `modules_to_not_convert`\n",
    "            if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n",
    "                # with init_empty_weights():\n",
    "                model._modules[name] = HQQLinear(module, \n",
    "                                                 quantization_config, \n",
    "                                                 del_orig=True,\n",
    "                                                 compute_dtype=compute_dtype, \n",
    "                                                 device_n=torch.cuda.current_device())\n",
    "                has_been_replaced = True\n",
    "                # Store the module class in case we need to transpose the weight later\n",
    "                model._modules[name].source_cls = type(module)\n",
    "                # Force requires grad to False to avoid unexpected errors\n",
    "                if keep_trainable: \n",
    "                    model._modules[name].requires_grad_(True)\n",
    "                else:\n",
    "                    model._modules[name].requires_grad_(False)\n",
    "        if len(list(module.children())) > 0:\n",
    "            _, has_been_replaced = replace_with_hqq_linear(\n",
    "                module,\n",
    "                modules_to_not_convert,\n",
    "                current_key_name,\n",
    "                quantization_config,\n",
    "                has_been_replaced=has_been_replaced\n",
    "            )\n",
    "        # Remove the last key for recursion\n",
    "        current_key_name.pop(-1)\n",
    "    return model, has_been_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f71873-662f-40e8-be83-cc2ef63cd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qlora_model(model_size=\"1B\", with_lora=True, gc_enabled=False, keep_trainable=False, backend=HQQBackend.ATEN):\n",
    "    \n",
    "    model_size_config = get_model_size_config(model_size)\n",
    "\n",
    "    # download model weights and config files.\n",
    "    config = LlamaConfig()\n",
    "    config.update(model_size_config)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    \n",
    "    quant_config = BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=True, quant_scale=False)\n",
    "    model, has_been_replaced = replace_with_hqq_linear(model,\n",
    "                                                        modules_to_not_convert=[\"lm_head\"], \n",
    "                                                        quantization_config=quant_config, \n",
    "                                                        keep_trainable=keep_trainable, \n",
    "                                                        quant_storage=torch.bfloat16,\n",
    "                                                        compute_dtype=torch.bfloat16)\n",
    "    \n",
    "    assert has_been_replaced\n",
    "    if with_lora:\n",
    "        base_lora_params = {'lora_type':'default',\n",
    "                            'r':8, \n",
    "                            'lora_alpha':32, \n",
    "                            'dropout':0.1,\n",
    "                            'compute_dtype':torch.bfloat16,\n",
    "                            'train_dtype':torch.bfloat16}\n",
    "        \n",
    "        lora_params      = {'self_attn.q_proj': base_lora_params,\n",
    "                            'self_attn.k_proj': base_lora_params,\n",
    "                            'self_attn.v_proj': base_lora_params,\n",
    "                            'self_attn.o_proj': base_lora_params,\n",
    "                            'mlp.gate_proj'   : base_lora_params,\n",
    "                            'mlp.up_proj'     : base_lora_params,\n",
    "                            'mlp.down_proj'   : base_lora_params}\n",
    "        \n",
    "        PeftUtils.add_lora(model, lora_params, base_class=LlamaHQQ, verbose=True)\n",
    "    if gc_enabled: model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    HQQLinear.set_backend(backend)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7a74d-e86d-4836-89c0-dbd29fc327b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "# model = create_qlora_model(model_size=\"DEBUG\", with_lora=True,\n",
    "#                            gc_enabled=False, keep_trainable=False, backend=HQQBackend.PYTORCH_BACKPROP_COMPILE)\n",
    "# model.to(0).to(torch.bfloat16);\n",
    "# x = torch.randint(0,100,(4, 128)).cuda()#.to(torch.bfloat16)\n",
    "# o = model(x)\n",
    "# loss = o.logits.mean()\n",
    "# loss.backward()\n",
    "# for n,p in model.named_parameters(): \n",
    "#     if p.requires_grad:\n",
    "#         print(n, p.dtype, p.device, p.grad.norm().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960139df-83a7-4d98-a306-48880572b7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 197.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL]: 0.862 GB\n",
      "Memory allocated [FWD]: 1.043 GB\n",
      "Elapsed time: 66.540\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 195.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL]: 0.871 GB\n",
      "Memory allocated [FWD]: 1.217 GB\n",
      "Elapsed time: 65.790\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 203.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL]: 0.871 GB\n",
      "Memory allocated [FWD]: 1.561 GB\n",
      "Elapsed time: 65.778\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 212.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL]: 0.871 GB\n",
      "Memory allocated [FWD]: 1.873 GB\n",
      "Elapsed time: 65.310\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.009 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_qlora_model, \"1B\", backend=HQQBackend.ATEN), inference=True, save_filename=save_dir/\"qlora-inference.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59def356-5ac1-48cc-89ea-d13cbc71c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:(1, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 217.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.871 GB\n",
      "Memory allocated [FWD]: 2.563 GB\n",
      "Memory allocated [BWD]: 1.065 GB\n",
      "Elapsed time: 65.322\n",
      "\n",
      "\n",
      "Input Size:(1, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 208.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.879 GB\n",
      "Memory allocated [FWD]: 4.289 GB\n",
      "Memory allocated [BWD]: 1.238 GB\n",
      "Elapsed time: 64.854\n",
      "\n",
      "\n",
      "Input Size:(1, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 207.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.879 GB\n",
      "Memory allocated [FWD]: 7.798 GB\n",
      "Memory allocated [BWD]: 1.582 GB\n",
      "Elapsed time: 64.948\n",
      "\n",
      "\n",
      "Input Size:(1, 3072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 207.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.879 GB\n",
      "Memory allocated [FWD]: 11.376 GB\n",
      "Memory allocated [BWD]: 1.895 GB\n",
      "Elapsed time: 65.371\n",
      "\n",
      "\n",
      "Memory allocated [finish]: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "profile_model(partial(create_qlora_model, \"1B\", backend=HQQBackend.ATEN_BACKPROP), inference=False, save_filename=save_dir/\"qlora-training.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4724c-2e12-4835-9dc7-631f09ba5b22",
   "metadata": {},
   "source": [
    "### QLORA + Gradient Ckpt.\n",
    "\n",
    "Using default HF grad ckpt strategy which wraps each individual decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56649878-2a47-4a4d-9932-79c3650e5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_model(partial(create_qlora_model, \"DEBUG\", gc_enabled=True, backend=HQQBackend.PYTORCH_BACKPROP),\n",
    "              inference=False, save_filename=save_dir/\"qlora-gc-training.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5e6f2-b24d-4d39-ac0b-c8c02aac7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n,p in model.named_parameters():\n",
    "#     print(n, p.name, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c9022-9ff7-4783-93b1-c02f225febc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 307.93it/s]\n"
     ]
    }
   ],
   "source": [
    "model = create_qlora_model(\"DEBUG\", gc_enabled=True, backend=HQQBackend.PYTORCH_BACKPROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c8d6b-717c-475f-89ba-260a33a1dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\", torch.bfloat16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961af51-f76e-4072-b366-3baff4b5e724",
   "metadata": {},
   "source": [
    "This is the correct timing, because this excludes model initialization and quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52343c81-1e9a-4f24-b545-ab87d2521fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 193.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.964 GB\n",
      "Memory allocated [FWD]: 1.092 GB\n",
      "Memory allocated [BWD]: 1.043 GB\n",
      "Max MemAlloc: 1.190423552\n",
      "Elapsed time: 0.402\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 196.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.964 GB\n",
      "Memory allocated [FWD]: 1.220 GB\n",
      "Memory allocated [BWD]: 1.115 GB\n",
      "Max MemAlloc: 1.417184256\n",
      "Elapsed time: 0.401\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 197.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.964 GB\n",
      "Memory allocated [FWD]: 1.471 GB\n",
      "Memory allocated [BWD]: 1.258 GB\n",
      "Max MemAlloc: 1.865462784\n",
      "Elapsed time: 0.411\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 138.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated [MODEL): 0.964 GB\n",
      "Memory allocated [FWD]: 1.717 GB\n",
      "Memory allocated [BWD]: 1.369 GB\n",
      "Max MemAlloc: 2.307974144\n",
      "Elapsed time: 0.500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in inputs:\n",
    "    set_seed(42)\n",
    "    model = create_qlora_model(\"1B\", gc_enabled=True, backend=HQQBackend.ATEN_BACKPROP)\n",
    "    model.to(\"cuda\", torch.bfloat16);\n",
    "    with torch.no_grad():\n",
    "        model(inputs[0].to(\"cuda\"))\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"Memory allocated [MODEL): {malloc_in_gb():.3f} GB\")\n",
    "    output = model(x.to(\"cuda\"))\n",
    "    print(f\"Memory allocated [FWD]: {malloc_in_gb():.3f} GB\")            \n",
    "    output.logits.mean().backward()\n",
    "    print(f\"Memory allocated [BWD]: {malloc_in_gb():.3f} GB\")\n",
    "    max_memory = torch.cuda.memory.max_memory_allocated()/1e9\n",
    "    print(f\"Max MemAlloc: {max_memory}\")\n",
    "    \n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    secs = start.elapsed_time(end) / 1000\n",
    "    print(f\"Elapsed time: {secs:.3f}\\n\\n\")\n",
    "\n",
    "    output, model = None, None\n",
    "    free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df492b7-f2c4-4cb6-a9d6-601484858493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3909d2-20c9-4e29-aebc-37b9be4a54af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27f8e9-5cd7-408e-bf89-d50ecf0ff806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a7dc7-e933-42c9-b341-2eed67868fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b8321-d994-4ce7-be6a-ca032d865b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60b942-9d0a-46fb-9666-150463323d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821da14b-65d8-44ee-875a-e31321e462e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
