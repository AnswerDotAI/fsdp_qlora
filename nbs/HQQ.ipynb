{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5affb06-f669-442a-b4da-136cdfc60e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d634f23-8f34-4cd9-9b58-95ff22d268c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import hqq_aten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b9cc2-3907-45db-bc3a-152529771941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import Quantizer, HQQLinear, BaseQuantizeConfig, HQQBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a374a1-480c-4b2f-bdbb-4dd5d909e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73124d9-480a-43ff-bbf5-c766a845792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import set_seed\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937c643d-19f9-44be-a93a-13c60b266369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME\n",
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac2c2630-5f08-44ed-a73c-0124444df98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.parallel import parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89765edb-1aa2-4fcd-ac06-ad4c5fd87fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally use the context manager to ensure one of the fused kernels is run\n",
    "query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "with torch.backends.cuda.sdp_kernel(True, False, False):\n",
    "    F.scaled_dot_product_attention(query,key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a4d3bd-920b-44ad-b725-220367a2af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7e1f833-9e60-48aa-b686-9ba689b926e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Linear(16,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b64ac929-ecf6-4e21-8f4f-483c55d07945",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=False, offload_meta=False)\n",
    "hqq_linear = HQQLinear(m, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99634e8b-e288-4338-8a8e-2f24df561117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hqq_linear.compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90d6c693-2d6b-4d12-aca0-7c75d73d2757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.8690e+31, -1.7469e-07, -9.8312e-20,  4.3347e+23, -1.0372e-23,\n",
       "         -5.6423e+16,  1.3304e-05,  6.1785e-24],\n",
       "        [-5.7602e+10,  5.1494e+18, -1.7353e+27, -7.9082e-32,  8.7318e+06,\n",
       "         -4.3186e-06,  1.4261e-18,  3.5633e+17],\n",
       "        [ 2.8733e-02, -6.6121e-15,  4.6052e-22, -5.8633e+18,  1.6486e+06,\n",
       "          1.2226e-18,  9.0436e+25,  5.9841e-04],\n",
       "        [ 6.3572e-37,  2.1430e-10,  5.6341e-01, -5.9994e-36,  1.9233e+11,\n",
       "          2.9263e-09,  3.3071e-09,  1.0180e-20],\n",
       "        [-1.0810e-13,  8.8023e+08,  6.2707e+18,  1.3579e-24, -4.7377e+23,\n",
       "          3.5615e+17,  2.6324e-14,  4.2122e-09],\n",
       "        [ 2.4662e-25, -3.4900e+27,  9.6193e+29,  2.6624e+03,  2.2651e-29,\n",
       "          3.0514e+14,  6.9221e+30,  1.6402e+19],\n",
       "        [ 7.4646e+22, -9.6859e-28, -4.3350e-10,  5.1519e-34, -4.1487e-07,\n",
       "         -7.7171e+37,  9.2547e+13,  8.3544e+23],\n",
       "        [-1.6869e-09, -2.6847e+18, -8.0041e-29,  9.5645e-38,  1.3935e-02,\n",
       "         -1.4938e-13,  1.0959e-11,  1.0414e-32],\n",
       "        [-3.7106e-07,  1.6020e-09,  5.3166e+36,  1.1653e-30,  5.6269e+17,\n",
       "          1.7686e-32,  2.3617e+02, -4.2526e+28],\n",
       "        [ 1.7555e+13,  7.6786e-05,  9.5206e+14,  4.9653e-02, -2.7269e-24,\n",
       "         -1.1017e-01, -4.1573e-16, -4.8174e-23],\n",
       "        [-2.9936e+07,  1.9641e-36, -8.3284e-35,  1.8591e-26,  1.4642e+25,\n",
       "          5.6287e-28,  7.7592e+09, -5.0669e+06],\n",
       "        [-1.8897e-21, -2.0112e+20,  4.7147e+34,  9.6051e-25, -5.1717e+05,\n",
       "          9.1546e+00,  5.4721e-24, -1.5698e+24],\n",
       "        [ 1.0694e+16,  5.4373e+04,  1.2801e-03,  4.4126e-09, -1.2773e-35,\n",
       "          3.7246e+07,  3.6701e+15,  6.3485e+06],\n",
       "        [ 2.6589e-09, -2.5449e+06,  9.6047e-39,  4.2585e+20, -1.7479e+02,\n",
       "         -4.3529e-26, -1.1987e+24, -1.1508e+25],\n",
       "        [ 4.6449e-32, -1.5308e-26,  3.9841e-18,  1.1292e-21,  3.8489e-08,\n",
       "         -2.8361e+01, -3.1611e+09, -2.5271e-27],\n",
       "        [-9.7359e-24,  2.7734e+28, -4.8315e-12,  3.0113e+32,  3.9759e+09,\n",
       "         -8.1162e+25,  1.6537e+08,  7.9032e-37],\n",
       "        [ 3.6381e-26,  1.4493e+38, -2.5790e+05, -2.4838e-34,  1.4304e+06,\n",
       "         -1.1399e-36, -2.0599e+23, -4.4556e-23],\n",
       "        [-4.8743e+26, -3.2384e-06,  8.0767e-16, -6.6715e+24,  3.5411e-24,\n",
       "          3.4405e+07,  4.9961e-37,  7.5914e+18],\n",
       "        [ 4.9612e+04, -1.9965e+25,  2.3972e+35, -9.3756e+10,  1.6764e-25,\n",
       "         -3.3598e-22,  3.7503e+10,  3.1760e+21],\n",
       "        [ 2.4561e-08,  1.1222e+35, -1.7132e+34,  4.8265e-19, -5.3818e-17,\n",
       "          4.3160e+01,  1.5106e+13,  4.2396e+25],\n",
       "        [-8.7586e+18,  2.2979e+16,  2.8853e-02, -5.4119e+12, -4.8991e+27,\n",
       "         -1.3176e+05, -1.5185e-35, -5.2663e-08],\n",
       "        [-4.9525e+22,  2.6456e+21, -6.6132e-16,  5.9137e+08, -6.8673e+30,\n",
       "         -1.1277e+03, -8.7609e+29,  5.9418e-28],\n",
       "        [-3.2768e-10, -5.1658e-14, -2.3504e+27,  3.2130e+06, -2.6921e+19,\n",
       "          7.4000e-20,  1.3070e-24, -1.1684e+29],\n",
       "        [-1.9485e+33, -1.6401e+27,  5.9458e-18, -1.1368e-24,  7.1163e-09,\n",
       "         -5.2176e+34,  1.3326e-02,  1.3937e-38],\n",
       "        [-3.4272e-07,  7.0026e+22,  3.3191e+23, -3.8086e-24, -3.1557e-28,\n",
       "         -1.4411e+19,  8.2169e-20, -2.2000e+35],\n",
       "        [-3.9428e+01, -4.0882e-06, -6.5982e-25,  1.6298e+12, -1.0176e+12,\n",
       "          3.0798e+06,  4.0689e+02,  1.3383e+38],\n",
       "        [-1.6804e+08,  3.0361e-01,  5.0893e-34,  1.2463e+18,  1.4580e+06,\n",
       "         -1.8916e+05, -9.8710e+36,  2.9459e+04],\n",
       "        [-2.7046e-11, -4.2445e+21,  5.9648e+01,  4.2992e+14, -3.0052e+05,\n",
       "          4.9578e+23,  1.8172e+25, -2.4127e-17],\n",
       "        [ 6.3310e+13,  1.4881e+32, -6.1006e-36, -6.1947e+11,  5.1969e+05,\n",
       "          1.7885e+25, -1.1800e-37, -4.9508e+04],\n",
       "        [ 1.3706e+17,  5.2504e-05,  8.2312e+13,  8.1923e+08,  5.6115e-25,\n",
       "          4.6359e+16,  1.9769e-20, -8.4875e-32],\n",
       "        [ 1.9187e+23,  9.1218e+25, -1.9125e-17,  5.3448e+23, -1.4947e+32,\n",
       "         -2.7552e+25, -1.3683e-25, -8.3450e-10],\n",
       "        [ 1.8771e+06,  7.4212e-37, -9.7615e-27,  5.3814e+07,  1.0501e-27,\n",
       "         -2.9047e+08, -5.6822e+03,  5.3259e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(hqq_linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "be2b2e23-d2b3-4acb-8b1c-b2fa5a22596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = m.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68c42774-f290-4cad-aeec-470ab2c4adc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ec2b74dd-d59e-4567-9d02-fa7438b70320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_q, meta = Quantizer.quantize(w, round_zero=True, optimize=True, view_as_float=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "83602a4a-3cae-411e-803c-577c118992e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32]), torch.uint8)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.shape, W_q.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4116d9de-ef72-4aed-8f9e-c1657a8ddda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta['scale'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2848f13f-7836-4869-85b7-558d5cb6fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dq = Quantizer.dequantize(W_q, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e48eea18-8fbe-4d8d-bb78-738f3068709e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1196,  0.0683, -0.0960,  ..., -0.2410, -0.1544, -0.0864],\n",
       "         [-0.0278, -0.0483,  0.1141,  ...,  0.0873,  0.0023,  0.2011],\n",
       "         [ 0.0982, -0.0460,  0.0086,  ...,  0.0627, -0.0216, -0.0140],\n",
       "         ...,\n",
       "         [-0.0208,  0.1148, -0.0562,  ..., -0.0961,  0.2354,  0.2077],\n",
       "         [ 0.1820,  0.1345, -0.0235,  ...,  0.0432, -0.1749,  0.1510],\n",
       "         [-0.2125,  0.0024, -0.2045,  ..., -0.1916,  0.1080,  0.0231]]),\n",
       " tensor([[ 0.1224,  0.0717, -0.0930,  ..., -0.2524, -0.1595, -0.0937],\n",
       "         [-0.0320, -0.0627,  0.1289,  ...,  0.0945,  0.0091,  0.1919],\n",
       "         [ 0.0917, -0.0519,  0.0014,  ...,  0.0705, -0.0320,  0.0009],\n",
       "         ...,\n",
       "         [-0.0320,  0.1304, -0.0645,  ..., -0.0981,  0.2344,  0.1919],\n",
       "         [ 0.1841,  0.1334, -0.0301,  ...,  0.0382, -0.1595,  0.1584],\n",
       "         [-0.2222,  0.0016, -0.1934,  ..., -0.1943,  0.1057,  0.0273]],\n",
       "        dtype=torch.float16))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w_dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8f71a73c-3efe-4330-93e9-c9150ec36008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(390.0982)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(w - w_dq, p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c2cf493-865f-4b1c-9cb9-9565acedf4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight_quant_params': {'nbits': 4,\n",
       "  'channel_wise': True,\n",
       "  'group_size': 64,\n",
       "  'optimize': True,\n",
       "  'round_zero': True},\n",
       " 'scale_quant_params': None,\n",
       " 'zero_quant_params': None,\n",
       " 'offload_meta': False}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=False, offload_meta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30251dc6-d4dd-4665-804b-e4bca21ad2e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quant_configs = [\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=False, offload_meta=False),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=True, quant_scale=False, offload_meta=False),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=True, offload_meta=False),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=True, quant_scale=True, offload_meta=False),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=True, quant_scale=True, offload_meta=True),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=False, offload_meta=True)\n",
    "]\n",
    "\n",
    "w_dqs = []\n",
    "for quant_cfg in quant_configs:\n",
    "    if quant_cfg['scale_quant_params']: \n",
    "        quant_cfg['scale_quant_params']['group_size'] = 8\n",
    "    if quant_cfg['zero_quant_params']: \n",
    "        if quant_cfg['offload_meta']:\n",
    "            quant_cfg['zero_quant_params']['group_size'] = 8\n",
    "            quant_cfg['zero_quant_params']['channel_wise'] = True\n",
    "        else:\n",
    "            quant_cfg['zero_quant_params']['group_size'] = None\n",
    "            quant_cfg['zero_quant_params']['channel_wise'] = False\n",
    "    mq = HQQLinear(m, quant_cfg, compute_dtype=torch.bfloat16, initialize=False)\n",
    "    HQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)\n",
    "    mq.initialize()\n",
    "    print(mq.W_q.dtype, mq.meta)\n",
    "    print()\n",
    "    w_dqs.append(mq.dequantize_aten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5c665281-3cbe-492c-9654-eabbb1d98de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(390.9176, device='cuda:0'),\n",
       " tensor(390.5967, device='cuda:0'),\n",
       " tensor(390.7930, device='cuda:0'),\n",
       " tensor(390.1439, device='cuda:0'),\n",
       " tensor(392.0999, device='cuda:0'))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.norm(w.cuda() - w_dqs[0], p=0.7),\n",
    "torch.norm(w.cuda() - w_dqs[1], p=0.7),\n",
    "torch.norm(w.cuda() - w_dqs[2], p=0.7),\n",
    "torch.norm(w.cuda() - w_dqs[3], p=0.7),\n",
    "torch.norm(w.cuda() - w_dqs[4], p=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894d24c3-36de-437a-b4ea-eb64a87d850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_hqq(model:nn.Module, quant_config, skip_modules:List[str]=[\"lm_head\"], **kwargs):\n",
    "    \"\"\"\n",
    "    Replace linear modules with a new Linear module.\n",
    "    Parameters:\n",
    "        model (`torch.nn.Module`):\n",
    "            Input model or `torch.nn.Module` as the function is run recursively.\n",
    "        quant_config (`Dict[str, Any]`):\n",
    "            The quantization configuration for the new linear module.\n",
    "        skip_modules (`List[str]`, *optional*, defaults to `lm_head`):\n",
    "            List of modules names not to convert. Defaults to `lm_head`.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear_hqq(module, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear) and name not in skip_modules:\n",
    "            model._modules[name] = HQQLinear(\n",
    "                module,\n",
    "                quant_config,\n",
    "                **kwargs\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3079ca1-0cd8-48d2-a199-1dc61589228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize_hqq(module:nn.Module, name:str, value:Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                                  skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False):\n",
    "    \"\"\"\n",
    "    Loads `value` tensor into submodule of `module`, optionally skipping `skip_names` and converting to `dtype`.\n",
    "\n",
    "    Quantizes `Params4bit` on `device` then places on \"cpu\" if low_memory=True or \"meta\" if is_meta_rank=True.\n",
    "    \"\"\"\n",
    "    def place_on_device(value):\n",
    "        if is_meta_rank:\n",
    "            device = 'meta'\n",
    "        elif low_memory:\n",
    "            device = 'cpu'\n",
    "        return value.to(device=device, dtype=dtype)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "        if verbose:\n",
    "            print(f\"Skipping {name} because it is in skip_names\")\n",
    "        return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "        submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "        print(f\"Module {module_key} not found:\\n{e}\")\n",
    "        return\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        if isinstance(submodule, HQQLinear):\n",
    "            if value_key == \"weight\":\n",
    "                # init meta weights as empty on cpu\n",
    "                submodule.linear_layer.to_empty(device=\"cpu\")\n",
    "                # copy pretrained weights\n",
    "                submodule.linear_layer.weight.data.copy_(value)\n",
    "                # quantize and update metadata\n",
    "                submodule.initialize()\n",
    "                \n",
    "                if is_meta_rank:\n",
    "                    setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to(\"meta\")))\n",
    "                elif low_memory:\n",
    "                    setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to(\"cpu\")))\n",
    "                submodule.in_gpu = False\n",
    "\n",
    "            if value_key == \"bias\":\n",
    "                raise ValueError(\"Bias not supported in HQQLinear yet!\")\n",
    "        \n",
    "            end = time.time()\n",
    "            if not is_meta_rank:\n",
    "                print(f\"Loaded HQQLinear quantized {module_key} in {end-start:.3f} seconds\")\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            param = submodule.get_parameter(value_key)\n",
    "            value = type(param)(place_on_device(value).data)\n",
    "\n",
    "    except AttributeError:\n",
    "        # it's a buffer\n",
    "        value = place_on_device(value)\n",
    "        pass\n",
    "    \n",
    "    setattr(submodule, value_key, value)\n",
    "    end = time.time()\n",
    "    torch.cuda.empty_cache()\n",
    "    if not is_meta_rank:\n",
    "        print(f\"Loaded {module_key} and {value_key} in {end-start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9a374d8-85e1-4ea8-b078-0076b9682bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = hub.cached_file(model_name, SAFE_WEIGHTS_INDEX_NAME)\n",
    "files, _ = hub.get_checkpoint_shard_files(model_name, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd9cb0d3-fb22-43e6-be64-e1729ec83415",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = torch.bfloat16\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(model_name)\n",
    "cfg.use_cache = False\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "# cfg.num_hidden_layers = 8 # DEBUG\n",
    "\n",
    "# load model on meta device without calling init and replace nn.Linear with Linear4bit\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    # TODO: Tune BaseQuantizeConfig.\n",
    "    quant_config = BaseQuantizeConfig(nbits=4, \n",
    "                                      group_size=64, \n",
    "                                      quant_zero=True, \n",
    "                                      quant_scale=True, \n",
    "                                      offload_meta=True)\n",
    "    model.model = replace_linear_hqq(model.model, quant_config, device_n=torch.cuda.current_device(),\n",
    "                                    compute_dtype=compute_dtype, del_orig=True, initialize=False)     \n",
    "    HQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b50ddd41-10fc-449e-8bf3-92505b789ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 0\n",
      "Loaded model.embed_tokens and weight in 0.067 seconds\n",
      "Loaded model.layers.0.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.down_proj in 0.271 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.gate_proj in 0.243 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.up_proj in 0.236 seconds\n",
      "Loaded model.layers.0.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.k_proj in 0.065 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.o_proj in 0.062 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.q_proj in 0.063 seconds\n",
      "Loaded model.layers.0.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.v_proj in 0.060 seconds\n",
      "Loaded model.layers.1.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.down_proj in 0.239 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.gate_proj in 0.247 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.up_proj in 0.283 seconds\n",
      "Loaded model.layers.1.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.k_proj in 0.078 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.o_proj in 0.065 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.q_proj in 0.061 seconds\n",
      "Loaded model.layers.1.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.v_proj in 0.074 seconds\n",
      "Loaded model.layers.10.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.down_proj in 0.976 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.gate_proj in 1.748 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.up_proj in 1.001 seconds\n",
      "Loaded model.layers.10.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.k_proj in 0.358 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.o_proj in 0.383 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.q_proj in 0.390 seconds\n",
      "Loaded model.layers.10.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.v_proj in 0.394 seconds\n",
      "Loaded model.layers.11.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.down_proj in 0.971 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.gate_proj in 0.959 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.up_proj in 1.649 seconds\n",
      "Loaded model.layers.11.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.k_proj in 0.410 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.o_proj in 0.391 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.q_proj in 0.375 seconds\n",
      "Loaded model.layers.11.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.v_proj in 0.401 seconds\n",
      "Loaded model.layers.12.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.down_proj in 0.961 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.gate_proj in 0.927 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.up_proj in 0.967 seconds\n",
      "Loaded model.layers.12.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.k_proj in 0.418 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.o_proj in 1.161 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.q_proj in 0.388 seconds\n",
      "Loaded model.layers.12.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.v_proj in 0.385 seconds\n",
      "Loaded model.layers.13.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.down_proj in 0.953 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.gate_proj in 0.949 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.up_proj in 0.950 seconds\n",
      "Loaded model.layers.13.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.k_proj in 0.382 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.o_proj in 0.370 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.q_proj in 0.386 seconds\n",
      "Loaded model.layers.13.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.v_proj in 1.341 seconds\n",
      "Loaded model.layers.14.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.down_proj in 0.947 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.gate_proj in 0.946 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.up_proj in 0.984 seconds\n",
      "Loaded model.layers.14.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.k_proj in 0.386 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.o_proj in 0.387 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.q_proj in 0.378 seconds\n",
      "Loaded model.layers.14.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.v_proj in 0.376 seconds\n",
      "Loaded model.layers.15.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.down_proj in 1.806 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.gate_proj in 0.921 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.up_proj in 0.939 seconds\n",
      "Loaded model.layers.15.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.k_proj in 0.386 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.o_proj in 0.378 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.q_proj in 0.377 seconds\n",
      "Loaded model.layers.15.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.v_proj in 0.391 seconds\n",
      "Loaded model.layers.16.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.down_proj in 0.981 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.gate_proj in 1.731 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.up_proj in 0.962 seconds\n",
      "Loaded model.layers.16.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.k_proj in 0.387 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.o_proj in 0.382 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.q_proj in 0.361 seconds\n",
      "Loaded model.layers.16.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.v_proj in 0.365 seconds\n",
      "Loaded model.layers.17.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.down_proj in 0.938 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.gate_proj in 0.966 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.up_proj in 1.776 seconds\n",
      "Loaded model.layers.17.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.k_proj in 0.397 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.o_proj in 0.401 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.q_proj in 0.400 seconds\n",
      "Loaded model.layers.17.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.v_proj in 0.359 seconds\n",
      "Loaded model.layers.18.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.down_proj in 0.956 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.gate_proj in 0.964 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.up_proj in 0.946 seconds\n",
      "Loaded model.layers.18.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.k_proj in 0.429 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.o_proj in 1.168 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.q_proj in 0.363 seconds\n",
      "Loaded model.layers.18.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.v_proj in 0.367 seconds\n",
      "Loaded model.layers.19.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.down_proj in 0.962 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.gate_proj in 0.942 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.up_proj in 0.956 seconds\n",
      "Loaded model.layers.19.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.k_proj in 0.407 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.o_proj in 0.373 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.q_proj in 0.404 seconds\n",
      "Loaded model.layers.19.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.v_proj in 1.342 seconds\n",
      "Loaded model.layers.2.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.down_proj in 0.251 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.gate_proj in 0.241 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.up_proj in 0.238 seconds\n",
      "Loaded model.layers.2.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.k_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.q_proj in 0.093 seconds\n",
      "Loaded model.layers.2.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.v_proj in 0.094 seconds\n",
      "Loaded model.layers.20.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.down_proj in 0.951 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.gate_proj in 0.962 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.up_proj in 0.947 seconds\n",
      "Loaded model.layers.20.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.k_proj in 0.370 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.o_proj in 0.401 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.q_proj in 1.345 seconds\n",
      "Loaded model.layers.20.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.v_proj in 0.411 seconds\n",
      "Loaded model.layers.21.input_layernorm and weight in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.down_proj in 0.966 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.gate_proj in 0.923 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.up_proj in 0.971 seconds\n",
      "Loaded model.layers.21.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.k_proj in 0.391 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.o_proj in 0.376 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.q_proj in 0.398 seconds\n",
      "Loaded model.layers.21.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.v_proj in 0.408 seconds\n",
      "Loaded model.layers.22.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.down_proj in 1.392 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.gate_proj in 0.947 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.up_proj in 0.970 seconds\n",
      "Loaded model.layers.22.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.k_proj in 0.398 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.o_proj in 0.383 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.q_proj in 0.443 seconds\n",
      "Loaded model.layers.22.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.v_proj in 0.375 seconds\n",
      "Loaded model.layers.23.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.down_proj in 0.961 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.gate_proj in 1.622 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.up_proj in 0.976 seconds\n",
      "Loaded model.layers.23.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.k_proj in 0.362 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.o_proj in 0.406 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.q_proj in 0.391 seconds\n",
      "Loaded model.layers.23.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.v_proj in 0.384 seconds\n",
      "Loaded model.layers.3.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.down_proj in 0.250 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.gate_proj in 0.237 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.up_proj in 0.246 seconds\n",
      "Loaded model.layers.3.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.k_proj in 0.091 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.o_proj in 0.091 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.q_proj in 0.094 seconds\n",
      "Loaded model.layers.3.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.v_proj in 0.089 seconds\n",
      "Loaded model.layers.4.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.down_proj in 0.235 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.gate_proj in 0.253 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.up_proj in 0.233 seconds\n",
      "Loaded model.layers.4.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.k_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.o_proj in 0.093 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.q_proj in 0.095 seconds\n",
      "Loaded model.layers.4.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.v_proj in 0.092 seconds\n",
      "Loaded model.layers.5.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.down_proj in 1.329 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.gate_proj in 0.250 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.up_proj in 0.232 seconds\n",
      "Loaded model.layers.5.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.k_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.q_proj in 0.092 seconds\n",
      "Loaded model.layers.5.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.v_proj in 0.093 seconds\n",
      "Loaded model.layers.6.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.down_proj in 0.248 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.gate_proj in 0.242 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.up_proj in 0.233 seconds\n",
      "Loaded model.layers.6.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.k_proj in 0.098 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.q_proj in 0.095 seconds\n",
      "Loaded model.layers.6.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.v_proj in 0.091 seconds\n",
      "Loaded model.layers.7.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.down_proj in 0.250 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.gate_proj in 0.232 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.up_proj in 0.234 seconds\n",
      "Loaded model.layers.7.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.k_proj in 0.096 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.o_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.7.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.v_proj in 0.092 seconds\n",
      "Loaded model.layers.8.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.down_proj in 0.955 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.gate_proj in 2.081 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.up_proj in 0.952 seconds\n",
      "Loaded model.layers.8.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.k_proj in 0.378 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.o_proj in 0.388 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.q_proj in 0.365 seconds\n",
      "Loaded model.layers.8.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.v_proj in 0.383 seconds\n",
      "Loaded model.layers.9.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.down_proj in 0.943 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.gate_proj in 0.949 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.up_proj in 1.898 seconds\n",
      "Loaded model.layers.9.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.k_proj in 0.375 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.o_proj in 0.392 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.q_proj in 0.389 seconds\n",
      "Loaded model.layers.9.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.v_proj in 0.385 seconds\n",
      "Loaded lm_head and weight in 0.066 seconds\n",
      "Loaded model.layers.24.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.down_proj in 0.239 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.gate_proj in 0.252 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.up_proj in 0.248 seconds\n",
      "Loaded model.layers.24.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.k_proj in 0.096 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.o_proj in 0.093 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.q_proj in 0.101 seconds\n",
      "Loaded model.layers.24.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.v_proj in 0.095 seconds\n",
      "Loaded model.layers.25.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.down_proj in 0.238 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.gate_proj in 0.261 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.up_proj in 0.250 seconds\n",
      "Loaded model.layers.25.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.k_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.o_proj in 0.093 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.q_proj in 0.095 seconds\n",
      "Loaded model.layers.25.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.v_proj in 0.103 seconds\n",
      "Loaded model.layers.26.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.down_proj in 0.244 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.gate_proj in 0.241 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.up_proj in 1.210 seconds\n",
      "Loaded model.layers.26.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.k_proj in 0.098 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.o_proj in 0.093 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.26.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.v_proj in 0.152 seconds\n",
      "Loaded model.layers.27.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.down_proj in 0.242 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.gate_proj in 0.237 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.up_proj in 0.235 seconds\n",
      "Loaded model.layers.27.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.k_proj in 0.097 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.27.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.v_proj in 0.097 seconds\n",
      "Loaded model.layers.28.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.down_proj in 0.249 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.gate_proj in 0.236 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.up_proj in 0.235 seconds\n",
      "Loaded model.layers.28.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.k_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.o_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.28.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.v_proj in 0.095 seconds\n",
      "Loaded model.layers.29.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.down_proj in 0.254 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.gate_proj in 0.240 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.up_proj in 0.240 seconds\n",
      "Loaded model.layers.29.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.k_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.o_proj in 0.096 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.29.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.v_proj in 0.095 seconds\n",
      "Loaded model.layers.30.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.down_proj in 0.240 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.gate_proj in 0.236 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.up_proj in 0.236 seconds\n",
      "Loaded model.layers.30.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.k_proj in 0.097 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.o_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.q_proj in 0.098 seconds\n",
      "Loaded model.layers.30.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.v_proj in 0.097 seconds\n",
      "Loaded model.layers.31.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.down_proj in 1.292 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.gate_proj in 0.255 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.up_proj in 0.235 seconds\n",
      "Loaded model.layers.31.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.k_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.q_proj in 0.094 seconds\n",
      "Loaded model.layers.31.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.v_proj in 0.094 seconds\n",
      "Loaded model.norm and weight in 0.000 seconds\n",
      "Loaded model weights in 103.558 seconds\n"
     ]
    }
   ],
   "source": [
    "local_rank = 0\n",
    "low_memory = True\n",
    "load_param_skip_names = []\n",
    "rank = 0\n",
    "\n",
    "print(\"Loading model\", rank)\n",
    "start = time.time()\n",
    "for filename in files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    for name, param in weights.items():\n",
    "        load_and_quantize_hqq(model, name, param, dtype=torch.bfloat16, device=local_rank, skip_names=load_param_skip_names,\n",
    "                                is_meta_rank=(low_memory and rank!=0), verbose=True)\n",
    "print(f\"Loaded model weights in {time.time()-start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2e2b5940-7eab-462d-9ef0-34465c678e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize_parallel(name_param, load_func, model, **kwargs):\n",
    "    name, param = name_param\n",
    "    load_func(model, name, param, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3e24ea0f-dc4a-470c-9f9d-6721d02f842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = torch.bfloat16\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(model_name)\n",
    "cfg.use_cache = False\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "# cfg.num_hidden_layers = 8 # DEBUG\n",
    "\n",
    "# load model on meta device without calling init and replace nn.Linear with Linear4bit\n",
    "with init_empty_weights():\n",
    "    model_fast = AutoModelForCausalLM.from_config(cfg)\n",
    "    # TODO: Tune BaseQuantizeConfig.\n",
    "    quant_config = BaseQuantizeConfig(nbits=4, \n",
    "                                      group_size=64, \n",
    "                                      quant_zero=True, \n",
    "                                      quant_scale=True, \n",
    "                                      offload_meta=True)\n",
    "    model_fast.model = replace_linear_hqq(model_fast.model, quant_config, device_n=torch.cuda.current_device(),\n",
    "                                          compute_dtype=compute_dtype, del_orig=True, initialize=False)     \n",
    "    HQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)\n",
    "model_fast.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a37a589e-d45e-49c2-aa6d-d52d8f175f96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 0\n",
      "Loaded model.layers.0.input_layernorm and weight in 0.003 seconds\n",
      "Loaded model.layers.0.post_attention_layernorm and weight in 0.004 seconds\n",
      "Loaded model.layers.0.self_attn.rotary_emb and inv_freq in 0.032 seconds\n",
      "Loaded model.embed_tokens and weight in 0.203 seconds\n",
      "Loaded model.layers.1.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.k_proj in 1.016 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.gate_proj in 1.065 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.down_proj in 1.201 seconds\n",
      "Loaded model.layers.1.post_attention_layernorm and weight in 0.008 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.v_proj in 1.155 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.q_proj in 1.211 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.up_proj in 1.252 seconds\n",
      "Loaded model.layers.1.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.o_proj in 1.386 seconds\n",
      "Loaded model.layers.10.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.down_proj in 1.298 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.o_proj in 0.402 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.v_proj in 1.823 seconds\n",
      "Loaded model.layers.10.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.k_proj in 2.032 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.up_proj in 2.188 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.q_proj in 2.030 seconds\n",
      "Loaded model.layers.10.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.gate_proj in 2.246 seconds\n",
      "Loaded model.layers.11.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.down_proj in 2.360 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.gate_proj in 2.378 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.v_proj in 0.571 seconds\n",
      "Loaded model.layers.11.post_attention_layernorm and weight in 0.018 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.k_proj in 0.867 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.up_proj in 2.499 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.q_proj in 0.913 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.o_proj in 0.953 seconds\n",
      "Loaded model.layers.11.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded model.layers.12.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.down_proj in 0.997 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.k_proj in 0.773 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.o_proj in 1.063 seconds\n",
      "Loaded model.layers.12.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.v_proj in 0.863 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.down_proj in 0.906 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.q_proj in 1.017 seconds\n",
      "Loaded model.layers.12.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.gate_proj in 1.516 seconds\n",
      "Loaded model.layers.13.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.up_proj in 1.494 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.gate_proj in 1.054 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.o_proj in 0.673 seconds\n",
      "Loaded model.layers.13.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.v_proj in 0.639 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.up_proj in 1.140 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.k_proj in 0.902 seconds\n",
      "Loaded model.layers.13.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.q_proj in 0.934 seconds\n",
      "Loaded model.layers.14.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.down_proj in 0.963 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.up_proj in 0.965 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.gate_proj in 1.018 seconds\n",
      "Loaded model.layers.14.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.k_proj in 0.812 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.q_proj in 0.942 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.v_proj in 0.828 seconds\n",
      "Loaded model.layers.14.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.down_proj in 0.778 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.o_proj in 1.024 seconds\n",
      "Loaded model.layers.15.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.o_proj in 0.542 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.k_proj in 1.054 seconds\n",
      "Loaded model.layers.15.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.up_proj in 1.978 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.gate_proj in 2.594 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.v_proj in 2.121 seconds\n",
      "Loaded model.layers.15.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.q_proj in 2.161 seconds\n",
      "Loaded model.layers.16.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.down_proj in 2.245 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.k_proj in 1.701 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.up_proj in 2.032 seconds\n",
      "Loaded model.layers.16.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.gate_proj in 2.374 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.o_proj in 1.184 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.v_proj in 0.704 seconds\n",
      "Loaded model.layers.16.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.q_proj in 0.981 seconds\n",
      "Loaded model.layers.17.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.k_proj in 0.747 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.o_proj in 0.767 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.v_proj in 0.632 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.q_proj in 0.738 seconds\n",
      "Loaded model.layers.17.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.gate_proj in 1.288 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.up_proj in 1.285 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.down_proj in 1.503 seconds\n",
      "Loaded model.layers.17.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded model.layers.18.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.down_proj in 1.219 secondsLoaded HQQLinear quantized model.layers.17.mlp.gate_proj in 1.209 seconds\n",
      "\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.o_proj in 0.855 seconds\n",
      "Loaded model.layers.18.post_attention_layernorm and weight in 0.029 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.k_proj in 0.922 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.q_proj in 0.810 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.v_proj in 0.849 seconds\n",
      "Loaded model.layers.18.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.up_proj in 1.460 seconds\n",
      "Loaded model.layers.19.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.down_proj in 1.052 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.k_proj in 0.612 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.v_proj in 0.581 seconds\n",
      "Loaded model.layers.19.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.o_proj in 1.007 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.q_proj in 1.012 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.gate_proj in 1.167 seconds\n",
      "Loaded model.layers.19.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.up_proj in 1.337 seconds\n",
      "Loaded model.layers.2.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.down_proj in 1.059 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.gate_proj in 1.102 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.k_proj in 1.013 seconds\n",
      "Loaded model.layers.2.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.up_proj in 1.142 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.v_proj in 0.642 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.q_proj in 0.751 seconds\n",
      "Loaded model.layers.2.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.o_proj in 0.763 seconds\n",
      "Loaded model.layers.20.input_layernorm and weight in 0.006 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.q_proj in 0.689 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.o_proj in 0.734 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.k_proj in 0.771 seconds\n",
      "Loaded model.layers.20.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.v_proj in 0.785 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.down_proj in 1.439 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.up_proj in 2.440 seconds\n",
      "Loaded model.layers.20.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.gate_proj in 2.582 seconds\n",
      "Loaded model.layers.21.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.down_proj in 2.197 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.o_proj in 1.730 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.q_proj in 1.778 seconds\n",
      "Loaded model.layers.21.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.v_proj in 0.687 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.up_proj in 2.315 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.k_proj in 2.336 seconds\n",
      "Loaded model.layers.21.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.down_proj in 1.099 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.gate_proj in 2.594 seconds\n",
      "Loaded model.layers.22.input_layernorm and weight in 0.007 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.gate_proj in 1.152 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.o_proj in 0.748 seconds\n",
      "Loaded model.layers.22.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.k_proj in 0.829 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.up_proj in 1.203 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.v_proj in 0.771 seconds\n",
      "Loaded model.layers.22.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.q_proj in 0.923 seconds\n",
      "Loaded model.layers.23.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.down_proj in 0.902 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.q_proj in 0.727 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.o_proj in 0.917 seconds\n",
      "Loaded model.layers.23.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.v_proj in 0.663 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.gate_proj in 1.293 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.k_proj in 1.033 seconds\n",
      "Loaded model.layers.23.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.up_proj in 1.217 seconds\n",
      "Loaded model.layers.3.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.v_proj in 0.604 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.o_proj in 0.804 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.down_proj in 1.380 seconds\n",
      "Loaded model.layers.3.post_attention_layernorm and weight in 0.021 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.up_proj in 1.099 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.k_proj in 1.108 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.gate_proj in 1.493 seconds\n",
      "Loaded model.layers.3.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.down_proj in 1.088 seconds\n",
      "Loaded model.layers.4.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.q_proj in 1.148 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.v_proj in 0.351 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.gate_proj in 1.057 seconds\n",
      "Loaded model.layers.4.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.o_proj in 0.767 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.k_proj in 0.978 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.q_proj in 0.947 seconds\n",
      "Loaded model.layers.4.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.up_proj in 1.494 seconds\n",
      "Loaded model.layers.5.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.gate_proj in 1.188 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.down_proj in 1.268 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.q_proj in 0.671 seconds\n",
      "Loaded model.layers.5.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.k_proj in 2.018 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.o_proj in 1.968 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.v_proj in 1.807 seconds\n",
      "Loaded model.layers.5.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.up_proj in 2.425 seconds\n",
      "Loaded model.layers.6.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.up_proj in 1.880 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.q_proj in 0.679 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.o_proj in 0.709 seconds\n",
      "Loaded model.layers.6.post_attention_layernorm and weight in 0.007 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.v_proj in 0.771 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.k_proj in 2.119 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.gate_proj in 2.472 seconds\n",
      "Loaded model.layers.6.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.down_proj in 2.591 seconds\n",
      "Loaded model.layers.7.input_layernorm and weight in 0.003 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.down_proj in 1.020 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.q_proj in 0.825 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.up_proj in 1.041 seconds\n",
      "Loaded model.layers.7.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.k_proj in 1.067 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.o_proj in 0.937 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.v_proj in 0.784 seconds\n",
      "Loaded model.layers.7.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.gate_proj in 1.527 seconds\n",
      "Loaded model.layers.8.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.gate_proj in 1.046 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.down_proj in 1.137 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.v_proj in 0.752 seconds\n",
      "Loaded model.layers.8.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.q_proj in 0.925 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.up_proj in 1.073 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.o_proj in 1.033 seconds\n",
      "Loaded model.layers.8.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.k_proj in 1.133 seconds\n",
      "Loaded model.layers.9.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.down_proj in 1.100 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.gate_proj in 1.235 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.v_proj in 0.645 seconds\n",
      "Loaded model.layers.9.post_attention_layernorm and weight in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.o_proj in 0.756 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.up_proj in 1.346 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.k_proj in 0.991 seconds\n",
      "Loaded model.layers.9.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.q_proj in 0.897 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.down_proj in 1.155 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.o_proj in 0.619 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.k_proj in 0.670 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.q_proj in 0.528 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.gate_proj in 0.970 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.v_proj in 0.566 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.up_proj in 0.756 seconds\n",
      "Loaded lm_head and weight in 0.330 secondsLoaded model.layers.24.input_layernorm and weight in 0.006 seconds\n",
      "\n",
      "Loaded model.layers.24.post_attention_layernorm and weight in 0.016 seconds\n",
      "Loaded model.layers.24.self_attn.rotary_emb and inv_freq in 0.001 seconds\n",
      "Loaded model.layers.25.input_layernorm and weight in 0.008 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.o_proj in 1.008 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.v_proj in 1.013 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.down_proj in 1.464 seconds\n",
      "Loaded model.layers.25.post_attention_layernorm and weight in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.k_proj in 1.130 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.up_proj in 1.169 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.q_proj in 1.338 seconds\n",
      "Loaded model.layers.25.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.gate_proj in 1.436 seconds\n",
      "Loaded model.layers.26.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.down_proj in 1.402 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.k_proj in 0.522 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.o_proj in 0.653 seconds\n",
      "Loaded model.layers.26.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.up_proj in 0.961 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.q_proj in 0.841 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.gate_proj in 1.216 seconds\n",
      "Loaded model.layers.26.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.v_proj in 0.897 seconds\n",
      "Loaded model.layers.27.input_layernorm and weight in 0.008 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.gate_proj in 0.943 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.k_proj in 0.647 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.q_proj in 0.673 seconds\n",
      "Loaded model.layers.27.post_attention_layernorm and weight in 0.003 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.up_proj in 1.228 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.o_proj in 0.894 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.down_proj in 1.497 seconds\n",
      "Loaded model.layers.27.self_attn.rotary_emb and inv_freq in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.v_proj in 0.723 seconds\n",
      "Loaded model.layers.28.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.gate_proj in 1.199 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.up_proj in 1.211 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.o_proj in 0.845 seconds\n",
      "Loaded model.layers.28.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.k_proj in 1.028 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.q_proj in 0.857 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.v_proj in 0.933 seconds\n",
      "Loaded model.layers.28.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.down_proj in 1.740 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.down_proj in 1.025 seconds\n",
      "Loaded model.layers.29.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.q_proj in 0.835 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.o_proj in 0.862 seconds\n",
      "Loaded model.layers.29.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.v_proj in 0.866 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.up_proj in 1.158 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.k_proj in 1.129 seconds\n",
      "Loaded model.layers.29.self_attn.rotary_emb and inv_freq in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.gate_proj in 1.404 seconds\n",
      "Loaded model.layers.30.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.gate_proj in 1.084 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.down_proj in 1.131 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.v_proj in 1.754 seconds\n",
      "Loaded model.layers.30.post_attention_layernorm and weight in 0.003 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.k_proj in 2.057 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.o_proj in 1.930 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.q_proj in 2.034 seconds\n",
      "Loaded model.layers.30.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.up_proj in 2.393 seconds\n",
      "Loaded model.layers.31.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.up_proj in 1.942 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.gate_proj in 2.062 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.down_proj in 2.221 seconds\n",
      "Loaded model.layers.31.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.o_proj in 0.757 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.v_proj in 0.664 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.k_proj in 1.169 seconds\n",
      "Loaded model.layers.31.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.q_proj in 1.238 seconds\n",
      "Loaded model.norm and weight in 0.015 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.k_proj in 0.725 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.q_proj in 0.440 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.o_proj in 0.576 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.gate_proj in 0.969 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.down_proj in 1.118 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.up_proj in 0.988 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.v_proj in 0.358 seconds\n",
      "Loaded model weights in 36.317 seconds\n"
     ]
    }
   ],
   "source": [
    "local_rank = 0\n",
    "low_memory = True\n",
    "load_param_skip_names = []\n",
    "rank = 0\n",
    "\n",
    "print(\"Loading model\", rank)\n",
    "start = time.time()\n",
    "for filename in files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    parallel(load_and_quantize_parallel, weights.items(), n_workers=8, threadpool=True, \n",
    "             load_func=load_and_quantize_hqq, model=model_fast, \n",
    "             dtype=torch.bfloat16, device=local_rank, skip_names=load_param_skip_names, \n",
    "             is_meta_rank=(low_memory and rank!=0), verbose=True)\n",
    "print(f\"Loaded model weights in {time.time()-start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a35d4c27-90d9-4492-a068-6c2afd8ecfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n1,p1), (n2,p2) in zip(model.named_parameters(), model_fast.named_parameters()):\n",
    "    if n1 == n2:\n",
    "        if \"proj\" in n1:\n",
    "            assert torch.allclose(p1.view(torch.uint8), p2.view(torch.uint8))\n",
    "        else:\n",
    "            assert torch.allclose(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64248a-632e-4371-b9b1-813b27b2da2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102c5e3-da86-46a2-bfa3-039de86f0215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fc635-67c7-4787-ace0-d5a747940d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115499c-2746-4f2c-a49e-2980ec4252aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f1b81-be8b-47bc-b047-37489cbda9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7b95b-0d39-4513-91c0-3aeeab0cce67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b97abb-87fc-4258-a1c2-a851c960c4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961d63c-c0cc-4445-a143-4cc9aa76c15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f401a-7f35-41ae-9303-722ba87b1e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6964555-be43-4695-9d0c-c15fc0853c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
