{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5affb06-f669-442a-b4da-136cdfc60e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d634f23-8f34-4cd9-9b58-95ff22d268c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import hqq_aten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b9cc2-3907-45db-bc3a-152529771941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhqq_aten package available. Set backend to HQQBackend.ATEN for faster inference and HQQBackend.ATEN_BACKPROP for faster training!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from hqq.core.quantize import Quantizer, HQQLinear, BaseQuantizeConfig, HQQBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a374a1-480c-4b2f-bdbb-4dd5d909e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73124d9-480a-43ff-bbf5-c766a845792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import set_seed\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c643d-19f9-44be-a93a-13c60b266369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME\n",
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2630-5f08-44ed-a73c-0124444df98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.parallel import parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89765edb-1aa2-4fcd-ac06-ad4c5fd87fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally use the context manager to ensure one of the fused kernels is run\n",
    "query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "with torch.backends.cuda.sdp_kernel(True, False, False):\n",
    "    F.scaled_dot_product_attention(query,key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4d3bd-920b-44ad-b725-220367a2af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1f833-9e60-48aa-b686-9ba689b926e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Linear(16,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb493d-cf91-4f6a-8b4f-60c9ed7164c0",
   "metadata": {},
   "source": [
    "### FSDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ac929-ecf6-4e21-8f4f-483c55d07945",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=False, offload_meta=False)\n",
    "hqq_linear = HQQLinear(m, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99634e8b-e288-4338-8a8e-2f24df561117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hqq_linear.compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6c693-2d6b-4d12-aca0-7c75d73d2757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.8690e+31, -1.7469e-07, -9.8312e-20,  4.3347e+23, -1.0372e-23,\n",
       "         -5.6423e+16,  1.3304e-05,  6.1785e-24],\n",
       "        [-5.7602e+10,  5.1494e+18, -1.7353e+27, -7.9082e-32,  8.7318e+06,\n",
       "         -4.3186e-06,  1.4261e-18,  3.5633e+17],\n",
       "        [ 2.8733e-02, -6.6121e-15,  4.6052e-22, -5.8633e+18,  1.6486e+06,\n",
       "          1.2226e-18,  9.0436e+25,  5.9841e-04],\n",
       "        [ 6.3572e-37,  2.1430e-10,  5.6341e-01, -5.9994e-36,  1.9233e+11,\n",
       "          2.9263e-09,  3.3071e-09,  1.0180e-20],\n",
       "        [-1.0810e-13,  8.8023e+08,  6.2707e+18,  1.3579e-24, -4.7377e+23,\n",
       "          3.5615e+17,  2.6324e-14,  4.2122e-09],\n",
       "        [ 2.4662e-25, -3.4900e+27,  9.6193e+29,  2.6624e+03,  2.2651e-29,\n",
       "          3.0514e+14,  6.9221e+30,  1.6402e+19],\n",
       "        [ 7.4646e+22, -9.6859e-28, -4.3350e-10,  5.1519e-34, -4.1487e-07,\n",
       "         -7.7171e+37,  9.2547e+13,  8.3544e+23],\n",
       "        [-1.6869e-09, -2.6847e+18, -8.0041e-29,  9.5645e-38,  1.3935e-02,\n",
       "         -1.4938e-13,  1.0959e-11,  1.0414e-32],\n",
       "        [-3.7106e-07,  1.6020e-09,  5.3166e+36,  1.1653e-30,  5.6269e+17,\n",
       "          1.7686e-32,  2.3617e+02, -4.2526e+28],\n",
       "        [ 1.7555e+13,  7.6786e-05,  9.5206e+14,  4.9653e-02, -2.7269e-24,\n",
       "         -1.1017e-01, -4.1573e-16, -4.8174e-23],\n",
       "        [-2.9936e+07,  1.9641e-36, -8.3284e-35,  1.8591e-26,  1.4642e+25,\n",
       "          5.6287e-28,  7.7592e+09, -5.0669e+06],\n",
       "        [-1.8897e-21, -2.0112e+20,  4.7147e+34,  9.6051e-25, -5.1717e+05,\n",
       "          9.1546e+00,  5.4721e-24, -1.5698e+24],\n",
       "        [ 1.0694e+16,  5.4373e+04,  1.2801e-03,  4.4126e-09, -1.2773e-35,\n",
       "          3.7246e+07,  3.6701e+15,  6.3485e+06],\n",
       "        [ 2.6589e-09, -2.5449e+06,  9.6047e-39,  4.2585e+20, -1.7479e+02,\n",
       "         -4.3529e-26, -1.1987e+24, -1.1508e+25],\n",
       "        [ 4.6449e-32, -1.5308e-26,  3.9841e-18,  1.1292e-21,  3.8489e-08,\n",
       "         -2.8361e+01, -3.1611e+09, -2.5271e-27],\n",
       "        [-9.7359e-24,  2.7734e+28, -4.8315e-12,  3.0113e+32,  3.9759e+09,\n",
       "         -8.1162e+25,  1.6537e+08,  7.9032e-37],\n",
       "        [ 3.6381e-26,  1.4493e+38, -2.5790e+05, -2.4838e-34,  1.4304e+06,\n",
       "         -1.1399e-36, -2.0599e+23, -4.4556e-23],\n",
       "        [-4.8743e+26, -3.2384e-06,  8.0767e-16, -6.6715e+24,  3.5411e-24,\n",
       "          3.4405e+07,  4.9961e-37,  7.5914e+18],\n",
       "        [ 4.9612e+04, -1.9965e+25,  2.3972e+35, -9.3756e+10,  1.6764e-25,\n",
       "         -3.3598e-22,  3.7503e+10,  3.1760e+21],\n",
       "        [ 2.4561e-08,  1.1222e+35, -1.7132e+34,  4.8265e-19, -5.3818e-17,\n",
       "          4.3160e+01,  1.5106e+13,  4.2396e+25],\n",
       "        [-8.7586e+18,  2.2979e+16,  2.8853e-02, -5.4119e+12, -4.8991e+27,\n",
       "         -1.3176e+05, -1.5185e-35, -5.2663e-08],\n",
       "        [-4.9525e+22,  2.6456e+21, -6.6132e-16,  5.9137e+08, -6.8673e+30,\n",
       "         -1.1277e+03, -8.7609e+29,  5.9418e-28],\n",
       "        [-3.2768e-10, -5.1658e-14, -2.3504e+27,  3.2130e+06, -2.6921e+19,\n",
       "          7.4000e-20,  1.3070e-24, -1.1684e+29],\n",
       "        [-1.9485e+33, -1.6401e+27,  5.9458e-18, -1.1368e-24,  7.1163e-09,\n",
       "         -5.2176e+34,  1.3326e-02,  1.3937e-38],\n",
       "        [-3.4272e-07,  7.0026e+22,  3.3191e+23, -3.8086e-24, -3.1557e-28,\n",
       "         -1.4411e+19,  8.2169e-20, -2.2000e+35],\n",
       "        [-3.9428e+01, -4.0882e-06, -6.5982e-25,  1.6298e+12, -1.0176e+12,\n",
       "          3.0798e+06,  4.0689e+02,  1.3383e+38],\n",
       "        [-1.6804e+08,  3.0361e-01,  5.0893e-34,  1.2463e+18,  1.4580e+06,\n",
       "         -1.8916e+05, -9.8710e+36,  2.9459e+04],\n",
       "        [-2.7046e-11, -4.2445e+21,  5.9648e+01,  4.2992e+14, -3.0052e+05,\n",
       "          4.9578e+23,  1.8172e+25, -2.4127e-17],\n",
       "        [ 6.3310e+13,  1.4881e+32, -6.1006e-36, -6.1947e+11,  5.1969e+05,\n",
       "          1.7885e+25, -1.1800e-37, -4.9508e+04],\n",
       "        [ 1.3706e+17,  5.2504e-05,  8.2312e+13,  8.1923e+08,  5.6115e-25,\n",
       "          4.6359e+16,  1.9769e-20, -8.4875e-32],\n",
       "        [ 1.9187e+23,  9.1218e+25, -1.9125e-17,  5.3448e+23, -1.4947e+32,\n",
       "         -2.7552e+25, -1.3683e-25, -8.3450e-10],\n",
       "        [ 1.8771e+06,  7.4212e-37, -9.7615e-27,  5.3814e+07,  1.0501e-27,\n",
       "         -2.9047e+08, -5.6822e+03,  5.3259e-01]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(hqq_linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b2e23-d2b3-4acb-8b1c-b2fa5a22596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = m.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c42774-f290-4cad-aeec-470ab2c4adc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b74dd-d59e-4567-9d02-fa7438b70320",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q, meta = Quantizer.quantize(w, round_zero=True, optimize=True, view_as_float=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83602a4a-3cae-411e-803c-577c118992e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32]), torch.uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.shape, W_q.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116d9de-ef72-4aed-8f9e-c1657a8ddda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta['scale'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848f13f-7836-4869-85b7-558d5cb6fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dq = Quantizer.dequantize(W_q, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48eea18-8fbe-4d8d-bb78-738f3068709e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1196,  0.0683, -0.0960,  ..., -0.2410, -0.1544, -0.0864],\n",
       "         [-0.0278, -0.0483,  0.1141,  ...,  0.0873,  0.0023,  0.2011],\n",
       "         [ 0.0982, -0.0460,  0.0086,  ...,  0.0627, -0.0216, -0.0140],\n",
       "         ...,\n",
       "         [-0.0208,  0.1148, -0.0562,  ..., -0.0961,  0.2354,  0.2077],\n",
       "         [ 0.1820,  0.1345, -0.0235,  ...,  0.0432, -0.1749,  0.1510],\n",
       "         [-0.2125,  0.0024, -0.2045,  ..., -0.1916,  0.1080,  0.0231]]),\n",
       " tensor([[ 0.1224,  0.0717, -0.0930,  ..., -0.2524, -0.1595, -0.0937],\n",
       "         [-0.0320, -0.0627,  0.1289,  ...,  0.0945,  0.0091,  0.1919],\n",
       "         [ 0.0917, -0.0519,  0.0014,  ...,  0.0705, -0.0320,  0.0009],\n",
       "         ...,\n",
       "         [-0.0320,  0.1304, -0.0645,  ..., -0.0981,  0.2344,  0.1919],\n",
       "         [ 0.1841,  0.1334, -0.0301,  ...,  0.0382, -0.1595,  0.1584],\n",
       "         [-0.2222,  0.0016, -0.1934,  ..., -0.1943,  0.1057,  0.0273]],\n",
       "        dtype=torch.float16))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w_dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71a73c-3efe-4330-93e9-c9150ec36008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(390.0982)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(w - w_dq, p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cf493-865f-4b1c-9cb9-9565acedf4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight_quant_params': {'nbits': 4,\n",
       "  'channel_wise': True,\n",
       "  'group_size': 64,\n",
       "  'optimize': True,\n",
       "  'round_zero': True},\n",
       " 'scale_quant_params': None,\n",
       " 'zero_quant_params': None,\n",
       " 'offload_meta': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=False, offload_meta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30251dc6-d4dd-4665-804b-e4bca21ad2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_configs = [\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=False, offload_meta=False),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=True, quant_scale=False, offload_meta=False),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=True, offload_meta=False),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=True, quant_scale=True, offload_meta=False),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=True, quant_scale=True, offload_meta=True),\n",
    "                 BaseQuantizeConfig(nbits=4, group_size=64, quant_zero=False, quant_scale=False, offload_meta=True)\n",
    "]\n",
    "\n",
    "w_dqs = []\n",
    "for quant_cfg in quant_configs:\n",
    "    if quant_cfg['scale_quant_params']: \n",
    "        quant_cfg['scale_quant_params']['group_size'] = 8\n",
    "    if quant_cfg['zero_quant_params']: \n",
    "        if quant_cfg['offload_meta']:\n",
    "            quant_cfg['zero_quant_params']['group_size'] = 8\n",
    "            quant_cfg['zero_quant_params']['channel_wise'] = True\n",
    "        else:\n",
    "            quant_cfg['zero_quant_params']['group_size'] = None\n",
    "            quant_cfg['zero_quant_params']['channel_wise'] = False\n",
    "    mq = HQQLinear(m, quant_cfg, compute_dtype=torch.bfloat16, initialize=False)\n",
    "    HQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)\n",
    "    mq.initialize()\n",
    "    print(mq.W_q.dtype, mq.meta)\n",
    "    print()\n",
    "    w_dqs.append(mq.dequantize_aten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c665281-3cbe-492c-9654-eabbb1d98de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(390.9176, device='cuda:0'),\n",
       " tensor(390.5967, device='cuda:0'),\n",
       " tensor(390.7930, device='cuda:0'),\n",
       " tensor(390.1439, device='cuda:0'),\n",
       " tensor(392.0999, device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.norm(w.cuda() - w_dqs[0], p=0.7),\n",
    "torch.norm(w.cuda() - w_dqs[1], p=0.7),\n",
    "torch.norm(w.cuda() - w_dqs[2], p=0.7),\n",
    "torch.norm(w.cuda() - w_dqs[3], p=0.7),\n",
    "torch.norm(w.cuda() - w_dqs[4], p=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d24c3-36de-437a-b4ea-eb64a87d850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_hqq(model:nn.Module, quant_config, skip_modules:List[str]=[\"lm_head\"], **kwargs):\n",
    "    \"\"\"\n",
    "    Replace linear modules with a new Linear module.\n",
    "    Parameters:\n",
    "        model (`torch.nn.Module`):\n",
    "            Input model or `torch.nn.Module` as the function is run recursively.\n",
    "        quant_config (`Dict[str, Any]`):\n",
    "            The quantization configuration for the new linear module.\n",
    "        skip_modules (`List[str]`, *optional*, defaults to `lm_head`):\n",
    "            List of modules names not to convert. Defaults to `lm_head`.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear_hqq(module, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear) and name not in skip_modules:\n",
    "            model._modules[name] = HQQLinear(\n",
    "                module,\n",
    "                quant_config,\n",
    "                **kwargs\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3079ca1-0cd8-48d2-a199-1dc61589228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize_hqq(module:nn.Module, name:str, value:Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                                  skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False):\n",
    "    \"\"\"\n",
    "    Loads `value` tensor into submodule of `module`, optionally skipping `skip_names` and converting to `dtype`.\n",
    "\n",
    "    Quantizes `Params4bit` on `device` then places on \"cpu\" if low_memory=True or \"meta\" if is_meta_rank=True.\n",
    "    \"\"\"\n",
    "    def place_on_device(value):\n",
    "        if is_meta_rank:\n",
    "            device = 'meta'\n",
    "        elif low_memory:\n",
    "            device = 'cpu'\n",
    "        return value.to(device=device, dtype=dtype)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "        if verbose:\n",
    "            print(f\"Skipping {name} because it is in skip_names\")\n",
    "        return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "        submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "        print(f\"Module {module_key} not found:\\n{e}\")\n",
    "        return\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        if isinstance(submodule, HQQLinear):\n",
    "            if value_key == \"weight\":\n",
    "                # init meta weights as empty on cpu\n",
    "                submodule.linear_layer.to_empty(device=\"cpu\")\n",
    "                # copy pretrained weights\n",
    "                submodule.linear_layer.weight.data.copy_(value)\n",
    "                # quantize and update metadata\n",
    "                submodule.initialize()\n",
    "                \n",
    "                if is_meta_rank:\n",
    "                    setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to(\"meta\")))\n",
    "                elif low_memory:\n",
    "                    setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to(\"cpu\")))\n",
    "                submodule.in_gpu = False\n",
    "\n",
    "            if value_key == \"bias\":\n",
    "                raise ValueError(\"Bias not supported in HQQLinear yet!\")\n",
    "        \n",
    "            end = time.time()\n",
    "            if not is_meta_rank:\n",
    "                print(f\"Loaded HQQLinear quantized {module_key} in {end-start:.3f} seconds\")\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            param = submodule.get_parameter(value_key)\n",
    "            value = type(param)(place_on_device(value).data)\n",
    "\n",
    "    except AttributeError:\n",
    "        # it's a buffer\n",
    "        value = place_on_device(value)\n",
    "        pass\n",
    "    \n",
    "    setattr(submodule, value_key, value)\n",
    "    end = time.time()\n",
    "    torch.cuda.empty_cache()\n",
    "    if not is_meta_rank:\n",
    "        print(f\"Loaded {module_key} and {value_key} in {end-start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a374d8-85e1-4ea8-b078-0076b9682bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = hub.cached_file(model_name, SAFE_WEIGHTS_INDEX_NAME)\n",
    "files, _ = hub.get_checkpoint_shard_files(model_name, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cb0d3-fb22-43e6-be64-e1729ec83415",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = torch.bfloat16\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(model_name)\n",
    "cfg.use_cache = False\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "# cfg.num_hidden_layers = 8 # DEBUG\n",
    "\n",
    "# load model on meta device without calling init and replace nn.Linear with Linear4bit\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    # TODO: Tune BaseQuantizeConfig.\n",
    "    quant_config = BaseQuantizeConfig(nbits=4, \n",
    "                                      group_size=64, \n",
    "                                      quant_zero=True, \n",
    "                                      quant_scale=True, \n",
    "                                      offload_meta=True)\n",
    "    model.model = replace_linear_hqq(model.model, quant_config, device_n=torch.cuda.current_device(),\n",
    "                                    compute_dtype=compute_dtype, del_orig=True, initialize=False)     \n",
    "    HQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ddd41-10fc-449e-8bf3-92505b789ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 0\n",
      "Loaded model.embed_tokens and weight in 0.067 seconds\n",
      "Loaded model.layers.0.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.down_proj in 0.271 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.gate_proj in 0.243 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.up_proj in 0.236 seconds\n",
      "Loaded model.layers.0.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.k_proj in 0.065 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.o_proj in 0.062 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.q_proj in 0.063 seconds\n",
      "Loaded model.layers.0.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.v_proj in 0.060 seconds\n",
      "Loaded model.layers.1.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.down_proj in 0.239 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.gate_proj in 0.247 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.up_proj in 0.283 seconds\n",
      "Loaded model.layers.1.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.k_proj in 0.078 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.o_proj in 0.065 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.q_proj in 0.061 seconds\n",
      "Loaded model.layers.1.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.v_proj in 0.074 seconds\n",
      "Loaded model.layers.10.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.down_proj in 0.976 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.gate_proj in 1.748 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.up_proj in 1.001 seconds\n",
      "Loaded model.layers.10.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.k_proj in 0.358 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.o_proj in 0.383 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.q_proj in 0.390 seconds\n",
      "Loaded model.layers.10.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.v_proj in 0.394 seconds\n",
      "Loaded model.layers.11.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.down_proj in 0.971 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.gate_proj in 0.959 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.up_proj in 1.649 seconds\n",
      "Loaded model.layers.11.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.k_proj in 0.410 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.o_proj in 0.391 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.q_proj in 0.375 seconds\n",
      "Loaded model.layers.11.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.v_proj in 0.401 seconds\n",
      "Loaded model.layers.12.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.down_proj in 0.961 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.gate_proj in 0.927 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.up_proj in 0.967 seconds\n",
      "Loaded model.layers.12.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.k_proj in 0.418 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.o_proj in 1.161 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.q_proj in 0.388 seconds\n",
      "Loaded model.layers.12.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.v_proj in 0.385 seconds\n",
      "Loaded model.layers.13.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.down_proj in 0.953 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.gate_proj in 0.949 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.up_proj in 0.950 seconds\n",
      "Loaded model.layers.13.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.k_proj in 0.382 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.o_proj in 0.370 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.q_proj in 0.386 seconds\n",
      "Loaded model.layers.13.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.v_proj in 1.341 seconds\n",
      "Loaded model.layers.14.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.down_proj in 0.947 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.gate_proj in 0.946 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.up_proj in 0.984 seconds\n",
      "Loaded model.layers.14.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.k_proj in 0.386 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.o_proj in 0.387 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.q_proj in 0.378 seconds\n",
      "Loaded model.layers.14.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.v_proj in 0.376 seconds\n",
      "Loaded model.layers.15.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.down_proj in 1.806 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.gate_proj in 0.921 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.up_proj in 0.939 seconds\n",
      "Loaded model.layers.15.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.k_proj in 0.386 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.o_proj in 0.378 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.q_proj in 0.377 seconds\n",
      "Loaded model.layers.15.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.v_proj in 0.391 seconds\n",
      "Loaded model.layers.16.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.down_proj in 0.981 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.gate_proj in 1.731 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.up_proj in 0.962 seconds\n",
      "Loaded model.layers.16.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.k_proj in 0.387 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.o_proj in 0.382 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.q_proj in 0.361 seconds\n",
      "Loaded model.layers.16.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.v_proj in 0.365 seconds\n",
      "Loaded model.layers.17.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.down_proj in 0.938 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.gate_proj in 0.966 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.up_proj in 1.776 seconds\n",
      "Loaded model.layers.17.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.k_proj in 0.397 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.o_proj in 0.401 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.q_proj in 0.400 seconds\n",
      "Loaded model.layers.17.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.v_proj in 0.359 seconds\n",
      "Loaded model.layers.18.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.down_proj in 0.956 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.gate_proj in 0.964 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.up_proj in 0.946 seconds\n",
      "Loaded model.layers.18.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.k_proj in 0.429 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.o_proj in 1.168 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.q_proj in 0.363 seconds\n",
      "Loaded model.layers.18.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.v_proj in 0.367 seconds\n",
      "Loaded model.layers.19.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.down_proj in 0.962 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.gate_proj in 0.942 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.up_proj in 0.956 seconds\n",
      "Loaded model.layers.19.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.k_proj in 0.407 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.o_proj in 0.373 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.q_proj in 0.404 seconds\n",
      "Loaded model.layers.19.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.v_proj in 1.342 seconds\n",
      "Loaded model.layers.2.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.down_proj in 0.251 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.gate_proj in 0.241 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.up_proj in 0.238 seconds\n",
      "Loaded model.layers.2.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.k_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.q_proj in 0.093 seconds\n",
      "Loaded model.layers.2.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.v_proj in 0.094 seconds\n",
      "Loaded model.layers.20.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.down_proj in 0.951 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.gate_proj in 0.962 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.up_proj in 0.947 seconds\n",
      "Loaded model.layers.20.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.k_proj in 0.370 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.o_proj in 0.401 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.q_proj in 1.345 seconds\n",
      "Loaded model.layers.20.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.v_proj in 0.411 seconds\n",
      "Loaded model.layers.21.input_layernorm and weight in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.down_proj in 0.966 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.gate_proj in 0.923 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.up_proj in 0.971 seconds\n",
      "Loaded model.layers.21.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.k_proj in 0.391 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.o_proj in 0.376 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.q_proj in 0.398 seconds\n",
      "Loaded model.layers.21.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.v_proj in 0.408 seconds\n",
      "Loaded model.layers.22.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.down_proj in 1.392 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.gate_proj in 0.947 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.up_proj in 0.970 seconds\n",
      "Loaded model.layers.22.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.k_proj in 0.398 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.o_proj in 0.383 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.q_proj in 0.443 seconds\n",
      "Loaded model.layers.22.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.v_proj in 0.375 seconds\n",
      "Loaded model.layers.23.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.down_proj in 0.961 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.gate_proj in 1.622 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.up_proj in 0.976 seconds\n",
      "Loaded model.layers.23.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.k_proj in 0.362 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.o_proj in 0.406 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.q_proj in 0.391 seconds\n",
      "Loaded model.layers.23.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.v_proj in 0.384 seconds\n",
      "Loaded model.layers.3.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.down_proj in 0.250 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.gate_proj in 0.237 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.up_proj in 0.246 seconds\n",
      "Loaded model.layers.3.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.k_proj in 0.091 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.o_proj in 0.091 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.q_proj in 0.094 seconds\n",
      "Loaded model.layers.3.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.v_proj in 0.089 seconds\n",
      "Loaded model.layers.4.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.down_proj in 0.235 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.gate_proj in 0.253 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.up_proj in 0.233 seconds\n",
      "Loaded model.layers.4.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.k_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.o_proj in 0.093 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.q_proj in 0.095 seconds\n",
      "Loaded model.layers.4.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.v_proj in 0.092 seconds\n",
      "Loaded model.layers.5.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.down_proj in 1.329 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.gate_proj in 0.250 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.up_proj in 0.232 seconds\n",
      "Loaded model.layers.5.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.k_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.q_proj in 0.092 seconds\n",
      "Loaded model.layers.5.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.v_proj in 0.093 seconds\n",
      "Loaded model.layers.6.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.down_proj in 0.248 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.gate_proj in 0.242 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.up_proj in 0.233 seconds\n",
      "Loaded model.layers.6.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.k_proj in 0.098 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.q_proj in 0.095 seconds\n",
      "Loaded model.layers.6.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.v_proj in 0.091 seconds\n",
      "Loaded model.layers.7.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.down_proj in 0.250 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.gate_proj in 0.232 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.up_proj in 0.234 seconds\n",
      "Loaded model.layers.7.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.k_proj in 0.096 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.o_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.7.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.v_proj in 0.092 seconds\n",
      "Loaded model.layers.8.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.down_proj in 0.955 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.gate_proj in 2.081 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.up_proj in 0.952 seconds\n",
      "Loaded model.layers.8.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.k_proj in 0.378 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.o_proj in 0.388 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.q_proj in 0.365 seconds\n",
      "Loaded model.layers.8.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.v_proj in 0.383 seconds\n",
      "Loaded model.layers.9.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.down_proj in 0.943 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.gate_proj in 0.949 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.up_proj in 1.898 seconds\n",
      "Loaded model.layers.9.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.k_proj in 0.375 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.o_proj in 0.392 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.q_proj in 0.389 seconds\n",
      "Loaded model.layers.9.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.v_proj in 0.385 seconds\n",
      "Loaded lm_head and weight in 0.066 seconds\n",
      "Loaded model.layers.24.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.down_proj in 0.239 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.gate_proj in 0.252 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.up_proj in 0.248 seconds\n",
      "Loaded model.layers.24.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.k_proj in 0.096 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.o_proj in 0.093 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.q_proj in 0.101 seconds\n",
      "Loaded model.layers.24.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.v_proj in 0.095 seconds\n",
      "Loaded model.layers.25.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.down_proj in 0.238 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.gate_proj in 0.261 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.up_proj in 0.250 seconds\n",
      "Loaded model.layers.25.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.k_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.o_proj in 0.093 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.q_proj in 0.095 seconds\n",
      "Loaded model.layers.25.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.v_proj in 0.103 seconds\n",
      "Loaded model.layers.26.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.down_proj in 0.244 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.gate_proj in 0.241 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.up_proj in 1.210 seconds\n",
      "Loaded model.layers.26.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.k_proj in 0.098 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.o_proj in 0.093 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.26.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.v_proj in 0.152 seconds\n",
      "Loaded model.layers.27.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.down_proj in 0.242 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.gate_proj in 0.237 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.up_proj in 0.235 seconds\n",
      "Loaded model.layers.27.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.k_proj in 0.097 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.27.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.v_proj in 0.097 seconds\n",
      "Loaded model.layers.28.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.down_proj in 0.249 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.gate_proj in 0.236 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.up_proj in 0.235 seconds\n",
      "Loaded model.layers.28.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.k_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.o_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.28.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.v_proj in 0.095 seconds\n",
      "Loaded model.layers.29.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.down_proj in 0.254 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.gate_proj in 0.240 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.up_proj in 0.240 seconds\n",
      "Loaded model.layers.29.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.k_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.o_proj in 0.096 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.q_proj in 0.096 seconds\n",
      "Loaded model.layers.29.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.v_proj in 0.095 seconds\n",
      "Loaded model.layers.30.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.down_proj in 0.240 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.gate_proj in 0.236 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.up_proj in 0.236 seconds\n",
      "Loaded model.layers.30.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.k_proj in 0.097 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.o_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.q_proj in 0.098 seconds\n",
      "Loaded model.layers.30.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.v_proj in 0.097 seconds\n",
      "Loaded model.layers.31.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.down_proj in 1.292 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.gate_proj in 0.255 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.up_proj in 0.235 seconds\n",
      "Loaded model.layers.31.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.k_proj in 0.095 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.o_proj in 0.094 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.q_proj in 0.094 seconds\n",
      "Loaded model.layers.31.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.v_proj in 0.094 seconds\n",
      "Loaded model.norm and weight in 0.000 seconds\n",
      "Loaded model weights in 103.558 seconds\n"
     ]
    }
   ],
   "source": [
    "local_rank = 0\n",
    "low_memory = True\n",
    "load_param_skip_names = []\n",
    "rank = 0\n",
    "\n",
    "print(\"Loading model\", rank)\n",
    "start = time.time()\n",
    "for filename in files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    for name, param in weights.items():\n",
    "        load_and_quantize_hqq(model, name, param, dtype=torch.bfloat16, device=local_rank, skip_names=load_param_skip_names,\n",
    "                                is_meta_rank=(low_memory and rank!=0), verbose=True)\n",
    "print(f\"Loaded model weights in {time.time()-start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b5940-7eab-462d-9ef0-34465c678e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize_parallel(name_param, load_func, model, **kwargs):\n",
    "    name, param = name_param\n",
    "    load_func(model, name, param, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24ea0f-dc4a-470c-9f9d-6721d02f842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = torch.bfloat16\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(model_name)\n",
    "cfg.use_cache = False\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "# cfg.num_hidden_layers = 8 # DEBUG\n",
    "\n",
    "# load model on meta device without calling init and replace nn.Linear with Linear4bit\n",
    "with init_empty_weights():\n",
    "    model_fast = AutoModelForCausalLM.from_config(cfg)\n",
    "    # TODO: Tune BaseQuantizeConfig.\n",
    "    quant_config = BaseQuantizeConfig(nbits=4, \n",
    "                                      group_size=64, \n",
    "                                      quant_zero=True, \n",
    "                                      quant_scale=True, \n",
    "                                      offload_meta=True)\n",
    "    model_fast.model = replace_linear_hqq(model_fast.model, quant_config, device_n=torch.cuda.current_device(),\n",
    "                                          compute_dtype=compute_dtype, del_orig=True, initialize=False)     \n",
    "    HQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)\n",
    "model_fast.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a589e-d45e-49c2-aa6d-d52d8f175f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 0\n",
      "Loaded model.layers.0.input_layernorm and weight in 0.003 seconds\n",
      "Loaded model.layers.0.post_attention_layernorm and weight in 0.004 seconds\n",
      "Loaded model.layers.0.self_attn.rotary_emb and inv_freq in 0.032 seconds\n",
      "Loaded model.embed_tokens and weight in 0.203 seconds\n",
      "Loaded model.layers.1.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.k_proj in 1.016 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.gate_proj in 1.065 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.down_proj in 1.201 seconds\n",
      "Loaded model.layers.1.post_attention_layernorm and weight in 0.008 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.v_proj in 1.155 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.q_proj in 1.211 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.mlp.up_proj in 1.252 seconds\n",
      "Loaded model.layers.1.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.0.self_attn.o_proj in 1.386 seconds\n",
      "Loaded model.layers.10.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.down_proj in 1.298 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.o_proj in 0.402 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.v_proj in 1.823 seconds\n",
      "Loaded model.layers.10.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.k_proj in 2.032 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.up_proj in 2.188 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.self_attn.q_proj in 2.030 seconds\n",
      "Loaded model.layers.10.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.1.mlp.gate_proj in 2.246 seconds\n",
      "Loaded model.layers.11.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.down_proj in 2.360 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.gate_proj in 2.378 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.v_proj in 0.571 seconds\n",
      "Loaded model.layers.11.post_attention_layernorm and weight in 0.018 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.k_proj in 0.867 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.mlp.up_proj in 2.499 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.q_proj in 0.913 seconds\n",
      "Loaded HQQLinear quantized model.layers.10.self_attn.o_proj in 0.953 seconds\n",
      "Loaded model.layers.11.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded model.layers.12.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.down_proj in 0.997 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.k_proj in 0.773 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.o_proj in 1.063 seconds\n",
      "Loaded model.layers.12.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.v_proj in 0.863 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.down_proj in 0.906 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.self_attn.q_proj in 1.017 seconds\n",
      "Loaded model.layers.12.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.gate_proj in 1.516 seconds\n",
      "Loaded model.layers.13.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.11.mlp.up_proj in 1.494 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.gate_proj in 1.054 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.o_proj in 0.673 seconds\n",
      "Loaded model.layers.13.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.v_proj in 0.639 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.mlp.up_proj in 1.140 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.k_proj in 0.902 seconds\n",
      "Loaded model.layers.13.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.12.self_attn.q_proj in 0.934 seconds\n",
      "Loaded model.layers.14.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.down_proj in 0.963 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.up_proj in 0.965 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.mlp.gate_proj in 1.018 seconds\n",
      "Loaded model.layers.14.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.k_proj in 0.812 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.q_proj in 0.942 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.v_proj in 0.828 seconds\n",
      "Loaded model.layers.14.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.down_proj in 0.778 seconds\n",
      "Loaded HQQLinear quantized model.layers.13.self_attn.o_proj in 1.024 seconds\n",
      "Loaded model.layers.15.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.o_proj in 0.542 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.k_proj in 1.054 seconds\n",
      "Loaded model.layers.15.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.up_proj in 1.978 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.mlp.gate_proj in 2.594 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.v_proj in 2.121 seconds\n",
      "Loaded model.layers.15.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.14.self_attn.q_proj in 2.161 seconds\n",
      "Loaded model.layers.16.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.down_proj in 2.245 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.k_proj in 1.701 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.up_proj in 2.032 seconds\n",
      "Loaded model.layers.16.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.mlp.gate_proj in 2.374 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.o_proj in 1.184 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.v_proj in 0.704 seconds\n",
      "Loaded model.layers.16.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.15.self_attn.q_proj in 0.981 seconds\n",
      "Loaded model.layers.17.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.k_proj in 0.747 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.o_proj in 0.767 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.v_proj in 0.632 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.self_attn.q_proj in 0.738 seconds\n",
      "Loaded model.layers.17.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.gate_proj in 1.288 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.up_proj in 1.285 seconds\n",
      "Loaded HQQLinear quantized model.layers.16.mlp.down_proj in 1.503 seconds\n",
      "Loaded model.layers.17.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded model.layers.18.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.down_proj in 1.219 secondsLoaded HQQLinear quantized model.layers.17.mlp.gate_proj in 1.209 seconds\n",
      "\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.o_proj in 0.855 seconds\n",
      "Loaded model.layers.18.post_attention_layernorm and weight in 0.029 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.k_proj in 0.922 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.q_proj in 0.810 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.self_attn.v_proj in 0.849 seconds\n",
      "Loaded model.layers.18.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.17.mlp.up_proj in 1.460 seconds\n",
      "Loaded model.layers.19.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.down_proj in 1.052 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.k_proj in 0.612 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.v_proj in 0.581 seconds\n",
      "Loaded model.layers.19.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.o_proj in 1.007 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.self_attn.q_proj in 1.012 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.gate_proj in 1.167 seconds\n",
      "Loaded model.layers.19.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.18.mlp.up_proj in 1.337 seconds\n",
      "Loaded model.layers.2.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.down_proj in 1.059 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.gate_proj in 1.102 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.k_proj in 1.013 seconds\n",
      "Loaded model.layers.2.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.mlp.up_proj in 1.142 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.v_proj in 0.642 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.q_proj in 0.751 seconds\n",
      "Loaded model.layers.2.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.19.self_attn.o_proj in 0.763 seconds\n",
      "Loaded model.layers.20.input_layernorm and weight in 0.006 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.q_proj in 0.689 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.o_proj in 0.734 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.k_proj in 0.771 seconds\n",
      "Loaded model.layers.20.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.self_attn.v_proj in 0.785 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.down_proj in 1.439 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.up_proj in 2.440 seconds\n",
      "Loaded model.layers.20.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.2.mlp.gate_proj in 2.582 seconds\n",
      "Loaded model.layers.21.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.down_proj in 2.197 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.o_proj in 1.730 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.q_proj in 1.778 seconds\n",
      "Loaded model.layers.21.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.v_proj in 0.687 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.up_proj in 2.315 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.self_attn.k_proj in 2.336 seconds\n",
      "Loaded model.layers.21.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.down_proj in 1.099 seconds\n",
      "Loaded HQQLinear quantized model.layers.20.mlp.gate_proj in 2.594 seconds\n",
      "Loaded model.layers.22.input_layernorm and weight in 0.007 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.gate_proj in 1.152 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.o_proj in 0.748 seconds\n",
      "Loaded model.layers.22.post_attention_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.k_proj in 0.829 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.mlp.up_proj in 1.203 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.v_proj in 0.771 seconds\n",
      "Loaded model.layers.22.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.21.self_attn.q_proj in 0.923 seconds\n",
      "Loaded model.layers.23.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.down_proj in 0.902 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.q_proj in 0.727 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.o_proj in 0.917 seconds\n",
      "Loaded model.layers.23.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.v_proj in 0.663 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.gate_proj in 1.293 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.self_attn.k_proj in 1.033 seconds\n",
      "Loaded model.layers.23.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.22.mlp.up_proj in 1.217 seconds\n",
      "Loaded model.layers.3.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.v_proj in 0.604 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.o_proj in 0.804 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.down_proj in 1.380 seconds\n",
      "Loaded model.layers.3.post_attention_layernorm and weight in 0.021 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.up_proj in 1.099 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.k_proj in 1.108 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.mlp.gate_proj in 1.493 seconds\n",
      "Loaded model.layers.3.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.down_proj in 1.088 seconds\n",
      "Loaded model.layers.4.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.23.self_attn.q_proj in 1.148 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.v_proj in 0.351 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.gate_proj in 1.057 seconds\n",
      "Loaded model.layers.4.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.o_proj in 0.767 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.k_proj in 0.978 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.self_attn.q_proj in 0.947 seconds\n",
      "Loaded model.layers.4.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.3.mlp.up_proj in 1.494 seconds\n",
      "Loaded model.layers.5.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.gate_proj in 1.188 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.down_proj in 1.268 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.q_proj in 0.671 seconds\n",
      "Loaded model.layers.5.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.k_proj in 2.018 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.o_proj in 1.968 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.self_attn.v_proj in 1.807 seconds\n",
      "Loaded model.layers.5.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.4.mlp.up_proj in 2.425 seconds\n",
      "Loaded model.layers.6.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.up_proj in 1.880 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.q_proj in 0.679 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.o_proj in 0.709 seconds\n",
      "Loaded model.layers.6.post_attention_layernorm and weight in 0.007 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.v_proj in 0.771 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.self_attn.k_proj in 2.119 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.gate_proj in 2.472 seconds\n",
      "Loaded model.layers.6.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.5.mlp.down_proj in 2.591 seconds\n",
      "Loaded model.layers.7.input_layernorm and weight in 0.003 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.down_proj in 1.020 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.q_proj in 0.825 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.up_proj in 1.041 seconds\n",
      "Loaded model.layers.7.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.k_proj in 1.067 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.o_proj in 0.937 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.self_attn.v_proj in 0.784 seconds\n",
      "Loaded model.layers.7.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.6.mlp.gate_proj in 1.527 seconds\n",
      "Loaded model.layers.8.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.gate_proj in 1.046 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.down_proj in 1.137 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.v_proj in 0.752 seconds\n",
      "Loaded model.layers.8.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.q_proj in 0.925 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.mlp.up_proj in 1.073 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.o_proj in 1.033 seconds\n",
      "Loaded model.layers.8.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.7.self_attn.k_proj in 1.133 seconds\n",
      "Loaded model.layers.9.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.down_proj in 1.100 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.gate_proj in 1.235 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.v_proj in 0.645 seconds\n",
      "Loaded model.layers.9.post_attention_layernorm and weight in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.o_proj in 0.756 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.mlp.up_proj in 1.346 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.k_proj in 0.991 seconds\n",
      "Loaded model.layers.9.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.8.self_attn.q_proj in 0.897 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.down_proj in 1.155 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.o_proj in 0.619 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.k_proj in 0.670 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.q_proj in 0.528 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.gate_proj in 0.970 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.self_attn.v_proj in 0.566 seconds\n",
      "Loaded HQQLinear quantized model.layers.9.mlp.up_proj in 0.756 seconds\n",
      "Loaded lm_head and weight in 0.330 secondsLoaded model.layers.24.input_layernorm and weight in 0.006 seconds\n",
      "\n",
      "Loaded model.layers.24.post_attention_layernorm and weight in 0.016 seconds\n",
      "Loaded model.layers.24.self_attn.rotary_emb and inv_freq in 0.001 seconds\n",
      "Loaded model.layers.25.input_layernorm and weight in 0.008 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.o_proj in 1.008 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.v_proj in 1.013 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.down_proj in 1.464 seconds\n",
      "Loaded model.layers.25.post_attention_layernorm and weight in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.k_proj in 1.130 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.up_proj in 1.169 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.self_attn.q_proj in 1.338 seconds\n",
      "Loaded model.layers.25.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.24.mlp.gate_proj in 1.436 seconds\n",
      "Loaded model.layers.26.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.down_proj in 1.402 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.k_proj in 0.522 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.o_proj in 0.653 seconds\n",
      "Loaded model.layers.26.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.up_proj in 0.961 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.q_proj in 0.841 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.mlp.gate_proj in 1.216 seconds\n",
      "Loaded model.layers.26.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.25.self_attn.v_proj in 0.897 seconds\n",
      "Loaded model.layers.27.input_layernorm and weight in 0.008 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.gate_proj in 0.943 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.k_proj in 0.647 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.q_proj in 0.673 seconds\n",
      "Loaded model.layers.27.post_attention_layernorm and weight in 0.003 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.up_proj in 1.228 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.o_proj in 0.894 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.mlp.down_proj in 1.497 seconds\n",
      "Loaded model.layers.27.self_attn.rotary_emb and inv_freq in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.26.self_attn.v_proj in 0.723 seconds\n",
      "Loaded model.layers.28.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.gate_proj in 1.199 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.up_proj in 1.211 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.o_proj in 0.845 seconds\n",
      "Loaded model.layers.28.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.k_proj in 1.028 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.q_proj in 0.857 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.self_attn.v_proj in 0.933 seconds\n",
      "Loaded model.layers.28.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.27.mlp.down_proj in 1.740 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.down_proj in 1.025 seconds\n",
      "Loaded model.layers.29.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.q_proj in 0.835 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.o_proj in 0.862 seconds\n",
      "Loaded model.layers.29.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.v_proj in 0.866 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.up_proj in 1.158 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.self_attn.k_proj in 1.129 seconds\n",
      "Loaded model.layers.29.self_attn.rotary_emb and inv_freq in 0.002 seconds\n",
      "Loaded HQQLinear quantized model.layers.28.mlp.gate_proj in 1.404 seconds\n",
      "Loaded model.layers.30.input_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.gate_proj in 1.084 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.down_proj in 1.131 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.v_proj in 1.754 seconds\n",
      "Loaded model.layers.30.post_attention_layernorm and weight in 0.003 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.k_proj in 2.057 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.o_proj in 1.930 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.self_attn.q_proj in 2.034 seconds\n",
      "Loaded model.layers.30.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.29.mlp.up_proj in 2.393 seconds\n",
      "Loaded model.layers.31.input_layernorm and weight in 0.001 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.up_proj in 1.942 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.gate_proj in 2.062 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.mlp.down_proj in 2.221 seconds\n",
      "Loaded model.layers.31.post_attention_layernorm and weight in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.o_proj in 0.757 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.v_proj in 0.664 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.k_proj in 1.169 seconds\n",
      "Loaded model.layers.31.self_attn.rotary_emb and inv_freq in 0.000 seconds\n",
      "Loaded HQQLinear quantized model.layers.30.self_attn.q_proj in 1.238 seconds\n",
      "Loaded model.norm and weight in 0.015 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.k_proj in 0.725 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.q_proj in 0.440 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.o_proj in 0.576 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.gate_proj in 0.969 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.down_proj in 1.118 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.mlp.up_proj in 0.988 seconds\n",
      "Loaded HQQLinear quantized model.layers.31.self_attn.v_proj in 0.358 seconds\n",
      "Loaded model weights in 36.317 seconds\n"
     ]
    }
   ],
   "source": [
    "local_rank = 0\n",
    "low_memory = True\n",
    "load_param_skip_names = []\n",
    "rank = 0\n",
    "\n",
    "print(\"Loading model\", rank)\n",
    "start = time.time()\n",
    "for filename in files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    parallel(load_and_quantize_parallel, weights.items(), n_workers=8, threadpool=True, \n",
    "             load_func=load_and_quantize_hqq, model=model_fast, \n",
    "             dtype=torch.bfloat16, device=local_rank, skip_names=load_param_skip_names, \n",
    "             is_meta_rank=(low_memory and rank!=0), verbose=True)\n",
    "print(f\"Loaded model weights in {time.time()-start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d4c27-90d9-4492-a068-6c2afd8ecfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n1,p1), (n2,p2) in zip(model.named_parameters(), model_fast.named_parameters()):\n",
    "    if n1 == n2:\n",
    "        if \"proj\" in n1:\n",
    "            assert torch.allclose(p1.view(torch.uint8), p2.view(torch.uint8))\n",
    "        else:\n",
    "            assert torch.allclose(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64248a-632e-4371-b9b1-813b27b2da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HQQDORA(nn.Module):\n",
    "    def __init__(self, base_layer, lora_rank, lora_dropout):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        dtype = getattr(base_layer, \"compute_dtype\", next(base_layer.parameters()).dtype)\n",
    "        device = next(base_layer.parameters()).device\n",
    "        \n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(lora_rank).float())\n",
    "        self.lora_A = nn.Parameter(torch.randn(base_layer.out_features, lora_rank).to(device=device,dtype=dtype)*std_dev)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(lora_rank, base_layer.in_features).to(device=device,dtype=dtype))\n",
    "\n",
    "        self.m = nn.Parameter(self.base_layer.dequantize_aten().clone().norm(p=2, dim=0, keepdim=True))\n",
    "    \n",
    "    def forward(self, x):        \n",
    "\n",
    "        lora = torch.matmul(self.lora_A, self.lora_B)\n",
    "        adapted = self.base_layer.dequantize_aten() + lora\n",
    "        column_norm = adapted.norm(p=2, dim=0, keepdim=True)\n",
    "\n",
    "        assert torch.equal(self.m, column_norm)\n",
    "        \n",
    "        calc_weights = self.m * (adapted / column_norm)\n",
    "\n",
    "        assert torch.allclose(self.base_layer.dequantize_aten(), calc_weights)\n",
    "        \n",
    "        return torch.matmul(x, calc_weights.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236c010-b7d8-42cb-b109-9fbf35161476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9985, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config = BaseQuantizeConfig(nbits=4, \n",
    "                                  group_size=64, \n",
    "                                  quant_zero=True, \n",
    "                                  quant_scale=True, \n",
    "                                  offload_meta=True)\n",
    "\n",
    "base_layer = HQQLinear(nn.Linear(128,256), quant_config, compute_dtype=torch.float32)\n",
    "dora = HQQDORA(base_layer, 8, 0)\n",
    "x = torch.randn(2,4,128).cuda()\n",
    "torch.isclose(dora(x), torch.matmul(x, base_layer.dequantize_aten().t())).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634a32d-c471-4c31-8d70-68c87b1b85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoRALayer(nn.Module):\n",
    "    def __init__(self, d_in, d_out, rank=4, weight=None, bias=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if weight is not None:\n",
    "            self.weight = nn.Parameter(weight, requires_grad=False)\n",
    "        else:\n",
    "            self.weight = nn.Parameter(torch.Tensor(d_out, d_in), requires_grad=False)\n",
    "\n",
    "        if bias is not None:\n",
    "            self.bias = nn.Parameter(bias, requires_grad=False)\n",
    "        else:\n",
    "            self.bias = nn.Parameter(torch.Tensor(d_out), requires_grad=False)\n",
    "\n",
    "        # m = Magnitude column-wise across output dimension\n",
    "        self.m = nn.Parameter(self.weight.norm(p=2, dim=0, keepdim=True))\n",
    "        \n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.lora_A = nn.Parameter(torch.randn(d_out, rank)*std_dev)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, d_in))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = torch.matmul(self.lora_A, self.lora_B)\n",
    "        adapted = self.weight + lora\n",
    "        column_norm = adapted.norm(p=2, dim=0, keepdim=True)\n",
    "        norm_adapted = adapted / column_norm\n",
    "        calc_weights = self.m * norm_adapted\n",
    "        return F.linear(x, calc_weights, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4a14c-6228-4d79-afb2-f3ef6e5ca78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(128,256,bias=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f747f5-ce80-4766-8be0-ebf249e15b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dora = DoRALayer(128,256,weight=m.weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e60aaa-43aa-4390-831e-35d8ad2c0480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2144, -0.1476, -0.0111,  ...,  0.3745,  0.1425, -0.1142],\n",
       "         [ 0.3202, -0.2039,  0.7589,  ..., -0.2859, -1.4159,  0.9623],\n",
       "         [-0.1714,  0.4437, -0.3377,  ...,  1.4839,  1.1261,  0.1933],\n",
       "         [-0.5015,  0.3812,  1.3170,  ...,  0.3666,  0.0282,  0.3237]],\n",
       "\n",
       "        [[ 0.2638,  0.0497,  0.2547,  ...,  0.5097,  0.0237,  0.8447],\n",
       "         [ 0.2788, -0.1295, -0.6743,  ...,  0.1924,  1.0936,  0.3154],\n",
       "         [-0.4722,  0.2377,  0.0317,  ..., -0.6017, -0.4683, -0.1920],\n",
       "         [-0.4582,  0.4022, -0.5113,  ...,  0.9794,  1.3093, -0.3878]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff84eeb-e792-4318-b23a-93c2552ea580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2144, -0.1476, -0.0111,  ...,  0.3745,  0.1425, -0.1142],\n",
       "         [ 0.3202, -0.2039,  0.7589,  ..., -0.2859, -1.4159,  0.9623],\n",
       "         [-0.1714,  0.4437, -0.3377,  ...,  1.4839,  1.1261,  0.1933],\n",
       "         [-0.5015,  0.3812,  1.3170,  ...,  0.3666,  0.0282,  0.3237]],\n",
       "\n",
       "        [[ 0.2638,  0.0497,  0.2547,  ...,  0.5097,  0.0237,  0.8447],\n",
       "         [ 0.2788, -0.1295, -0.6743,  ...,  0.1924,  1.0936,  0.3154],\n",
       "         [-0.4722,  0.2377,  0.0317,  ..., -0.6017, -0.4683, -0.1920],\n",
       "         [-0.4582,  0.4022, -0.5113,  ...,  0.9794,  1.3093, -0.3878]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f2ef1-e5fd-4213-b0fd-c612ec63df76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b422b0-1c68-4f46-a5fd-90fc1391bd35",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9020f4-2d6e-4655-8539-ea1a249d2463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhqq_aten package available. Set backend to HQQBackend.ATEN for faster inference and HQQBackend.ATEN_BACKPROP for faster training!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88be82-022b-4682-bc13-6aa7ef93a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = torch.bfloat16\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(model_name)\n",
    "cfg.use_cache = False\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "cfg.num_hidden_layers = 2 # DEBUG\n",
    "\n",
    "# load model on meta device without calling init and replace nn.Linear with Linear4bit\n",
    "model = AutoModelForCausalLM.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27614d-2d47-40d5-b044-c1d7e2e967b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed69d9-fc38-4941-8baf-802eaa60c633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 144.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "quant_config = BaseQuantizeConfig(nbits=4, group_size=64, view_as_float=True)\n",
    "HQQModelForCausalLM.quantize_model_(model, quant_config, compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da003c17-e351-4b5e-bb64-b2234bb2eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.q_proj.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376d0cc-ae62-4858-bbfa-66018b309743",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.q_proj.W_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823fbac-7562-4878-a491-b644b1aab051",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(\"/weka/home-keremturgutlu/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a97c4c-44e0-45b2-9fe7-979a6fafa080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "quantized_config = json.load(open(\"/weka/home-keremturgutlu/models/config.json\"))\n",
    "quantized_weights = torch.load(\"/weka/home-keremturgutlu/models/qmodel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2a4bd-1262-47c8-859b-325d52402ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc0b44-8aa1-4a5a-aaa1-5423f934ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(quantized_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a033ed-216f-48e1-b1ef-bcf728cc4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_weights['model.layers.0.self_attn.q_proj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39aa72-e5cb-4b24-bf12-e50690b6bc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1804.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 364.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model_qt = HQQModelForCausalLM.from_quantized(\"/weka/home-keremturgutlu/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938d280-b7fb-44b2-918a-39e1158a1f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'model',\n",
       " 'model.embed_tokens',\n",
       " 'model.layers',\n",
       " 'model.layers.0',\n",
       " 'model.layers.0.self_attn',\n",
       " 'model.layers.0.self_attn.q_proj',\n",
       " 'model.layers.0.self_attn.k_proj',\n",
       " 'model.layers.0.self_attn.v_proj',\n",
       " 'model.layers.0.self_attn.o_proj',\n",
       " 'model.layers.0.self_attn.rotary_emb',\n",
       " 'model.layers.0.mlp',\n",
       " 'model.layers.0.mlp.gate_proj',\n",
       " 'model.layers.0.mlp.up_proj',\n",
       " 'model.layers.0.mlp.down_proj',\n",
       " 'model.layers.0.mlp.act_fn',\n",
       " 'model.layers.0.input_layernorm',\n",
       " 'model.layers.0.post_attention_layernorm',\n",
       " 'model.layers.1',\n",
       " 'model.layers.1.self_attn',\n",
       " 'model.layers.1.self_attn.q_proj',\n",
       " 'model.layers.1.self_attn.k_proj',\n",
       " 'model.layers.1.self_attn.v_proj',\n",
       " 'model.layers.1.self_attn.o_proj',\n",
       " 'model.layers.1.self_attn.rotary_emb',\n",
       " 'model.layers.1.mlp',\n",
       " 'model.layers.1.mlp.gate_proj',\n",
       " 'model.layers.1.mlp.up_proj',\n",
       " 'model.layers.1.mlp.down_proj',\n",
       " 'model.layers.1.mlp.act_fn',\n",
       " 'model.layers.1.input_layernorm',\n",
       " 'model.layers.1.post_attention_layernorm',\n",
       " 'model.norm',\n",
       " 'lm_head']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(n for n,p in model_qt.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927c679-b497-4242-9f63-7fdfc5f4bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_state_dict(v1,v2):\n",
    "    if isinstance(v1, torch.Tensor):\n",
    "        assert torch.isclose(v1,v2, rtol=1e-5).float().mean().item() > 0.99\n",
    "    if isinstance(v1, dict):\n",
    "        for _k,_v in v1.items():\n",
    "            if isinstance(_v, torch.Tensor):\n",
    "                assert torch.equal(_v, v2[_k])\n",
    "            else:\n",
    "                assert _v == v2[_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df6236-ad25-4b1d-9fc9-41c6b20f1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    \n",
    "    module_key, _, value_key = n.rpartition('.')\n",
    "    \n",
    "    d1 = model.get_submodule(module_key).state_dict()\n",
    "    d2 = model_qt.get_submodule(module_key).state_dict()\n",
    "    \n",
    "    for (k1,v1),(k2,v2) in zip(d1.items(), d2.items()):\n",
    "        assert k1 == k2\n",
    "        assert_state_dict(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b143db-8a80-452d-9f1a-8db939625046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "from safetensors.torch import save_file\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa721a7b-a813-47f3-92aa-0f2818c4b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init = safetensors.torch.load_file(\"/weka/home-keremturgutlu/models/hqq_lora_dummy_init/model_state_dict.safetensors\")\n",
    "weights = safetensors.torch.load_file(\"/weka/home-keremturgutlu/models/hqq_lora_dummy/model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a25d7-976e-4016-bead-ee5d296bd00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.down_proj.lora_AB.0.weight': tensor([[-9.1553e-03,  6.0120e-03, -1.9379e-03,  ..., -7.8201e-04,\n",
       "          -6.0120e-03,  7.2861e-04],\n",
       "         [ 1.8616e-03,  8.5449e-03,  6.9275e-03,  ..., -1.3885e-03,\n",
       "           7.6599e-03,  3.2043e-03],\n",
       "         [ 7.6599e-03,  3.3417e-03,  4.3030e-03,  ...,  4.6082e-03,\n",
       "          -5.3711e-03, -1.1139e-03],\n",
       "         ...,\n",
       "         [-4.0894e-03, -4.3945e-03,  8.1787e-03,  ...,  5.4321e-03,\n",
       "          -8.4839e-03, -8.4839e-03],\n",
       "         [-6.6757e-05,  3.9368e-03,  6.0272e-04,  ..., -5.1270e-03,\n",
       "          -4.8218e-03, -5.3711e-03],\n",
       "         [ 4.9744e-03,  1.6556e-03, -1.5640e-03,  ...,  4.1504e-03,\n",
       "           7.7515e-03,  6.8359e-03]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.down_proj.lora_AB.1.weight': tensor([[-6.2943e-05,  7.9155e-05, -7.9632e-05,  ...,  7.5340e-05,\n",
       "           7.9632e-05,  7.6294e-05],\n",
       "         [-6.8665e-05, -7.5817e-05,  7.2002e-05,  ...,  6.6757e-05,\n",
       "          -7.6771e-05, -7.1526e-05],\n",
       "         [ 5.6744e-05,  7.1049e-05,  3.7432e-05,  ..., -6.0320e-05,\n",
       "           7.2956e-05,  6.6757e-05],\n",
       "         ...,\n",
       "         [ 7.4387e-05,  8.0109e-05, -8.0109e-05,  ...,  7.5817e-05,\n",
       "           7.9155e-05,  7.8678e-05],\n",
       "         [-7.5817e-05, -7.6771e-05, -7.2002e-05,  ..., -2.3365e-05,\n",
       "          -7.7248e-05, -7.4863e-05],\n",
       "         [-7.5817e-05, -7.9155e-05,  7.9632e-05,  ..., -7.4387e-05,\n",
       "          -7.9632e-05, -7.8201e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.gate_proj.lora_AB.0.weight': tensor([[ 0.0073,  0.0133, -0.0061,  ..., -0.0149, -0.0030, -0.0018],\n",
       "         [ 0.0068, -0.0081, -0.0049,  ...,  0.0010,  0.0132,  0.0133],\n",
       "         [ 0.0018,  0.0052,  0.0026,  ..., -0.0033, -0.0059,  0.0154],\n",
       "         ...,\n",
       "         [ 0.0055, -0.0043,  0.0087,  ..., -0.0020,  0.0033, -0.0044],\n",
       "         [-0.0128, -0.0116,  0.0094,  ...,  0.0137,  0.0044, -0.0029],\n",
       "         [ 0.0077,  0.0098,  0.0051,  ..., -0.0092, -0.0049, -0.0122]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.gate_proj.lora_AB.1.weight': tensor([[ 6.4850e-05,  6.5327e-05,  4.8876e-05,  ..., -6.1512e-05,\n",
       "          -6.6280e-05,  1.1921e-06],\n",
       "         [ 7.6294e-05, -7.4387e-05, -7.2002e-05,  ...,  7.8678e-05,\n",
       "          -7.8678e-05,  6.2466e-05],\n",
       "         [-5.6744e-05, -3.1710e-05,  2.6226e-05,  ...,  5.3644e-05,\n",
       "           4.9353e-05,  4.8637e-05],\n",
       "         ...,\n",
       "         [ 6.4850e-05,  4.3392e-05, -7.0572e-05,  ...,  7.5817e-05,\n",
       "          -7.5340e-05,  3.7432e-05],\n",
       "         [-4.5300e-05, -3.4809e-05,  6.9618e-05,  ..., -7.2956e-05,\n",
       "           7.2479e-05, -1.7881e-05],\n",
       "         [ 5.6744e-05, -4.6968e-05, -4.1723e-05,  ...,  6.9141e-05,\n",
       "          -6.2466e-05, -2.6345e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.up_proj.lora_AB.0.weight': tensor([[ 0.0087,  0.0010,  0.0009,  ..., -0.0128,  0.0009, -0.0126],\n",
       "         [-0.0003, -0.0109,  0.0051,  ...,  0.0079,  0.0143,  0.0076],\n",
       "         [ 0.0022, -0.0090, -0.0013,  ...,  0.0071, -0.0138, -0.0023],\n",
       "         ...,\n",
       "         [-0.0103, -0.0153, -0.0061,  ..., -0.0076, -0.0004,  0.0093],\n",
       "         [ 0.0066,  0.0066, -0.0040,  ...,  0.0046, -0.0043, -0.0063],\n",
       "         [ 0.0049, -0.0040, -0.0118,  ...,  0.0065,  0.0112,  0.0110]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.up_proj.lora_AB.1.weight': tensor([[-3.7909e-05, -6.8665e-05, -7.6294e-05,  ...,  6.9141e-05,\n",
       "           6.9618e-05,  7.4387e-05],\n",
       "         [-6.0081e-05,  7.7724e-05,  7.8678e-05,  ..., -7.5817e-05,\n",
       "          -7.6771e-05, -7.8201e-05],\n",
       "         [-6.3419e-05, -6.9618e-05, -7.7248e-05,  ...,  6.9618e-05,\n",
       "           7.0572e-05,  7.5817e-05],\n",
       "         ...,\n",
       "         [-6.2943e-05,  6.1512e-05,  6.5327e-05,  ..., -3.7432e-05,\n",
       "          -5.5075e-05, -6.2466e-05],\n",
       "         [ 4.5300e-05, -6.1512e-05, -6.9141e-05,  ...,  5.0068e-05,\n",
       "           5.7936e-05,  6.5804e-05],\n",
       "         [-2.7776e-05,  7.1526e-05,  7.6294e-05,  ..., -6.6757e-05,\n",
       "          -7.1049e-05, -7.4387e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.k_proj.lora_AB.0.weight': tensor([[ 9.3994e-03, -5.8594e-03,  1.2085e-02,  ..., -5.6152e-03,\n",
       "           1.2573e-02, -1.9531e-03],\n",
       "         [-9.6436e-03,  8.5449e-04,  5.6152e-03,  ..., -1.2207e-04,\n",
       "          -1.3672e-02,  5.6152e-03],\n",
       "         [-2.4414e-04, -9.0332e-03,  1.5259e-02,  ..., -7.3242e-03,\n",
       "           1.2451e-02,  1.4893e-02],\n",
       "         ...,\n",
       "         [-1.2085e-02,  1.0620e-02,  1.5503e-02,  ...,  1.1841e-02,\n",
       "           8.9111e-03, -4.6387e-03],\n",
       "         [ 1.2573e-02, -8.4229e-03, -1.0376e-02,  ..., -1.3794e-02,\n",
       "           1.5381e-02,  8.5449e-04],\n",
       "         [ 3.7842e-03, -8.0566e-03,  9.0804e-08,  ...,  7.5251e-07,\n",
       "           1.2207e-04, -1.0986e-03]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.k_proj.lora_AB.1.weight': tensor([[-2.3842e-05, -2.2292e-05, -3.2187e-05,  ...,  3.5524e-05,\n",
       "          -3.4094e-05, -1.0967e-05],\n",
       "         [ 1.1444e-05,  2.6107e-05,  3.2425e-05,  ..., -3.6240e-05,\n",
       "           3.7193e-05,  1.1206e-05],\n",
       "         [-2.9325e-05, -1.8954e-05, -3.6955e-05,  ...,  4.2200e-05,\n",
       "          -3.5048e-05,  3.7402e-06],\n",
       "         ...,\n",
       "         [ 3.1948e-05,  4.5300e-06,  1.0192e-05,  ..., -9.8944e-06,\n",
       "           2.6941e-05,  7.8678e-06],\n",
       "         [-5.2929e-05, -1.3590e-05, -2.5392e-05,  ...,  3.3855e-05,\n",
       "          -5.3644e-05, -2.2173e-05],\n",
       "         [ 3.5286e-05, -1.1623e-06,  1.7524e-05,  ..., -2.5988e-05,\n",
       "           4.7445e-05,  2.3961e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.q_proj.lora_AB.0.weight': tensor([[-0.0155, -0.0035,  0.0033,  ..., -0.0059,  0.0007, -0.0093],\n",
       "         [ 0.0115, -0.0034,  0.0081,  ...,  0.0051,  0.0127, -0.0049],\n",
       "         [-0.0087,  0.0144,  0.0103,  ..., -0.0065,  0.0093,  0.0146],\n",
       "         ...,\n",
       "         [ 0.0151, -0.0115, -0.0122,  ..., -0.0070, -0.0148, -0.0117],\n",
       "         [-0.0115, -0.0093, -0.0039,  ..., -0.0133,  0.0023,  0.0063],\n",
       "         [-0.0115,  0.0020,  0.0040,  ..., -0.0060, -0.0133,  0.0048]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.q_proj.lora_AB.1.weight': tensor([[ 1.3888e-05, -2.3246e-05,  2.0504e-05,  ...,  3.6478e-05,\n",
       "           1.6332e-05,  1.6570e-05],\n",
       "         [-3.5763e-05,  3.3379e-05, -3.5048e-05,  ..., -4.7922e-05,\n",
       "          -2.2650e-05, -3.0756e-05],\n",
       "         [ 3.5524e-05, -7.8082e-06,  1.1206e-05,  ...,  2.5749e-05,\n",
       "          -1.3113e-05,  2.5034e-05],\n",
       "         ...,\n",
       "         [-6.2943e-05,  5.7936e-05, -4.1246e-05,  ..., -6.4850e-05,\n",
       "           3.9339e-05, -6.4373e-05],\n",
       "         [ 5.0306e-05, -1.9185e-07,  4.5538e-05,  ...,  5.2214e-05,\n",
       "          -3.9101e-05,  4.6730e-05],\n",
       "         [ 1.1802e-05,  3.9101e-05, -3.6716e-05,  ..., -5.8651e-05,\n",
       "          -4.5776e-05, -3.1948e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.v_proj.lora_AB.0.weight': tensor([[-0.0106,  0.0065, -0.0109,  ...,  0.0062,  0.0038,  0.0002],\n",
       "         [-0.0055,  0.0057,  0.0050,  ..., -0.0070, -0.0024, -0.0087],\n",
       "         [ 0.0095,  0.0143,  0.0037,  ...,  0.0115,  0.0078, -0.0049],\n",
       "         ...,\n",
       "         [-0.0072,  0.0030,  0.0105,  ..., -0.0118,  0.0081, -0.0072],\n",
       "         [-0.0040, -0.0140, -0.0146,  ..., -0.0135, -0.0066, -0.0125],\n",
       "         [ 0.0120,  0.0150,  0.0098,  ..., -0.0070,  0.0013,  0.0040]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.v_proj.lora_AB.1.weight': tensor([[ 7.9155e-05, -7.3910e-05, -6.2466e-05,  ...,  7.9632e-05,\n",
       "           7.8678e-05,  7.9632e-05],\n",
       "         [ 7.8201e-05, -7.6294e-05,  7.6294e-05,  ...,  7.8678e-05,\n",
       "           7.8678e-05,  7.9632e-05],\n",
       "         [-6.9618e-05, -7.8678e-05,  5.3883e-05,  ..., -7.8678e-05,\n",
       "          -7.9155e-05, -7.9632e-05],\n",
       "         ...,\n",
       "         [ 5.9128e-05, -7.6294e-05,  7.0572e-05,  ..., -3.2425e-05,\n",
       "          -7.6294e-05, -7.6294e-05],\n",
       "         [ 7.6771e-05,  3.5048e-05,  6.8665e-05,  ...,  7.8678e-05,\n",
       "           7.6771e-05,  7.9155e-05],\n",
       "         [-7.9155e-05,  7.2002e-05, -7.4863e-05,  ..., -7.9632e-05,\n",
       "          -7.9632e-05, -7.9632e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.down_proj.lora_AB.0.weight': tensor([[-0.0056,  0.0058,  0.0009,  ...,  0.0093,  0.0085, -0.0095],\n",
       "         [ 0.0070,  0.0086,  0.0059,  ...,  0.0032, -0.0076,  0.0060],\n",
       "         [-0.0048, -0.0082, -0.0031,  ..., -0.0081,  0.0025,  0.0034],\n",
       "         ...,\n",
       "         [-0.0009, -0.0007, -0.0081,  ...,  0.0042,  0.0076,  0.0089],\n",
       "         [ 0.0038,  0.0073,  0.0059,  ..., -0.0019,  0.0092, -0.0081],\n",
       "         [ 0.0038,  0.0071, -0.0018,  ...,  0.0075, -0.0034,  0.0079]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.down_proj.lora_AB.1.weight': tensor([[-7.5817e-05,  7.8678e-05, -7.9632e-05,  ...,  7.8678e-05,\n",
       "          -7.8678e-05,  7.9632e-05],\n",
       "         [ 7.6294e-05, -7.8678e-05,  7.9632e-05,  ..., -7.9155e-05,\n",
       "           7.8678e-05, -7.9632e-05],\n",
       "         [-7.7724e-05,  7.9155e-05, -7.9632e-05,  ...,  7.9632e-05,\n",
       "          -7.9632e-05,  7.9632e-05],\n",
       "         ...,\n",
       "         [-7.8201e-05,  7.9632e-05, -8.0109e-05,  ...,  7.9632e-05,\n",
       "          -7.9632e-05,  7.9632e-05],\n",
       "         [ 7.7724e-05, -7.8678e-05,  7.9632e-05,  ..., -7.9632e-05,\n",
       "           7.8678e-05, -7.9632e-05],\n",
       "         [ 7.9155e-05, -7.9632e-05,  8.0109e-05,  ..., -7.9632e-05,\n",
       "           8.0109e-05, -8.0109e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.gate_proj.lora_AB.0.weight': tensor([[-0.0096, -0.0100,  0.0037,  ..., -0.0073, -0.0101, -0.0040],\n",
       "         [-0.0025,  0.0040,  0.0065,  ..., -0.0127,  0.0104, -0.0142],\n",
       "         [-0.0060, -0.0090, -0.0045,  ..., -0.0031,  0.0145,  0.0132],\n",
       "         ...,\n",
       "         [ 0.0122, -0.0121,  0.0054,  ...,  0.0054, -0.0125,  0.0112],\n",
       "         [-0.0071,  0.0063,  0.0035,  ..., -0.0060, -0.0054,  0.0007],\n",
       "         [ 0.0020,  0.0083, -0.0073,  ..., -0.0084,  0.0153, -0.0142]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.gate_proj.lora_AB.1.weight': tensor([[-7.6294e-05, -7.6771e-05, -7.6294e-05,  ..., -6.8665e-05,\n",
       "          -7.8678e-05,  7.6294e-05],\n",
       "         [-6.2466e-05, -6.2943e-05, -5.5075e-05,  ..., -4.1008e-05,\n",
       "          -7.0095e-05,  6.0558e-05],\n",
       "         [ 7.1049e-05,  7.2479e-05,  7.2002e-05,  ...,  6.0558e-05,\n",
       "           7.6771e-05, -7.2002e-05],\n",
       "         ...,\n",
       "         [-3.7193e-05, -5.5313e-05, -6.4373e-05,  ..., -3.5286e-05,\n",
       "          -7.1049e-05,  6.0320e-05],\n",
       "         [-6.6757e-05, -6.7234e-05, -6.2466e-05,  ..., -4.4584e-05,\n",
       "          -7.2956e-05,  6.5804e-05],\n",
       "         [ 3.0249e-06, -2.0504e-05, -4.1723e-05,  ..., -1.6570e-05,\n",
       "          -5.3167e-05,  3.1233e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.up_proj.lora_AB.0.weight': tensor([[-0.0005,  0.0094, -0.0146,  ..., -0.0083, -0.0120, -0.0103],\n",
       "         [ 0.0025, -0.0045, -0.0135,  ...,  0.0118, -0.0095, -0.0140],\n",
       "         [ 0.0032,  0.0143, -0.0052,  ...,  0.0096, -0.0054, -0.0072],\n",
       "         ...,\n",
       "         [-0.0143, -0.0050, -0.0090,  ..., -0.0144, -0.0083, -0.0112],\n",
       "         [-0.0150,  0.0100,  0.0040,  ...,  0.0137, -0.0118,  0.0140],\n",
       "         [-0.0010,  0.0009, -0.0063,  ...,  0.0103, -0.0009, -0.0050]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.mlp._fsdp_wrapped_module.up_proj.lora_AB.1.weight': tensor([[-7.6294e-05,  6.2943e-05,  4.2677e-05,  ...,  6.9141e-05,\n",
       "           6.8188e-05,  6.5327e-05],\n",
       "         [ 7.2002e-05, -4.6492e-05, -3.1948e-05,  ..., -5.7697e-05,\n",
       "          -5.5552e-05, -5.2929e-05],\n",
       "         [-7.7248e-05,  6.8665e-05,  6.1035e-05,  ...,  7.1049e-05,\n",
       "           7.2479e-05,  6.9141e-05],\n",
       "         ...,\n",
       "         [ 7.5340e-05, -6.4850e-05, -5.9366e-05,  ..., -5.9843e-05,\n",
       "          -6.9618e-05, -5.6267e-05],\n",
       "         [-7.8678e-05,  7.2956e-05,  6.1512e-05,  ...,  7.6294e-05,\n",
       "           7.5817e-05,  7.5340e-05],\n",
       "         [-6.3896e-05,  2.2888e-05, -1.5199e-05,  ...,  4.9353e-05,\n",
       "           2.7776e-05,  4.1962e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.k_proj.lora_AB.0.weight': tensor([[-0.0049, -0.0134, -0.0111,  ...,  0.0154,  0.0094,  0.0090],\n",
       "         [-0.0101, -0.0021, -0.0040,  ...,  0.0038, -0.0110, -0.0116],\n",
       "         [-0.0076,  0.0057, -0.0142,  ...,  0.0046,  0.0100,  0.0110],\n",
       "         ...,\n",
       "         [ 0.0057,  0.0115, -0.0063,  ...,  0.0096,  0.0128,  0.0013],\n",
       "         [-0.0142, -0.0150, -0.0146,  ...,  0.0126,  0.0061,  0.0038],\n",
       "         [ 0.0066, -0.0099,  0.0096,  ..., -0.0072,  0.0090, -0.0112]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.k_proj.lora_AB.1.weight': tensor([[-8.8811e-06,  1.8120e-05, -8.8215e-06,  ..., -1.4424e-05,\n",
       "           2.6464e-05,  8.2254e-06],\n",
       "         [ 6.9439e-06, -1.4365e-05,  6.8843e-06,  ...,  1.4782e-05,\n",
       "          -2.8014e-05, -1.2636e-05],\n",
       "         [-9.8348e-07, -2.2650e-06, -1.0133e-06,  ..., -1.2591e-06,\n",
       "           5.5507e-07,  1.9372e-06],\n",
       "         ...,\n",
       "         [-5.1975e-05, -7.4387e-05, -6.6280e-05,  ..., -7.2479e-05,\n",
       "           7.2956e-05,  6.9618e-05],\n",
       "         [ 5.2452e-05,  7.4387e-05,  6.5327e-05,  ...,  7.2479e-05,\n",
       "          -7.2956e-05, -6.9618e-05],\n",
       "         [-5.8889e-05, -7.5340e-05, -6.9141e-05,  ..., -7.4387e-05,\n",
       "           7.5340e-05,  7.2002e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.q_proj.lora_AB.0.weight': tensor([[-0.0050,  0.0043, -0.0043,  ..., -0.0057,  0.0144,  0.0094],\n",
       "         [-0.0121, -0.0088, -0.0100,  ...,  0.0059, -0.0149, -0.0121],\n",
       "         [-0.0100,  0.0126, -0.0060,  ..., -0.0100,  0.0118, -0.0099],\n",
       "         ...,\n",
       "         [ 0.0122, -0.0095, -0.0039,  ..., -0.0140, -0.0016, -0.0140],\n",
       "         [-0.0048,  0.0043, -0.0027,  ..., -0.0020, -0.0090, -0.0046],\n",
       "         [-0.0150, -0.0138, -0.0146,  ...,  0.0029,  0.0095,  0.0100]],\n",
       "        dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.q_proj.lora_AB.1.weight': tensor([[ 3.4809e-05, -9.7752e-06,  2.8491e-05,  ..., -3.5524e-05,\n",
       "           1.6928e-05,  4.7445e-05],\n",
       "         [ 2.4319e-05, -4.1425e-06,  1.8716e-05,  ..., -2.5153e-05,\n",
       "           8.8215e-06,  3.6001e-05],\n",
       "         [ 1.1563e-05, -1.8254e-06,  7.8678e-06,  ..., -1.0610e-05,\n",
       "           2.9206e-06,  1.8239e-05],\n",
       "         ...,\n",
       "         [ 7.2956e-05, -3.6240e-05,  6.8665e-05,  ..., -7.0095e-05,\n",
       "           6.5804e-05,  7.5340e-05],\n",
       "         [-7.3433e-05,  3.6240e-05, -6.8665e-05,  ...,  7.1049e-05,\n",
       "          -6.5327e-05, -7.5817e-05],\n",
       "         [ 7.2002e-05, -3.2187e-05,  6.6280e-05,  ..., -6.8665e-05,\n",
       "           6.2943e-05,  7.4863e-05]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.v_proj.lora_AB.0.weight': tensor([[-3.7537e-03,  1.1108e-02, -3.1281e-04,  ...,  7.4463e-03,\n",
       "           1.1230e-02, -1.5015e-02],\n",
       "         [-1.4114e-03,  9.1553e-03,  1.2695e-02,  ..., -8.4229e-03,\n",
       "           1.2817e-02,  8.0566e-03],\n",
       "         [-1.5259e-02, -1.5335e-03,  2.9907e-03,  ..., -1.2817e-02,\n",
       "          -1.4114e-03, -1.2329e-02],\n",
       "         ...,\n",
       "         [ 1.0254e-02,  4.5166e-03,  1.2939e-02,  ..., -1.1108e-02,\n",
       "           6.7139e-03, -1.3062e-02],\n",
       "         [ 6.4373e-05, -1.0452e-03, -1.0452e-03,  ..., -1.7624e-03,\n",
       "           6.7139e-03,  1.1841e-02],\n",
       "         [-5.8594e-03, -1.2329e-02, -1.1841e-02,  ..., -2.2583e-03,\n",
       "          -3.7384e-03,  9.1553e-03]], dtype=torch.bfloat16),\n",
       " '_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module._checkpoint_wrapped_module.self_attn._fsdp_wrapped_module.v_proj.lora_AB.1.weight': tensor([[-7.9632e-05,  7.9632e-05, -7.9632e-05,  ..., -7.9632e-05,\n",
       "          -7.9632e-05,  7.8678e-05],\n",
       "         [-7.5340e-05, -7.7724e-05,  7.9632e-05,  ...,  7.9155e-05,\n",
       "           7.4387e-05, -6.9141e-05],\n",
       "         [ 8.0109e-05, -8.0109e-05,  8.0109e-05,  ...,  8.0109e-05,\n",
       "           8.0109e-05, -8.0109e-05],\n",
       "         ...,\n",
       "         [ 7.9632e-05, -8.0109e-05,  7.9632e-05,  ...,  7.9632e-05,\n",
       "           7.9632e-05, -7.9632e-05],\n",
       "         [ 7.8678e-05, -7.8678e-05,  7.9632e-05,  ...,  7.9632e-05,\n",
       "           7.9632e-05, -7.8678e-05],\n",
       "         [ 7.9632e-05, -8.0109e-05,  7.9632e-05,  ...,  7.9632e-05,\n",
       "           7.9632e-05, -7.9632e-05]], dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe4319-77b9-4af5-ab8f-951ea516a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed model.layers.0.mlp.down_proj.lora_AB.0.weight\n",
      "Changed model.layers.0.mlp.down_proj.lora_AB.1.weight\n",
      "Changed model.layers.0.mlp.gate_proj.lora_AB.0.weight\n",
      "Changed model.layers.0.mlp.gate_proj.lora_AB.1.weight\n",
      "Changed model.layers.0.mlp.up_proj.lora_AB.0.weight\n",
      "Changed model.layers.0.mlp.up_proj.lora_AB.1.weight\n",
      "Changed model.layers.0.self_attn.k_proj.lora_AB.0.weight\n",
      "Changed model.layers.0.self_attn.k_proj.lora_AB.1.weight\n",
      "Changed model.layers.0.self_attn.q_proj.lora_AB.0.weight\n",
      "Changed model.layers.0.self_attn.q_proj.lora_AB.1.weight\n",
      "Changed model.layers.0.self_attn.v_proj.lora_AB.0.weight\n",
      "Changed model.layers.0.self_attn.v_proj.lora_AB.1.weight\n",
      "Changed model.layers.1.mlp.down_proj.lora_AB.0.weight\n",
      "Changed model.layers.1.mlp.down_proj.lora_AB.1.weight\n",
      "Changed model.layers.1.mlp.gate_proj.lora_AB.0.weight\n",
      "Changed model.layers.1.mlp.gate_proj.lora_AB.1.weight\n",
      "Changed model.layers.1.mlp.up_proj.lora_AB.0.weight\n",
      "Changed model.layers.1.mlp.up_proj.lora_AB.1.weight\n",
      "Changed model.layers.1.self_attn.k_proj.lora_AB.0.weight\n",
      "Changed model.layers.1.self_attn.k_proj.lora_AB.1.weight\n",
      "Changed model.layers.1.self_attn.q_proj.lora_AB.0.weight\n",
      "Changed model.layers.1.self_attn.q_proj.lora_AB.1.weight\n",
      "Changed model.layers.1.self_attn.v_proj.lora_AB.0.weight\n",
      "Changed model.layers.1.self_attn.v_proj.lora_AB.1.weight\n"
     ]
    }
   ],
   "source": [
    "for k, v in weights_init.items():\n",
    "\n",
    "    if ('base_layer' in k) or ('W_q' in k):    \n",
    "        if not torch.equal(v.view(torch.uint8), weights[k].view(torch.uint8)):\n",
    "            print(\"Changed\", k)\n",
    "    else:\n",
    "        if not torch.equal(v, weights[k]):\n",
    "            print(\"Changed\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0368661-b3bb-44d9-b531-936281969b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8cc196-1e4d-4460-ab3f-8fb075319104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba2f2e-ae89-473b-b15b-ab68cf3b0881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080dd88-70b3-499b-be14-94d327406b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02d551-eb4b-4b22-9986-e19c30726a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
