{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1435d47-4707-4de5-8d4d-2395d965a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "from safetensors.torch import save_file\n",
    "from pathlib import Path\n",
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME\n",
    "import copy,os,sys\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6253154-6357-4d2c-a0b3-7905f6ed03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.configuration_utils import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aaecaa-7976-4833-b988-eb0e588d5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a08440-5ecf-42e1-bebd-989c18956500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.parallel import parallel\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707e59cd-6079-4785-add7-e00b49750462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes.nn import Linear4bit, Params4bit\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from accelerate import init_empty_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab9c512-57bc-4179-9fc3-36599167aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"/weka/home-keremturgutlu/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e57112-2228-4fcf-9520-db0f9d9fe3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "idx = hub.cached_file(MODEL_NAME, SAFE_WEIGHTS_INDEX_NAME)\n",
    "files, _ = hub.get_checkpoint_shard_files(MODEL_NAME, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92b3dee-edf1-405b-8c1a-1729033dd558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/admin/home-keremturgutlu/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model-00001-of-00002.safetensors',\n",
       " '/admin/home-keremturgutlu/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model-00002-of-00002.safetensors']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a945496e-6217-40c7-8daa-f0c9bfb7c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get(\"CUDA_VISIBLE_DEVICES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b82ba7-35f6-4df6-9e31-9a1ddc194b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88165001-9931-4e70-9cd4-c409388d8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f98fd2a3-51c9-43c1-a215-02c6bcb2fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40f680-1fdb-4350-b050-0309fb5e9577",
   "metadata": {},
   "source": [
    "**TODO:** vllm bnb quantized custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2695d576-5b04-4724-a16d-2ba5cc0bc6e3",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29b59f7b-5980-478a-96a3-250309e3a63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9204b5d-bb75-4809-aaf7-e830c1f4547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7372c0a8-8815-4c35-87c7-01b87213a6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"microsoft/orca-math-word-problems-200k\")['train'].shuffle(seed=42)\n",
    "# train with 10k for starters. Then 100k.\n",
    "# dataset = dataset.select(range(0,100000))\n",
    "\n",
    "# select last 5k as validation\n",
    "dataset = dataset.select(range(len(dataset)-5000,len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fecd16d3-1e63-4c3d-85ff-3af516438989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af419f48-ecb4-4dc7-92da-e07543e39504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Charlie decided to make metal supports for the wings. He needs 635 lbs of metal in total and needs to buy an additional 359 lbs of metal. How much metal does he have in storage?',\n",
       " 'answer': 'Charlie needs a total of 635 lbs of metal and has to buy an additional 359 lbs of metal. To find out how much metal he already has in storage, we subtract the amount he needs to buy from the total amount he needs.\\n\\nSo, 635 lbs (total needed) - 359 lbs (amount to buy) = 276 lbs.\\n\\nCharlie has 276 lbs of metal in storage.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c87a28c-c048-4630-bc52-1fa3bb00c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_ANSWER_PROMPT = \"\"\"Given a passage with a detailed answer to a question, extract the final answer as \"Answer:<the final answer>\\n\"\n",
    "\n",
    "<example>\n",
    "To solve this problem, we need to find out how much of the job each worker can do in one hour, and then add those amounts together to find out how much of the job they can do together in one hour.\n",
    "\n",
    "Worker A can do 1/10 of the job in one hour.\n",
    "Worker B can do 1/12 of the job in one hour.\n",
    "Worker C can do 1/15 of the job in one hour.\n",
    "\n",
    "Now, let's add these fractions to find out how much of the job they can do together in one hour:\n",
    "\n",
    "1/10 + 1/12 + 1/15\n",
    "\n",
    "To add these fractions, we need a common denominator. The least common multiple (LCM) of 10, 12, and 15 is 60. So we convert each fraction to have a denominator of 60:\n",
    "\n",
    "(6/60) + (5/60) + (4/60)\n",
    "\n",
    "Now we can add the fractions:\n",
    "\n",
    "6/60 + 5/60 + 4/60 = 15/60\n",
    "\n",
    "This simplifies to 1/4, which means that together, workers A, B, and C can do 1/4 of the job in one hour.\n",
    "\n",
    "To find out how long it will take them to complete the entire job, we take the reciprocal of 1/4:\n",
    "\n",
    "1 / (1/4) = 4\n",
    "\n",
    "So, it will take workers A, B, and C working together 4 hours to complete the job.\n",
    "\n",
    "Answer: 4 hours\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "If Mr. Roberts chooses the installment plan, he would pay a down payment of $120 and then $30 a month for 12 months. \n",
    "\n",
    "The total cost for the installment plan would be:\n",
    "Down payment: $120\n",
    "Monthly payments: $30/month * 12 months = $360\n",
    "\n",
    "Total cost with installment plan: $120 + $360 = $480\n",
    "\n",
    "If he pays cash, the television costs $400.\n",
    "\n",
    "To find out how much he can save by paying cash, we subtract the cash price from the total cost of the installment plan:\n",
    "\n",
    "Savings by paying cash: $480 - $400 = $80\n",
    "\n",
    "Mr. Roberts can save $80 by paying cash for the television.\n",
    "\n",
    "Answer $80\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "Let's denote the total capacity of the reservoir as T gallons.\n",
    "\n",
    "According to the information given, 6 million gallons is 60% of the total capacity. So we can write the following equation:\n",
    "\n",
    "0.60 * T = 6 million gallons\n",
    "\n",
    "From this, we can solve for T:\n",
    "\n",
    "T = 6 million gallons / 0.60\n",
    "T = 10 million gallons\n",
    "\n",
    "Now, we are told that the normal level is 5 million gallons short of the total capacity. Therefore, the normal level (N) is:\n",
    "\n",
    "N = T - 5 million gallons\n",
    "N = 10 million gallons - 5 million gallons\n",
    "N = 5 million gallons\n",
    "\n",
    "Now we need to find the ratio of the amount of water in the reservoir at the end of the month (6 million gallons) to the normal level (5 million gallons):\n",
    "\n",
    "Ratio = Amount at the end of the month / Normal level\n",
    "Ratio = 6 million gallons / 5 million gallons\n",
    "Ratio = 6 / 5\n",
    "Ratio = 1.2\n",
    "\n",
    "So the ratio of the amount of water in the reservoir at the end of the month to the normal level is 1.2:1.\n",
    "\n",
    "Answer: 1.2:1\n",
    "</example>\n",
    "\n",
    "Now it is your turn:\n",
    "\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db1521f5-6553-4089-9f8f-ca75bd0943b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = anthropic.Anthropic(\n",
    "#     api_key=open(\"/weka/home-keremturgutlu/claude-api.key\").read().strip(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65c173ef-1943-49ac-a8c8-18dc12e43fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_short_answer(answer_text):\n",
    "#     message = client.messages.create(\n",
    "#         model=\"claude-3-haiku-20240307\",\n",
    "#         max_tokens=20,\n",
    "#         temperature=0.0,\n",
    "#         messages=[\n",
    "#             {\"role\": \"user\", \"content\": EXTRACT_ANSWER_PROMPT.format(text=answer_text)},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Answer:\"}\n",
    "#         ],\n",
    "#     )\n",
    "    \n",
    "#     return message.content[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d39e5de5-d701-40d5-b7de-da43adbbea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_answers_gt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1450e7d2-63a1-41bf-a7f5-b740435de113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, a in tqdm(enumerate(dataset[:100]['answer'])):\n",
    "#     if len(short_answers_gt) > i: continue\n",
    "#     short_answers_gt.append(extract_short_answer(a))\n",
    "#     if i % 10 == 0: time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b1fb856-1aae-449d-86e5-e717b4c507b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "602d5e74-b11e-4515-9043-25f91145c5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last occurring number or ratio in \"The item costs $123.45, but with a discount of $10.00, the final price is $113.45.\" is: $113.45\n",
      "The last occurring number or ratio in \"The ratio of water to concentrate is 5.5:1 for the mixture.\" is: 5.5:1\n",
      "The last occurring number or ratio in \"The investment return was 10:1.\" is: 10:1\n",
      "The last occurring number or ratio in \"Answer is 42.3.\n",
      "Answer is 42\" is: 42\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_last_number_or_ratio(s):\n",
    "    # Find all sequences of digits, possibly with leading currency symbols, decimal points, and ratios\n",
    "    patterns = re.findall(r'[\\$€£]?\\d+(?:\\.\\d+)?(?:\\:\\d+(?:\\.\\d+)?)?', s)\n",
    "    \n",
    "    # Return the last pattern found, or None if there are no matches\n",
    "    if patterns:\n",
    "        return patterns[-1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "examples = [\n",
    "    \"The item costs $123.45, but with a discount of $10.00, the final price is $113.45.\",\n",
    "    \"The ratio of water to concentrate is 5.5:1 for the mixture.\",\n",
    "    \"The investment return was 10:1.\",\n",
    "    \"Answer is 42.3.\\nAnswer is 42\"\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(f\"The last occurring number or ratio in \\\"{s}\\\" is: {extract_last_number_or_ratio(s)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c3f2be-dd7c-4f42-9a5f-b829a09d49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers_gt = parallel(extract_last_number_or_ratio, dataset['answer'], progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09d19081-57b6-4634-9d02-087a7671b8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_answers_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f159dff-0fac-4ced-80b4-e1ddca3da48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) ['4','$80','1.2:1','50','2:1']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_answers_gt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2b8f9f7-7ce2-448b-a20c-e1f8de05863d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([True if a == '' else False for a in short_answers_gt]).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588263ef-ccf5-4a39-aea5-9a872dab0ecd",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "#### Zero-shot:\n",
    "\n",
    "`n=500, exact_match_score=0.068`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaba20a6-d6a0-43f0-a7ab-0d1461bd260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-26 10:16:09 config.py:618] Casting torch.float16 to torch.bfloat16.\n",
      "INFO 03-26 10:16:10 llm_engine.py:87] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-26 10:16:15 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 03-26 10:18:21 llm_engine.py:357] # GPU blocks: 2820, # CPU blocks: 512\n",
      "INFO 03-26 10:18:23 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-26 10:18:23 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-26 10:18:29 model_runner.py:756] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=MODEL_NAME, tokenizer=MODEL_NAME, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6898e24e-f420-4bc0-853e-8672308fe502",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in dataset[:500]['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccc2d38d-7e30-4a69-a73d-00b1b63f6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████| 500/500 [04:04<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs, SamplingParams(temperature=0.0, stop_token_ids=[tokenizer.eos_token_id], max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b76d7713-ad58-4d58-97fb-2d907171875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers_pred = [extract_last_number_or_ratio(o.outputs[0].text) for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f37048f5-dc32-4c69-884a-1c4d98f2eb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.068"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691af822-bf4c-4a4f-85e7-8432726a2883",
   "metadata": {},
   "source": [
    "### 5-shot\n",
    "\n",
    "`n=500, exact_match_score=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac5b28ee-127d-4b7c-aaa5-e5f964c543dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-27 09:16:50 config.py:618] Casting torch.float16 to torch.bfloat16.\n",
      "INFO 03-27 09:16:50 llm_engine.py:87] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-27 09:16:55 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 03-27 09:20:52 llm_engine.py:357] # GPU blocks: 2820, # CPU blocks: 512\n",
      "INFO 03-27 09:20:56 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-27 09:20:56 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-27 09:21:04 model_runner.py:756] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=MODEL_NAME, tokenizer=MODEL_NAME, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58bf8a9c-8643-446b-b54e-63c547a062d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "few_shot_examples = [f\"###Question:\\n{ex['question']}\\n###Answer:{ex['answer']}\" for ex in dataset.select(range(len(dataset)-5,len(dataset)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b53c2c1f-757c-4b6e-91d2-9614ed318ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\\n\\n\".join(few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3f45fcf-c135-492c-878b-6fcd9b718d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [few_shot_prompt + \"\\n\\n\" + f\"###Question:\\n{question}\\n###Answer:\\n\" for question in dataset[:500]['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b465a90-240d-4512-8282-b4178eef2a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 333/500 [11:15<07:12,  2.59s/it]"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs, SamplingParams(temperature=0.0, stop_token_ids=[tokenizer.eos_token_id], max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65760f1-2c58-46ff-946d-3b89a9f8dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers_pred = [extract_last_number_or_ratio(o.outputs[0].text) for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee435c-dcd9-4210-9e71-3490384e6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c3cde-5916-46f7-921a-11cb6509a277",
   "metadata": {},
   "source": [
    "### Full Finetune \n",
    "\n",
    "`n=500, exact_match_score=0.182`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a65841-d40b-4c87-a1d6-2347964fe497",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2384df0-2ce6-41ae-a75a-944a227f5b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-26 09:58:27 config.py:618] Casting torch.float16 to torch.bfloat16.\n",
      "INFO 03-26 09:58:27 llm_engine.py:87] Initializing an LLM engine with config: model='/weka/home-keremturgutlu/models/llama-7b-orca-math-10k-full', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-26 09:58:41 llm_engine.py:357] # GPU blocks: 2867, # CPU blocks: 512\n",
      "INFO 03-26 09:58:43 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-26 09:58:43 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-26 09:58:49 model_runner.py:756] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=str(models_dir/\"llama-7b-orca-math-10k-full\"), tokenizer=MODEL_NAME, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3fc79a60-2db4-4c71-975c-44f67cdfe8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in dataset[:500]['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "893930aa-fe76-42ef-94af-2500b04e596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████| 500/500 [01:35<00:00,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs, SamplingParams(temperature=0.0, stop_token_ids=[tokenizer.eos_token_id], max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed63e50b-e049-4c70-952b-43c369f30e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers_pred = [extract_last_number_or_ratio(o.outputs[0].text) for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1111fbcf-3922-45cc-8068-8ee066f5d259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.182"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ea308f8-a689-4c05-a171-6bf38d86dbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.8 s, sys: 10.4 ms, total: 5.81 s\n",
      "Wall time: 5.82 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = llm.generate(inputs[0], SamplingParams(temperature=0.0, stop_token_ids=[tokenizer.eos_token_id], max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d03efa59-8166-4388-89a4-441c1c7b2395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o = output[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "48fb04ab-15dd-4504-b98a-21a470423642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.31958762886597"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(o.token_ids)/5.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2fc0513a-d05f-4e71-b83c-5fa86a06297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = output[0].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dfe01c96-6b1c-4004-8ef2-f582d35fda8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RequestMetrics(arrival_time=1673463.661995299, last_token_time=1673463.661995299, first_scheduled_time=1711447501.5359676, first_token_time=1711447501.5566397, time_in_queue=1709774037.8739722, finished_time=1711447507.3578482)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a2c8d-b114-48ef-af30-0eeadc68c101",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### HF (Much Slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b998ad9-6ed3-497c-8a72-350a4a5ca8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_state_dict = safetensors.torch.load_file(model_path/\"model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc740cbb-b45f-4c83-84fe-835d72cb9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model_state_dict.keys())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7801b2a5-4f91-479d-a582-241ee567a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     use_cache=False,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     _attn_implementation=\"sdpa\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d750fc2d-44ef-4942-abad-ec1a8c5ff6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(model_state_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fca10bab-c0ae-41f0-aa9e-6d7c2ab83545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b3d0d3e-c742-454b-aade-43e2a4e0e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4009e8f0-0261-4d48-862b-60d5f06b2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b32570-7922-4483-83d8-a5e7639ea33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = sample['question']\n",
    "# output = model.generate(torch.tensor(tokenizer.encode(f\"###Question:\\n{question}\\n###Answer:\\n\")).view(1,-1).cuda(),\n",
    "#                          do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13896b4a-fb69-4303-91cc-4dd8a1a63921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_last_number_or_ratio(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "804f261f-4975-4243-867d-42e5d8438eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_answers_pred = []\n",
    "# for i in tqdm(range(0,len(dataset),4)):\n",
    "    \n",
    "#     inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in dataset[i:i+4]['question']]\n",
    "#     input_ids = tokenizer(inputs)['input_ids']\n",
    "    \n",
    "#     max_toks = max(len(toks) for toks in input_ids)\n",
    "    \n",
    "#     b = torch.stack([torch.tensor(((max_toks-len(toks))*[tokenizer.unk_token_id])+toks) for toks in input_ids])\n",
    "    \n",
    "#     input_lens = [len(toks) for toks in input_ids]\n",
    "    \n",
    "#     output = model.generate(b.cuda(), \n",
    "#                             do_sample=False, \n",
    "#                             use_cache=True,\n",
    "#                             pad_token_id=tokenizer.unk_token_id, \n",
    "#                             eos_token_id=tokenizer.eos_token_id, \n",
    "#                             max_new_tokens=1024).cpu()\n",
    "    \n",
    "#     pred = [extract_last_number_or_ratio(tokenizer.decode(o[o!=tokenizer.unk_token_id][n:])) for o,n in zip(output,input_lens)]\n",
    "\n",
    "#     short_answers_pred.extend(pred)\n",
    "#     if i > 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e51633a5-403a-4bc7-8504-82ab0617363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254b3de-c40d-4a53-8b58-f68f921fbb3e",
   "metadata": {},
   "source": [
    "### Full Finetune + Post Quantize \n",
    "\n",
    "vLLM only supports AWQ quantization at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896be351-1cfd-4f9b-8f1e-b8b5f6da94e1",
   "metadata": {},
   "source": [
    "#### HF BnB 4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22d59213-7b04-4d6e-ac3a-e5fe90952017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:dict|None=None,\n",
    "                   skip_modules:List[str]=[\"lm_head\"], **kwargs):\n",
    "    \"\"\"\n",
    "    Replace linear modules with a new Linear module.\n",
    "    Parameters:\n",
    "        model (`torch.nn.Module`):\n",
    "            Input model or `torch.nn.Module` as the function is run recursively.\n",
    "        linear_replacement (`torch.nn.Module`):\n",
    "            The linear module that replaces the old one. Only expects standard arguments.\n",
    "            If other arguments need to be passed, use a lambda.\n",
    "        skip_modules (`List[str]`, *optional*, defaults to `lm_head`):\n",
    "            List of modules names not to convert. Defaults to `lm_head`.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if name in skip_modules:\n",
    "            print(f\"Skipping {name}\")\n",
    "            continue\n",
    "        \n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                model._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            # elif issubclass(linear_replacement, HQQLinear):\n",
    "            #     model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de210ef2-4996-4884-b617-a27c21973fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize(module:nn.Module, name:str, value:torch.Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                      skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False,\n",
    "                      quant_method:str='bnb', is_dora:bool=False):\n",
    "    \"\"\"\n",
    "    Loads `value` tensor into submodule of `module`, optionally skipping `skip_names` and converting to `dtype`.\n",
    "\n",
    "    Quantizes `Params4bit` on `device` then places on \"cpu\" if low_memory=True or \"meta\" if is_meta_rank=True.\n",
    "    \"\"\"\n",
    "    def place_on_device(value):\n",
    "        if is_meta_rank:\n",
    "            device = 'meta'\n",
    "        elif low_memory:\n",
    "            device = 'cpu'\n",
    "        return value.to(device=device, dtype=dtype)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "        if verbose:\n",
    "            print(f\"Skipping {name} because it is in skip_names\")\n",
    "        return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "        submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "        print(f\"Module {module_key} not found:\\n{e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if quant_method=='bnb':\n",
    "            param = submodule.get_parameter(value_key)\n",
    "            if isinstance(param, Params4bit):\n",
    "                # With `sync_module_states=True`, a meta device Params4bit needs to be the same\n",
    "                # shape as the quantized Params4bit with an initialized quant_state. However,\n",
    "                # FSDP only syncs parameters and buffers, so the quant_state isn't copied. This\n",
    "                # workaround quantizes Params4bit to initialize quant_state on all ranks, then\n",
    "                # replaces Params4bit's data with a meta tensor to free memory on non-rank 0.\n",
    "                if is_dora:\n",
    "                    setattr(submodule, \"dora_scale\", value.norm(p=2, dim=1).to(dtype=dtype).to(\"cpu\"))                \n",
    "                    print(\"DORA scale initialized\")\n",
    "                value = type(param)(value.to(device=device, dtype=dtype).data, **param.__dict__).cuda(device)\n",
    "                if is_meta_rank:\n",
    "                    value = type(param)(value.data.to(\"meta\"), **value.__dict__)\n",
    "                elif low_memory:\n",
    "                    value = type(param)(value.data.to(\"cpu\"), **value.__dict__)\n",
    "                # print(\"Loaded quantized layer\")\n",
    "            else:\n",
    "                value = type(param)(place_on_device(value).data)\n",
    "                # print(\"Loaded regular layer\")\n",
    "    except AttributeError:\n",
    "        # it's a buffer\n",
    "        value = place_on_device(value)\n",
    "        pass\n",
    "    setattr(submodule, value_key, value)\n",
    "\n",
    "def load_and_quantize_parallel(name_param, model, **kwargs):\n",
    "    name, param = name_param\n",
    "    load_and_quantize(model, name, param, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0ee90b4a-fed5-4fea-b4bb-99fd10564679",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "skip_modules = [\"lm_head\"]\n",
    "load_param_skip_names = ['inv_freq']\n",
    "compute_dtype = torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "90623827-dc4e-405d-96d8-d267829fd628",
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                 quant_type='nf4', quant_storage=torch_dtype, skip_modules=skip_modules)\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9eda4e90-8ff8-4d56-bc07-c114355f7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(glob(str(models_dir/\"llama-7b-orca-math-10k-full/*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2ed57-87ec-48d1-ab97-3eaa04a9c473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel(load_and_quantize_parallel, \n",
    "         iter(weights.items()), \n",
    "         n_workers=8, \n",
    "         threadpool=True,\n",
    "         model=model, \n",
    "         dtype=torch_dtype, \n",
    "         device=torch.cuda.current_device(),\n",
    "         skip_names=load_param_skip_names,\n",
    "         is_meta_rank=False,\n",
    "         verbose=True,\n",
    "         quant_method=\"bnb\",\n",
    "         is_dora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "00dd5f37-771f-41ba-b209-aec1af95a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "76552491-ae2c-4d89-bcbe-d91556c4b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.32 s, sys: 1.9 ms, total: 8.32 s\n",
      "Wall time: 8.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = dataset[5]['question']\n",
    "output = model.generate(torch.tensor(tokenizer.encode(f\"###Question:\\n{question}\\n###Answer:\\n\")).view(1,-1).cuda(),\n",
    "                         do_sample=False, max_new_tokens=1024, use_cache=True).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "204f24b7-6896-4d43-a7e1-e74c7fb42382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ###Question:\n",
      "A certain tax rate is some amount per $100.00. The rate, expressed as a percent, is 65%. What is the tax rate in dollars per $100.00?\n",
      "###Answer:\n",
      "To find the tax rate in dollars per $100.00, we need to convert the rate from a percent to a decimal and then multiply it by $100.00.\n",
      "\n",
      "The tax rate expressed as a percent is 65%. To convert this to a decimal, we divide by 100:\n",
      "\n",
      "65% / 100 = 0.65\n",
      "\n",
      "Now, we multiply this decimal by $100.00 to find the tax rate in dollars per $100.00:\n",
      "\n",
      "0.65 * $100.00 = $65.00\n",
      "\n",
      "Therefore, the tax rate in dollars per $100.00 is $65.00.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "635214a8-0727-4cee-a9bc-c6535f0a114c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(tokenizer.encode(f\"###Question:\\n{question}\\n###Answer:\\n\")).view(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "395a22fc-df92-4544-bb4d-1509f15daeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.644970414201186"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output[0].shape[0] - 56) / 8.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "764ec0c2-3604-449d-9445-4068767e7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_last_number_or_ratio(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa7054cb-66d6-4fab-956e-0ee5ec558b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = dataset.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7998bdf4-da32-42a2-a770-216ac4800a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "63ae21e5-0bdd-40a8-8d32-ef780801cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                      | 1/32 [02:53<1:29:31, 173.28s/it]\n"
     ]
    }
   ],
   "source": [
    "short_answers_pred = []\n",
    "bs = 16\n",
    "for i in tqdm(range(0,len(valid_dataset),bs)):\n",
    "    \n",
    "    inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in valid_dataset[i:i+bs]['question']]\n",
    "    input_ids = tokenizer(inputs)['input_ids']\n",
    "    \n",
    "    max_toks = max(len(toks) for toks in input_ids)\n",
    "    b = torch.stack([torch.tensor(((max_toks-len(toks))*[tokenizer.unk_token_id])+toks) for toks in input_ids])\n",
    "    input_lens = [len(toks) for toks in input_ids]\n",
    "    \n",
    "    output = model.generate(b.cuda(), \n",
    "                            do_sample=False, \n",
    "                            use_cache=True,\n",
    "                            pad_token_id=tokenizer.unk_token_id, \n",
    "                            eos_token_id=tokenizer.eos_token_id, \n",
    "                            max_new_tokens=1024).cpu()\n",
    "    \n",
    "    pred = [extract_last_number_or_ratio(tokenizer.decode(o[o!=tokenizer.unk_token_id][n:])) for o,n in zip(output,input_lens)]\n",
    "    short_answers_pred.extend(pred)\n",
    "\n",
    "    if i > 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "376744e9-cff5-4eb0-9d3f-1c8a1c88e444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435fe0f-2b36-476a-9d31-d3bf69a77cd2",
   "metadata": {},
   "source": [
    "#### VLLM BnB 4bit (Custom Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555ddab-5005-410c-9b6c-7099b8c45957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd550420-1dc2-4748-8a94-db77f7d142ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d00fd-56c9-4b24-9f00-a17dc4349865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9fa22f9-9aed-443c-baca-fa5f624dcd69",
   "metadata": {},
   "source": [
    "### QLoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10c6bc18-4a6e-4a15-920f-a8a7f6c31b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ef1defa-55ee-4844-bada-fa380c310674",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = hub.cached_file(MODEL_NAME, SAFE_WEIGHTS_INDEX_NAME)\n",
    "pretrained_files, _ = hub.get_checkpoint_shard_files(MODEL_NAME, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0ee537b-0d03-4a6c-a04a-d87990a73db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/weka/home-keremturgutlu/models/llama-7b-orca-math-10k-bnb-qlora/model_state_dict.safetensors'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(str(models_dir/\"llama-7b-orca-math-10k-bnb-qlora/*.safetensors\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5b72468-388b-4b2b-a5ac-bab8c465a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = safetensors.torch.load_file(glob(str(models_dir/\"llama-7b-orca-math-10k-bnb-qlora/*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7141e290-7f51-4304-a0c8-08f27d9e86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "skip_modules = [\"lm_head\"]\n",
    "load_param_skip_names = ['inv_freq']\n",
    "compute_dtype = torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e53cde1-5bc8-46d8-a2df-e6370f06144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                 quant_type='nf4', quant_storage=torch_dtype, skip_modules=skip_modules)\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddb187cc-b7d9-4eed-8f95-a1cf08275df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model.layers.0.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.1.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.10.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.11.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.12.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.13.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.14.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.15.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.16.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.17.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.18.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.19.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.2.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.20.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.21.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.22.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.23.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.3.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.4.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.5.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.6.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.7.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.8.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.9.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.24.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.25.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.26.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.27.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.28.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.29.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.30.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.31.self_attn.rotary_emb.inv_freq because it is in skip_names\n"
     ]
    }
   ],
   "source": [
    "for filename in pretrained_files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    parallel(load_and_quantize_parallel, \n",
    "             iter(weights.items()), \n",
    "             n_workers=8, \n",
    "             threadpool=True,\n",
    "             model=model, \n",
    "             dtype=torch_dtype, \n",
    "             device=torch.cuda.current_device(),\n",
    "             skip_names=load_param_skip_names,\n",
    "             is_meta_rank=False,\n",
    "             verbose=True,\n",
    "             quant_method=\"bnb\",\n",
    "             is_dora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "614ea75c-2b19-4530-b532-cc8b0484f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dba6a93e-32cc-4697-b77c-cce4a8390a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d84f3-ec85-486e-8baa-488cbd5634f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    if \"lora\" in n: \n",
    "        print(n)\n",
    "        p.data.copy_(trained_weights[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88af57e4-51c4-4d9e-9f13-c55cce5a1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab766241-8aaf-41b7-825c-994cf2a875a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = dataset.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22d16ef8-ca3c-4424-b7da-a4d4fe78200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "537ce269-944e-4d6c-a25b-512faeaffb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▊                                                                                        | 1/32 [03:41<1:54:18, 221.24s/it]\n"
     ]
    }
   ],
   "source": [
    "short_answers_pred = []\n",
    "bs = 16\n",
    "for i in tqdm(range(0,len(valid_dataset),bs)):\n",
    "    \n",
    "    inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in valid_dataset[i:i+bs]['question']]\n",
    "    input_ids = tokenizer(inputs)['input_ids']\n",
    "    \n",
    "    max_toks = max(len(toks) for toks in input_ids)\n",
    "    b = torch.stack([torch.tensor(((max_toks-len(toks))*[tokenizer.unk_token_id])+toks) for toks in input_ids])\n",
    "    input_lens = [len(toks) for toks in input_ids]\n",
    "    \n",
    "    output = model.generate(b.cuda(), \n",
    "                            do_sample=False, \n",
    "                            use_cache=True,\n",
    "                            pad_token_id=tokenizer.unk_token_id, \n",
    "                            eos_token_id=tokenizer.eos_token_id, \n",
    "                            max_new_tokens=1024).cpu()\n",
    "    \n",
    "    pred = [extract_last_number_or_ratio(tokenizer.decode(o[o!=tokenizer.unk_token_id][n:])) for o,n in zip(output,input_lens)]\n",
    "    short_answers_pred.extend(pred)\n",
    "\n",
    "    if i > 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d6f0768-cb01-4296-a015-3c507f6fb35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273cd9f-72f1-4cab-8966-1d21a98698dd",
   "metadata": {},
   "source": [
    "### QDoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7d562b3-b574-47a4-9bb3-e20ca534c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dora import BNBDORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09cb4288-9631-4faf-8428-96fb0f8ad93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = hub.cached_file(MODEL_NAME, SAFE_WEIGHTS_INDEX_NAME)\n",
    "pretrained_files, _ = hub.get_checkpoint_shard_files(MODEL_NAME, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f9bd33f-7bdf-466a-a09e-f798c02cf6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/weka/home-keremturgutlu/models/llama-7b-orca-math-10k-bnb-qdora/model_state_dict.safetensors'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(str(models_dir/\"llama-7b-orca-math-10k-bnb-qdora/*.safetensors\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aac40eb6-2a16-4713-aa58-e79cc1c39167",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = safetensors.torch.load_file(glob(str(models_dir/\"llama-7b-orca-math-10k-bnb-qdora/*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe2dbbda-dc82-43d6-b776-e5967e63a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "skip_modules = [\"lm_head\"]\n",
    "load_param_skip_names = ['inv_freq']\n",
    "compute_dtype = torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e94e1e4c-ba9a-4e7b-947c-666dd7ad534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                 quant_type='nf4', quant_storage=torch_dtype, skip_modules=skip_modules)\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ea8823c-477c-4a52-a4cb-b33f14a85108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model.layers.0.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.1.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.10.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.11.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "Skipping model.layers.12.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.13.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.14.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.15.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.16.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.17.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.18.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.19.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.2.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.20.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.21.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.22.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.23.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.3.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.4.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.5.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.6.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.7.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.8.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.9.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.24.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.25.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.26.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.27.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.28.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.29.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.30.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "Skipping model.layers.31.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n",
      "DORA scale initialized\n"
     ]
    }
   ],
   "source": [
    "for filename in pretrained_files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    parallel(load_and_quantize_parallel, \n",
    "             iter(weights.items()), \n",
    "             n_workers=8, \n",
    "             threadpool=True,\n",
    "             model=model, \n",
    "             dtype=torch_dtype, \n",
    "             device=torch.cuda.current_device(),\n",
    "             skip_names=load_param_skip_names,\n",
    "             is_meta_rank=False,\n",
    "             verbose=True,\n",
    "             quant_method=\"bnb\",\n",
    "             is_dora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e3c0746-2d83-4365-9287-1da76bbbc158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create LORA layers.\n",
    "lora_target_modules = [\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    "lora_rank=64\n",
    "lora_alpha=16\n",
    "lora_dropout=0.1\n",
    "lora_cls = BNBDORA \n",
    "\n",
    "for name, _ in model.named_modules():\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    if value_key in lora_target_modules:\n",
    "        m = model.get_submodule(name)\n",
    "        qlora_layer = lora_cls(m, lora_rank, lora_alpha, lora_dropout)\n",
    "        parent_module = model.get_submodule(module_key)\n",
    "        setattr(parent_module, value_key, qlora_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7fe2ca2-8e24-41f9-8dd8-f5a20674f385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.0.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.0.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.0.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.0.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.0.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.0.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.0.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.0.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.0.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.0.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.0.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.0.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.0.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.0.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.0.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.0.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.0.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.1.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.1.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.1.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.1.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.1.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.1.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.1.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.1.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.1.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.1.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.1.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.1.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.1.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.1.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.1.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.1.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.1.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.1.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.10.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.10.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.10.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.10.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.10.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.10.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.10.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.10.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.10.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.10.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.10.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.10.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.10.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.10.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.10.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.10.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.10.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.10.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.11.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.11.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.11.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.11.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.11.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.11.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.11.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.11.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.11.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.11.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.11.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.11.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.11.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.11.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.11.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.11.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.11.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.11.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.12.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.12.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.12.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.12.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.12.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.12.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.12.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.12.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.12.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.12.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.12.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.12.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.12.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.12.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.12.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.12.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.12.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.12.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.13.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.13.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.13.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.13.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.13.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.13.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.13.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.13.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.13.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.13.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.13.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.13.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.13.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.13.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.13.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.13.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.13.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.13.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.14.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.14.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.14.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.14.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.14.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.14.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.14.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.14.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.14.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.14.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.14.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.14.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.14.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.14.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.14.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.14.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.14.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.14.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.15.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.15.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.15.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.15.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.15.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.15.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.15.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.15.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.15.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.15.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.15.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.15.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.15.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.15.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.15.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.15.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.15.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.15.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.16.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.16.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.16.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.16.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.16.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.16.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.16.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.16.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.16.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.16.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.16.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.16.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.16.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.16.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.16.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.16.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.16.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.16.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.17.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.17.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.17.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.17.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.17.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.17.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.17.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.17.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.17.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.17.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.17.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.17.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.17.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.17.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.17.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.17.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.17.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.17.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.18.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.18.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.18.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.18.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.18.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.18.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.18.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.18.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.18.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.18.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.18.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.18.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.18.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.18.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.18.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.18.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.18.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.18.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.19.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.19.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.19.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.19.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.19.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.19.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.19.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.19.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.19.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.19.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.19.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.19.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.19.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.19.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.19.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.19.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.19.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.19.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.2.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.2.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.2.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.2.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.2.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.2.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.2.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.2.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.2.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.2.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.2.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.2.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.2.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.2.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.2.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.2.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.2.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.2.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.20.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.20.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.20.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.20.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.20.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.20.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.20.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.20.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.20.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.20.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.20.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.20.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.20.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.20.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.20.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.20.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.20.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.20.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.21.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.21.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.21.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.21.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.21.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.21.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.21.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.21.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.21.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.21.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.21.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.21.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.21.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.21.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.21.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.21.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.21.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.21.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.22.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.22.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.22.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.22.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.22.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.22.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.22.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.22.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.22.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.22.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.22.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.22.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.22.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.22.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.22.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.22.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.22.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.22.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.23.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.23.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.23.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.23.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.23.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.23.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.23.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.23.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.23.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.23.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.23.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.23.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.23.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.23.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.23.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.23.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.23.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.23.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.24.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.24.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.24.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.24.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.24.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.24.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.24.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.24.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.24.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.24.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.24.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.24.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.24.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.24.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.24.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.24.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.24.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.24.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.25.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.25.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.25.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.25.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.25.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.25.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.25.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.25.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.25.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.25.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.25.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.25.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.25.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.25.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.25.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.25.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.25.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.25.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.26.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.26.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.26.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.26.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.26.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.26.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.26.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.26.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.26.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.26.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.26.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.26.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.26.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.26.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.26.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.26.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.26.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.26.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.27.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.27.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.27.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.27.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.27.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.27.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.27.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.27.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.27.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.27.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.27.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.27.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.27.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.27.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.27.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.27.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.27.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.27.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.28.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.28.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.28.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.28.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.28.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.28.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.28.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.28.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.28.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.28.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.28.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.28.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.28.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.28.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.28.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.28.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.28.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.28.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.29.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.29.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.29.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.29.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.29.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.29.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.29.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.29.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.29.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.29.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.29.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.29.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.29.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.29.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.29.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.29.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.29.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.29.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.3.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.3.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.3.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.3.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.3.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.3.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.3.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.3.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.3.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.3.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.3.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.3.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.3.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.3.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.3.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.3.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.3.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.3.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.30.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.30.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.30.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.30.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.30.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.30.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.30.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.30.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.30.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.30.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.30.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.30.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.30.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.30.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.30.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.30.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.30.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.30.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.31.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.31.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.31.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.31.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.31.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.31.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.31.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.31.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.31.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.31.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.31.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.31.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.31.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.31.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.31.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.31.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.31.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.31.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.4.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.4.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.4.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.4.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.4.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.4.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.4.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.4.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.4.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.4.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.4.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.4.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.4.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.4.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.4.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.4.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.4.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.4.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.5.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.5.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.5.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.5.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.5.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.5.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.5.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.5.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.5.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.5.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.5.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.5.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.5.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.5.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.5.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.5.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.5.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.5.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.6.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.6.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.6.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.6.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.6.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.6.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.6.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.6.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.6.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.6.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.6.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.6.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.6.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.6.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.6.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.6.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.6.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.6.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.7.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.7.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.7.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.7.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.7.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.7.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.7.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.7.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.7.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.7.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.7.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.7.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.7.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.7.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.7.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.7.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.7.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.7.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.8.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.8.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.8.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.8.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.8.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.8.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.8.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.8.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.8.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.8.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.8.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.8.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.8.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.8.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.8.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.8.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.8.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.8.self_attn.v_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.9.mlp.down_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.9.mlp.down_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.9.mlp.down_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.9.mlp.gate_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.9.mlp.gate_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.9.mlp.gate_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.9.mlp.up_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.9.mlp.up_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.9.mlp.up_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.9.self_attn.k_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.9.self_attn.k_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.9.self_attn.k_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.9.self_attn.q_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.9.self_attn.q_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.9.self_attn.q_proj.magnitude_layer.magnitude',\n",
       " 'model.layers.9.self_attn.v_proj.dora_layer.lora_A.weight',\n",
       " 'model.layers.9.self_attn.v_proj.dora_layer.lora_B.weight',\n",
       " 'model.layers.9.self_attn.v_proj.magnitude_layer.magnitude']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trained_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b9daf81-0778-4bb4-ad77-6d400081c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.0.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.0.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.0.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.0.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.0.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.0.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.0.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.0.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.0.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.0.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.0.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.0.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.0.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.0.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.0.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.0.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.0.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.1.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.1.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.1.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.1.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.1.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.1.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.1.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.1.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.1.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.1.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.1.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.1.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.1.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.1.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.1.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.1.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.1.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.1.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.2.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.2.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.2.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.2.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.2.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.2.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.2.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.2.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.2.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.2.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.2.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.2.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.2.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.2.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.2.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.2.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.2.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.2.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.3.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.3.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.3.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.3.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.3.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.3.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.3.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.3.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.3.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.3.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.3.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.3.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.3.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.3.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.3.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.3.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.3.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.3.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.4.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.4.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.4.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.4.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.4.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.4.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.4.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.4.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.4.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.4.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.4.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.4.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.4.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.4.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.4.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.4.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.4.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.4.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.5.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.5.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.5.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.5.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.5.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.5.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.5.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.5.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.5.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.5.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.5.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.5.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.5.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.5.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.5.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.5.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.5.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.5.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.6.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.6.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.6.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.6.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.6.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.6.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.6.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.6.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.6.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.6.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.6.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.6.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.6.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.6.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.6.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.6.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.6.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.6.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.7.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.7.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.7.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.7.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.7.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.7.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.7.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.7.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.7.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.7.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.7.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.7.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.7.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.7.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.7.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.7.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.7.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.7.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.8.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.8.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.8.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.8.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.8.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.8.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.8.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.8.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.8.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.8.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.8.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.8.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.8.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.8.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.8.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.8.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.8.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.8.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.9.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.9.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.9.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.9.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.9.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.9.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.9.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.9.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.9.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.9.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.9.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.9.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.9.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.9.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.9.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.9.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.9.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.9.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.10.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.10.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.10.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.10.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.10.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.10.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.10.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.10.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.10.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.10.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.10.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.10.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.10.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.10.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.10.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.10.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.10.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.10.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.11.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.11.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.11.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.11.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.11.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.11.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.11.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.11.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.11.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.11.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.11.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.11.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.11.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.11.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.11.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.11.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.11.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.11.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.12.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.12.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.12.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.12.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.12.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.12.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.12.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.12.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.12.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.12.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.12.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.12.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.12.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.12.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.12.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.12.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.12.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.12.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.13.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.13.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.13.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.13.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.13.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.13.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.13.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.13.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.13.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.13.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.13.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.13.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.13.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.13.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.13.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.13.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.13.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.13.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.14.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.14.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.14.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.14.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.14.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.14.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.14.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.14.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.14.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.14.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.14.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.14.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.14.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.14.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.14.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.14.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.14.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.14.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.15.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.15.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.15.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.15.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.15.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.15.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.15.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.15.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.15.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.15.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.15.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.15.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.15.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.15.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.15.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.15.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.15.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.15.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.16.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.16.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.16.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.16.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.16.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.16.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.16.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.16.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.16.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.16.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.16.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.16.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.16.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.16.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.16.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.16.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.16.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.16.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.17.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.17.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.17.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.17.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.17.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.17.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.17.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.17.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.17.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.17.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.17.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.17.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.17.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.17.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.17.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.17.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.17.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.17.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.18.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.18.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.18.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.18.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.18.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.18.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.18.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.18.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.18.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.18.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.18.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.18.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.18.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.18.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.18.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.18.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.18.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.18.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.19.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.19.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.19.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.19.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.19.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.19.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.19.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.19.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.19.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.19.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.19.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.19.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.19.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.19.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.19.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.19.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.19.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.19.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.20.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.20.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.20.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.20.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.20.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.20.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.20.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.20.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.20.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.20.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.20.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.20.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.20.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.20.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.20.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.20.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.20.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.20.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.21.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.21.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.21.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.21.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.21.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.21.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.21.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.21.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.21.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.21.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.21.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.21.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.21.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.21.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.21.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.21.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.21.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.21.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.22.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.22.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.22.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.22.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.22.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.22.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.22.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.22.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.22.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.22.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.22.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.22.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.22.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.22.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.22.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.22.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.22.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.22.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.23.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.23.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.23.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.23.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.23.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.23.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.23.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.23.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.23.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.23.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.23.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.23.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.23.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.23.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.23.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.23.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.23.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.23.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.24.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.24.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.24.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.24.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.24.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.24.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.24.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.24.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.24.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.24.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.24.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.24.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.24.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.24.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.24.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.24.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.24.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.24.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.25.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.25.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.25.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.25.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.25.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.25.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.25.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.25.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.25.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.25.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.25.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.25.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.25.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.25.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.25.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.25.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.25.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.25.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.26.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.26.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.26.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.26.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.26.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.26.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.26.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.26.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.26.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.26.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.26.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.26.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.26.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.26.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.26.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.26.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.26.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.26.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.27.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.27.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.27.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.27.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.27.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.27.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.27.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.27.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.27.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.27.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.27.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.27.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.27.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.27.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.27.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.27.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.27.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.27.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.28.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.28.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.28.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.28.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.28.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.28.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.28.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.28.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.28.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.28.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.28.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.28.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.28.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.28.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.28.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.28.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.28.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.28.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.29.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.29.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.29.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.29.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.29.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.29.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.29.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.29.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.29.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.29.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.29.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.29.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.29.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.29.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.29.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.29.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.29.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.29.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.30.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.30.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.30.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.30.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.30.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.30.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.30.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.30.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.30.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.30.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.30.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.30.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.30.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.30.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.30.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.30.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.30.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.30.mlp.down_proj.dora_layer.lora_B.weight\n",
      "model.layers.31.self_attn.q_proj.magnitude_layer.magnitude\n",
      "model.layers.31.self_attn.q_proj.dora_layer.lora_A.weight\n",
      "model.layers.31.self_attn.q_proj.dora_layer.lora_B.weight\n",
      "model.layers.31.self_attn.k_proj.magnitude_layer.magnitude\n",
      "model.layers.31.self_attn.k_proj.dora_layer.lora_A.weight\n",
      "model.layers.31.self_attn.k_proj.dora_layer.lora_B.weight\n",
      "model.layers.31.self_attn.v_proj.magnitude_layer.magnitude\n",
      "model.layers.31.self_attn.v_proj.dora_layer.lora_A.weight\n",
      "model.layers.31.self_attn.v_proj.dora_layer.lora_B.weight\n",
      "model.layers.31.mlp.gate_proj.magnitude_layer.magnitude\n",
      "model.layers.31.mlp.gate_proj.dora_layer.lora_A.weight\n",
      "model.layers.31.mlp.gate_proj.dora_layer.lora_B.weight\n",
      "model.layers.31.mlp.up_proj.magnitude_layer.magnitude\n",
      "model.layers.31.mlp.up_proj.dora_layer.lora_A.weight\n",
      "model.layers.31.mlp.up_proj.dora_layer.lora_B.weight\n",
      "model.layers.31.mlp.down_proj.magnitude_layer.magnitude\n",
      "model.layers.31.mlp.down_proj.dora_layer.lora_A.weight\n",
      "model.layers.31.mlp.down_proj.dora_layer.lora_B.weight\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    if (\"dora_layer\" in n) or (\"magnitude_layer\" in n): \n",
    "        print(n)\n",
    "        p.data.copy_(trained_weights[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87668205-5621-4e7f-b9f4-27ba5423a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22669a99-85f5-4e73-881e-322abc12154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = dataset.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcd6446f-f9d6-4ffc-ba7b-fa688dc881b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be04a942-c691-4420-9181-784f49213999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                      | 1/32 [05:41<2:56:15, 341.15s/it]\n"
     ]
    }
   ],
   "source": [
    "short_answers_pred = []\n",
    "bs = 16\n",
    "for i in tqdm(range(0,len(valid_dataset),bs)):\n",
    "    \n",
    "    inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in valid_dataset[i:i+bs]['question']]\n",
    "    input_ids = tokenizer(inputs)['input_ids']\n",
    "    \n",
    "    max_toks = max(len(toks) for toks in input_ids)\n",
    "    b = torch.stack([torch.tensor(((max_toks-len(toks))*[tokenizer.unk_token_id])+toks) for toks in input_ids])\n",
    "    input_lens = [len(toks) for toks in input_ids]\n",
    "    \n",
    "    output = model.generate(b.cuda(), \n",
    "                            do_sample=False, \n",
    "                            use_cache=True,\n",
    "                            pad_token_id=tokenizer.unk_token_id, \n",
    "                            eos_token_id=tokenizer.eos_token_id, \n",
    "                            max_new_tokens=1024).cpu()\n",
    "    \n",
    "    pred = [extract_last_number_or_ratio(tokenizer.decode(o[o!=tokenizer.unk_token_id][n:])) for o,n in zip(output,input_lens)]\n",
    "    short_answers_pred.extend(pred)\n",
    "\n",
    "    if i > 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52de6c15-2638-4e7b-8344-5f56edbfa7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182f9a0-1cb1-4dc8-b43e-5ea87b75812e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7e827d3-4b84-4b3f-8306-090f6235d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = safetensors.torch.load_file(str(models_dir/\"test-hqq-dora/model_state_dict.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53148161-8468-4001-9669-3b83579f2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_directory = models_dir/\"test-hqq-dora/merged\"\n",
    "os.makedirs(merged_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1a29150-59dd-45c3-a3b2-a0efa83ca8bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: model.layers.0.mlp.down_proj\n",
      "Merging: model.layers.0.mlp.gate_proj\n",
      "Merging: model.layers.0.mlp.up_proj\n",
      "Merging: model.layers.0.self_attn.k_proj\n",
      "Merging: model.layers.0.self_attn.q_proj\n",
      "Merging: model.layers.0.self_attn.v_proj\n",
      "Merging: model.layers.1.mlp.down_proj\n",
      "Merging: model.layers.1.mlp.gate_proj\n",
      "Merging: model.layers.1.mlp.up_proj\n",
      "Merging: model.layers.1.self_attn.k_proj\n",
      "Merging: model.layers.1.self_attn.q_proj\n",
      "Merging: model.layers.1.self_attn.v_proj\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m      3\u001b[0m     weights \u001b[38;5;241m=\u001b[39m safetensors\u001b[38;5;241m.\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload_file(filename)\n\u001b[0;32m----> 4\u001b[0m     weights_copy \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, frozen_weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(weights\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m      7\u001b[0m         module_key, _, value_key \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.11/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/lib/python3.11/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m/weka/home-keremturgutlu/py_venvs/fsdp-qlora-py311/lib/python3.11/site-packages/torch/_tensor.py:122\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default implementation of __deepcopy__() for wrapper subclasses \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly works for subclass types that implement clone() and for which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferent type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     new_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_typed_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quantized:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;66;03m# quantizer_params can be different type based on torch attribute\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         quantizer_params: Union[\n\u001b[1;32m    126\u001b[0m             Tuple[torch\u001b[38;5;241m.\u001b[39mqscheme, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    127\u001b[0m             Tuple[torch\u001b[38;5;241m.\u001b[39mqscheme, Tensor, Tensor, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    128\u001b[0m         ]\n",
      "File \u001b[0;32m/weka/home-keremturgutlu/py_venvs/fsdp-qlora-py311/lib/python3.11/site-packages/torch/storage.py:839\u001b[0m, in \u001b[0;36mTypedStorage._deepcopy\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deepcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, memo):\n\u001b[0;32m--> 839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_wrapped_storage(\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m/weka/home-keremturgutlu/py_venvs/fsdp-qlora-py311/lib/python3.11/site-packages/torch/storage.py:112\u001b[0m, in \u001b[0;36m_StorageBase.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata]\n\u001b[0;32m--> 112\u001b[0m new_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m memo[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata] \u001b[38;5;241m=\u001b[39m new_storage\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_storage\n",
      "File \u001b[0;32m/weka/home-keremturgutlu/py_venvs/fsdp-qlora-py311/lib/python3.11/site-packages/torch/storage.py:126\u001b[0m, in \u001b[0;36m_StorageBase.clone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a copy of this storage.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Merge and Save\n",
    "for filename in files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    weights_copy = copy.deepcopy(weights)\n",
    "\n",
    "    for k, frozen_weight in iter(weights.items()):\n",
    "        module_key, _, value_key = k.rpartition(\".\")\n",
    "        if value_key == \"weight\" and module_key + \".dora_layer.lora_A.weight\" in trained_weights:\n",
    "            print(\"Merging:\", module_key)\n",
    "    \n",
    "            lora_A_weight = trained_weights[module_key + \".dora_layer.lora_A.weight\"]\n",
    "            lora_B_weight = trained_weights[module_key + \".dora_layer.lora_B.weight\"]\n",
    "            magnitude = trained_weights[module_key + \".magnitude_layer.magnitude\"]\n",
    "    \n",
    "            weight = (frozen_weight + lora_B_weight @ lora_A_weight)\n",
    "            norm_adapted = weight / weight.norm(p=2, dim=1).view(-1,1)\n",
    "            new_weight = norm_adapted * magnitude.view(-1,1)\n",
    "    \n",
    "            weights_copy[k] = new_weight\n",
    "    save_file(weights_copy, merged_directory/Path(filename).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9c945-6c08-4258-8792-fd42e1b7aa15",
   "metadata": {},
   "source": [
    "### Quantized-Llama-Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36e8b5f0-a967-4174-b6b7-ee680f5668b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "skip_modules = [\"lm_head\"]\n",
    "load_param_skip_names = ['inv_freq']\n",
    "compute_dtype = torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "38dc24f9-a61b-439e-a479-bc2958374276",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pro_path = Path(\"/weka/home-keremturgutlu/models/meta-llama/Llama-2-7b-hf_blk_exp-32-35/\")\n",
    "num_original_layers, num_expanded_layers = llama_pro_path.name.split(\"blk_exp-\")[1].split(\"-\")\n",
    "num_original_layers, num_expanded_layers = int(num_original_layers), int(num_expanded_layers)\n",
    "total_new_layers = num_expanded_layers - num_original_layers\n",
    "split = int(num_original_layers / (num_expanded_layers - num_original_layers))\n",
    "new_layer_ids = [split+(split+1)*n for n in range(total_new_layers)]\n",
    "new_layer_names = [f\"layers.{i}\" for i in new_layer_ids]\n",
    "skip_modules += [str(lid) for lid in new_layer_ids]\n",
    "cfg.num_hidden_layers = num_expanded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0701257-e397-4a92-9ee6-e70742054da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lm_head', '10', '21', '32']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5c60f088-b639-4122-86a4-04ea42a4cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = safetensors.torch.load_file(glob(str(models_dir/\"llama-7b-orca-math-10k-bnb-llama-pro/*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f50e06e6-8d37-42d2-a3fc-37ff314f7d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 10\n",
      "Skipping 21\n",
      "Skipping 32\n"
     ]
    }
   ],
   "source": [
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                 quant_type='nf4', quant_storage=torch_dtype, skip_modules=skip_modules)\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06ad5a4e-7807-4b83-b6ae-6f604c49ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_files = glob(str(llama_pro_path/\"*.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "935d833b-7eab-4b16-b9fd-0168258b6428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model.layers.26.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.27.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.28.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.29.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.30.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.31.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.32.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.33.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.34.self_attn.rotary_emb.inv_freq because it is in skip_names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model.layers.0.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.1.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.10.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.11.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.12.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.13.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.14.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.15.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.16.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.17.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.18.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.19.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.2.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.20.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.21.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.22.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.23.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.24.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.25.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.3.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.4.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.5.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.6.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.7.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.8.self_attn.rotary_emb.inv_freq because it is in skip_names\n",
      "Skipping model.layers.9.self_attn.rotary_emb.inv_freq because it is in skip_names\n"
     ]
    }
   ],
   "source": [
    "for filename in pretrained_files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    parallel(load_and_quantize_parallel, \n",
    "             iter(weights.items()), \n",
    "             n_workers=8, \n",
    "             threadpool=True,\n",
    "             model=model, \n",
    "             dtype=torch_dtype, \n",
    "             device=torch.cuda.current_device(),\n",
    "             skip_names=load_param_skip_names,\n",
    "             is_meta_rank=False,\n",
    "             verbose=True,\n",
    "             quant_method=\"bnb\",\n",
    "             is_dora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9629ceac-3868-4188-a201-4e32a30e921c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.32.self_attn.q_proj.weight\n",
      "model.layers.32.self_attn.k_proj.weight\n",
      "model.layers.32.self_attn.v_proj.weight\n",
      "model.layers.32.self_attn.o_proj.weight\n",
      "model.layers.32.mlp.gate_proj.weight\n",
      "model.layers.32.mlp.up_proj.weight\n",
      "model.layers.32.mlp.down_proj.weight\n",
      "model.layers.32.input_layernorm.weight\n",
      "model.layers.32.post_attention_layernorm.weight\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    if n in trained_weights: \n",
    "        print(n)\n",
    "        p.data.copy_(trained_weights[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5c57de1-b7ec-49d3-a70a-85d2457b0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1945bb5d-bf7c-4d43-9c07-40d905575069",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = dataset.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20903c99-a9d0-4780-a400-ecf3fb471083",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32b36bd4-e9cb-41a8-8c39-6674c61b6436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                      | 1/32 [02:49<1:27:37, 169.59s/it]\n"
     ]
    }
   ],
   "source": [
    "short_answers_pred = []\n",
    "bs = 16\n",
    "for i in tqdm(range(0,len(valid_dataset),bs)):\n",
    "    \n",
    "    inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in valid_dataset[i:i+bs]['question']]\n",
    "    input_ids = tokenizer(inputs)['input_ids']\n",
    "    \n",
    "    max_toks = max(len(toks) for toks in input_ids)\n",
    "    b = torch.stack([torch.tensor(((max_toks-len(toks))*[tokenizer.unk_token_id])+toks) for toks in input_ids])\n",
    "    input_lens = [len(toks) for toks in input_ids]\n",
    "    \n",
    "    output = model.generate(b.cuda(), \n",
    "                            do_sample=False, \n",
    "                            use_cache=True,\n",
    "                            pad_token_id=tokenizer.unk_token_id, \n",
    "                            eos_token_id=tokenizer.eos_token_id, \n",
    "                            max_new_tokens=1024).cpu()\n",
    "    \n",
    "    pred = [extract_last_number_or_ratio(tokenizer.decode(o[o!=tokenizer.unk_token_id][n:])) for o,n in zip(output,input_lens)]\n",
    "    short_answers_pred.extend(pred)\n",
    "\n",
    "    if i > 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f676f014-5e4d-4672-bef5-4879e0c73cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21875"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a3dfa-48af-4026-9417-605e694eec85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c6280-dbf9-4d45-b514-682ba04c70c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8d45a-c8ab-4de4-b27a-419a543e3518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cd43e-2a79-4666-ad6e-fdc649b6a83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f1b58-f1db-444c-a244-6403c22a5f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c3a7e-1e05-47ff-9117-a68e568b9264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b70772-274c-4e29-a130-065480e25caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2da6e-37cc-4b1d-8076-96da898da554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795a5f6-425d-40de-9687-38d627bef721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc68c4-22cf-4114-be80-8f58d196cef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074c2f8-17c6-45ed-acb0-33b1a9e315ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f65fd-e9aa-4a89-9579-aaa21b2db8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a532e54-9789-48c6-90cd-46b718a96706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dcfb19-d0f1-47a6-a53a-e3c01a225b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee73f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"model_name\" : 'meta-llama/Llama-2-7b-hf'}\n",
    "args['expansion_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa17de-d41f-4414-8547-b1f5f2de12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = hub.cached_file(args[\"model_name\"], SAFE_WEIGHTS_INDEX_NAME)\n",
    "files, _ = hub.get_checkpoint_shard_files(args[\"model_name\"], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b22c0-3b7f-4cdf-baad-1e9434bfe6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/admin/home-keremturgutlu/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model-00001-of-00002.safetensors',\n",
       " '/admin/home-keremturgutlu/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model-00002-of-00002.safetensors']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(args[\"model_name\"])\n",
    "num_original_layers = cfg.num_hidden_layers\n",
    "num_new_layers = num_original_layers + int(num_original_layers * args['expansion_rate'])\n",
    "split = int(num_original_layers / (num_new_layers - num_original_layers))\n",
    "layer_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46b774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 32, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_layers, num_original_layers, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff4e8d20-61e4-41df-b049-52a035d86b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model.layers.10.input_layernorm.weight to 11\n",
      "Moving model.layers.10.mlp.down_proj.weight to 11\n",
      "Moving model.layers.10.mlp.gate_proj.weight to 11\n",
      "Moving model.layers.10.mlp.up_proj.weight to 11\n",
      "Moving model.layers.10.post_attention_layernorm.weight to 11\n",
      "Moving model.layers.10.self_attn.k_proj.weight to 11\n",
      "Moving model.layers.10.self_attn.o_proj.weight to 11\n",
      "Moving model.layers.10.self_attn.q_proj.weight to 11\n",
      "Moving model.layers.10.self_attn.rotary_emb.inv_freq to 11\n",
      "Moving model.layers.10.self_attn.v_proj.weight to 11\n",
      "Moving model.layers.11.input_layernorm.weight to 12\n",
      "Moving model.layers.11.mlp.down_proj.weight to 12\n",
      "Moving model.layers.11.mlp.gate_proj.weight to 12\n",
      "Moving model.layers.11.mlp.up_proj.weight to 12\n",
      "Moving model.layers.11.post_attention_layernorm.weight to 12\n",
      "Moving model.layers.11.self_attn.k_proj.weight to 12\n",
      "Moving model.layers.11.self_attn.o_proj.weight to 12\n",
      "Moving model.layers.11.self_attn.q_proj.weight to 12\n",
      "Moving model.layers.11.self_attn.rotary_emb.inv_freq to 12\n",
      "Moving model.layers.11.self_attn.v_proj.weight to 12\n",
      "Moving model.layers.12.input_layernorm.weight to 13\n",
      "Moving model.layers.12.mlp.down_proj.weight to 13\n",
      "Moving model.layers.12.mlp.gate_proj.weight to 13\n",
      "Moving model.layers.12.mlp.up_proj.weight to 13\n",
      "Moving model.layers.12.post_attention_layernorm.weight to 13\n",
      "Moving model.layers.12.self_attn.k_proj.weight to 13\n",
      "Moving model.layers.12.self_attn.o_proj.weight to 13\n",
      "Moving model.layers.12.self_attn.q_proj.weight to 13\n",
      "Moving model.layers.12.self_attn.rotary_emb.inv_freq to 13\n",
      "Moving model.layers.12.self_attn.v_proj.weight to 13\n",
      "Moving model.layers.13.input_layernorm.weight to 14\n",
      "Moving model.layers.13.mlp.down_proj.weight to 14\n",
      "Moving model.layers.13.mlp.gate_proj.weight to 14\n",
      "Moving model.layers.13.mlp.up_proj.weight to 14\n",
      "Moving model.layers.13.post_attention_layernorm.weight to 14\n",
      "Moving model.layers.13.self_attn.k_proj.weight to 14\n",
      "Moving model.layers.13.self_attn.o_proj.weight to 14\n",
      "Moving model.layers.13.self_attn.q_proj.weight to 14\n",
      "Moving model.layers.13.self_attn.rotary_emb.inv_freq to 14\n",
      "Moving model.layers.13.self_attn.v_proj.weight to 14\n",
      "Moving model.layers.14.input_layernorm.weight to 15\n",
      "Moving model.layers.14.mlp.down_proj.weight to 15\n",
      "Moving model.layers.14.mlp.gate_proj.weight to 15\n",
      "Moving model.layers.14.mlp.up_proj.weight to 15\n",
      "Moving model.layers.14.post_attention_layernorm.weight to 15\n",
      "Moving model.layers.14.self_attn.k_proj.weight to 15\n",
      "Moving model.layers.14.self_attn.o_proj.weight to 15\n",
      "Moving model.layers.14.self_attn.q_proj.weight to 15\n",
      "Moving model.layers.14.self_attn.rotary_emb.inv_freq to 15\n",
      "Moving model.layers.14.self_attn.v_proj.weight to 15\n",
      "Moving model.layers.15.input_layernorm.weight to 16\n",
      "Moving model.layers.15.mlp.down_proj.weight to 16\n",
      "Moving model.layers.15.mlp.gate_proj.weight to 16\n",
      "Moving model.layers.15.mlp.up_proj.weight to 16\n",
      "Moving model.layers.15.post_attention_layernorm.weight to 16\n",
      "Moving model.layers.15.self_attn.k_proj.weight to 16\n",
      "Moving model.layers.15.self_attn.o_proj.weight to 16\n",
      "Moving model.layers.15.self_attn.q_proj.weight to 16\n",
      "Moving model.layers.15.self_attn.rotary_emb.inv_freq to 16\n",
      "Moving model.layers.15.self_attn.v_proj.weight to 16\n",
      "Moving model.layers.16.input_layernorm.weight to 17\n",
      "Moving model.layers.16.mlp.down_proj.weight to 17\n",
      "Moving model.layers.16.mlp.gate_proj.weight to 17\n",
      "Moving model.layers.16.mlp.up_proj.weight to 17\n",
      "Moving model.layers.16.post_attention_layernorm.weight to 17\n",
      "Moving model.layers.16.self_attn.k_proj.weight to 17\n",
      "Moving model.layers.16.self_attn.o_proj.weight to 17\n",
      "Moving model.layers.16.self_attn.q_proj.weight to 17\n",
      "Moving model.layers.16.self_attn.rotary_emb.inv_freq to 17\n",
      "Moving model.layers.16.self_attn.v_proj.weight to 17\n",
      "Moving model.layers.17.input_layernorm.weight to 18\n",
      "Moving model.layers.17.mlp.down_proj.weight to 18\n",
      "Moving model.layers.17.mlp.gate_proj.weight to 18\n",
      "Moving model.layers.17.mlp.up_proj.weight to 18\n",
      "Moving model.layers.17.post_attention_layernorm.weight to 18\n",
      "Moving model.layers.17.self_attn.k_proj.weight to 18\n",
      "Moving model.layers.17.self_attn.o_proj.weight to 18\n",
      "Moving model.layers.17.self_attn.q_proj.weight to 18\n",
      "Moving model.layers.17.self_attn.rotary_emb.inv_freq to 18\n",
      "Moving model.layers.17.self_attn.v_proj.weight to 18\n",
      "Moving model.layers.18.input_layernorm.weight to 19\n",
      "Moving model.layers.18.mlp.down_proj.weight to 19\n",
      "Moving model.layers.18.mlp.gate_proj.weight to 19\n",
      "Moving model.layers.18.mlp.up_proj.weight to 19\n",
      "Moving model.layers.18.post_attention_layernorm.weight to 19\n",
      "Moving model.layers.18.self_attn.k_proj.weight to 19\n",
      "Moving model.layers.18.self_attn.o_proj.weight to 19\n",
      "Moving model.layers.18.self_attn.q_proj.weight to 19\n",
      "Moving model.layers.18.self_attn.rotary_emb.inv_freq to 19\n",
      "Moving model.layers.18.self_attn.v_proj.weight to 19\n",
      "Moving model.layers.19.input_layernorm.weight to 20\n",
      "Moving model.layers.19.mlp.down_proj.weight to 20\n",
      "Moving model.layers.19.mlp.gate_proj.weight to 20\n",
      "Moving model.layers.19.mlp.up_proj.weight to 20\n",
      "Moving model.layers.19.post_attention_layernorm.weight to 20\n",
      "Moving model.layers.19.self_attn.k_proj.weight to 20\n",
      "Moving model.layers.19.self_attn.o_proj.weight to 20\n",
      "Moving model.layers.19.self_attn.q_proj.weight to 20\n",
      "Moving model.layers.19.self_attn.rotary_emb.inv_freq to 20\n",
      "Moving model.layers.19.self_attn.v_proj.weight to 20\n",
      "Moving model.layers.20.input_layernorm.weight to 22\n",
      "Moving model.layers.20.mlp.down_proj.weight to 22\n",
      "Moving model.layers.20.mlp.gate_proj.weight to 22\n",
      "Moving model.layers.20.mlp.up_proj.weight to 22\n",
      "Moving model.layers.20.post_attention_layernorm.weight to 22\n",
      "Moving model.layers.20.self_attn.k_proj.weight to 22\n",
      "Moving model.layers.20.self_attn.o_proj.weight to 22\n",
      "Moving model.layers.20.self_attn.q_proj.weight to 22\n",
      "Moving model.layers.20.self_attn.rotary_emb.inv_freq to 22\n",
      "Moving model.layers.20.self_attn.v_proj.weight to 22\n",
      "Moving model.layers.21.input_layernorm.weight to 23\n",
      "Moving model.layers.21.mlp.down_proj.weight to 23\n",
      "Moving model.layers.21.mlp.gate_proj.weight to 23\n",
      "Moving model.layers.21.mlp.up_proj.weight to 23\n",
      "Moving model.layers.21.post_attention_layernorm.weight to 23\n",
      "Moving model.layers.21.self_attn.k_proj.weight to 23\n",
      "Moving model.layers.21.self_attn.o_proj.weight to 23\n",
      "Moving model.layers.21.self_attn.q_proj.weight to 23\n",
      "Moving model.layers.21.self_attn.rotary_emb.inv_freq to 23\n",
      "Moving model.layers.21.self_attn.v_proj.weight to 23\n",
      "Moving model.layers.22.input_layernorm.weight to 24\n",
      "Moving model.layers.22.mlp.down_proj.weight to 24\n",
      "Moving model.layers.22.mlp.gate_proj.weight to 24\n",
      "Moving model.layers.22.mlp.up_proj.weight to 24\n",
      "Moving model.layers.22.post_attention_layernorm.weight to 24\n",
      "Moving model.layers.22.self_attn.k_proj.weight to 24\n",
      "Moving model.layers.22.self_attn.o_proj.weight to 24\n",
      "Moving model.layers.22.self_attn.q_proj.weight to 24\n",
      "Moving model.layers.22.self_attn.rotary_emb.inv_freq to 24\n",
      "Moving model.layers.22.self_attn.v_proj.weight to 24\n",
      "Moving model.layers.23.input_layernorm.weight to 25\n",
      "Moving model.layers.23.mlp.down_proj.weight to 25\n",
      "Moving model.layers.23.mlp.gate_proj.weight to 25\n",
      "Moving model.layers.23.mlp.up_proj.weight to 25\n",
      "Moving model.layers.23.post_attention_layernorm.weight to 25\n",
      "Moving model.layers.23.self_attn.k_proj.weight to 25\n",
      "Moving model.layers.23.self_attn.o_proj.weight to 25\n",
      "Moving model.layers.23.self_attn.q_proj.weight to 25\n",
      "Moving model.layers.23.self_attn.rotary_emb.inv_freq to 25\n",
      "Moving model.layers.23.self_attn.v_proj.weight to 25\n",
      "Moving model.layers.24.input_layernorm.weight to 26\n",
      "Moving model.layers.24.mlp.down_proj.weight to 26\n",
      "Moving model.layers.24.mlp.gate_proj.weight to 26\n",
      "Moving model.layers.24.mlp.up_proj.weight to 26\n",
      "Moving model.layers.24.post_attention_layernorm.weight to 26\n",
      "Moving model.layers.24.self_attn.k_proj.weight to 26\n",
      "Moving model.layers.24.self_attn.o_proj.weight to 26\n",
      "Moving model.layers.24.self_attn.q_proj.weight to 26\n",
      "Moving model.layers.24.self_attn.rotary_emb.inv_freq to 26\n",
      "Moving model.layers.24.self_attn.v_proj.weight to 26\n",
      "Moving model.layers.25.input_layernorm.weight to 27\n",
      "Moving model.layers.25.mlp.down_proj.weight to 27\n",
      "Moving model.layers.25.mlp.gate_proj.weight to 27\n",
      "Moving model.layers.25.mlp.up_proj.weight to 27\n",
      "Moving model.layers.25.post_attention_layernorm.weight to 27\n",
      "Moving model.layers.25.self_attn.k_proj.weight to 27\n",
      "Moving model.layers.25.self_attn.o_proj.weight to 27\n",
      "Moving model.layers.25.self_attn.q_proj.weight to 27\n",
      "Moving model.layers.25.self_attn.rotary_emb.inv_freq to 27\n",
      "Moving model.layers.25.self_attn.v_proj.weight to 27\n",
      "Moving model.layers.26.input_layernorm.weight to 28\n",
      "Moving model.layers.26.mlp.down_proj.weight to 28\n",
      "Moving model.layers.26.mlp.gate_proj.weight to 28\n",
      "Moving model.layers.26.mlp.up_proj.weight to 28\n",
      "Moving model.layers.26.post_attention_layernorm.weight to 28\n",
      "Moving model.layers.26.self_attn.k_proj.weight to 28\n",
      "Moving model.layers.26.self_attn.o_proj.weight to 28\n",
      "Moving model.layers.26.self_attn.q_proj.weight to 28\n",
      "Moving model.layers.26.self_attn.rotary_emb.inv_freq to 28\n",
      "Moving model.layers.26.self_attn.v_proj.weight to 28\n",
      "Moving model.layers.27.input_layernorm.weight to 29\n",
      "Moving model.layers.27.mlp.down_proj.weight to 29\n",
      "Moving model.layers.27.mlp.gate_proj.weight to 29\n",
      "Moving model.layers.27.mlp.up_proj.weight to 29\n",
      "Moving model.layers.27.post_attention_layernorm.weight to 29\n",
      "Moving model.layers.27.self_attn.k_proj.weight to 29\n",
      "Moving model.layers.27.self_attn.o_proj.weight to 29\n",
      "Moving model.layers.27.self_attn.q_proj.weight to 29\n",
      "Moving model.layers.27.self_attn.rotary_emb.inv_freq to 29\n",
      "Moving model.layers.27.self_attn.v_proj.weight to 29\n",
      "Moving model.layers.28.input_layernorm.weight to 30\n",
      "Moving model.layers.28.mlp.down_proj.weight to 30\n",
      "Moving model.layers.28.mlp.gate_proj.weight to 30\n",
      "Moving model.layers.28.mlp.up_proj.weight to 30\n",
      "Moving model.layers.28.post_attention_layernorm.weight to 30\n",
      "Moving model.layers.28.self_attn.k_proj.weight to 30\n",
      "Moving model.layers.28.self_attn.o_proj.weight to 30\n",
      "Moving model.layers.28.self_attn.q_proj.weight to 30\n",
      "Moving model.layers.28.self_attn.rotary_emb.inv_freq to 30\n",
      "Moving model.layers.28.self_attn.v_proj.weight to 30\n",
      "Moving model.layers.29.input_layernorm.weight to 31\n",
      "Moving model.layers.29.mlp.down_proj.weight to 31\n",
      "Moving model.layers.29.mlp.gate_proj.weight to 31\n",
      "Moving model.layers.29.mlp.up_proj.weight to 31\n",
      "Moving model.layers.29.post_attention_layernorm.weight to 31\n",
      "Moving model.layers.29.self_attn.k_proj.weight to 31\n",
      "Moving model.layers.29.self_attn.o_proj.weight to 31\n",
      "Moving model.layers.29.self_attn.q_proj.weight to 31\n",
      "Moving model.layers.29.self_attn.rotary_emb.inv_freq to 31\n",
      "Moving model.layers.29.self_attn.v_proj.weight to 31\n",
      "Moving model.layers.30.input_layernorm.weight to 33\n",
      "Moving model.layers.30.mlp.down_proj.weight to 33\n",
      "Moving model.layers.30.mlp.gate_proj.weight to 33\n",
      "Moving model.layers.30.mlp.up_proj.weight to 33\n",
      "Moving model.layers.30.post_attention_layernorm.weight to 33\n",
      "Moving model.layers.30.self_attn.k_proj.weight to 33\n",
      "Moving model.layers.30.self_attn.o_proj.weight to 33\n",
      "Moving model.layers.30.self_attn.q_proj.weight to 33\n",
      "Moving model.layers.30.self_attn.rotary_emb.inv_freq to 33\n",
      "Moving model.layers.30.self_attn.v_proj.weight to 33\n",
      "Moving model.layers.31.input_layernorm.weight to 34\n",
      "Moving model.layers.31.mlp.down_proj.weight to 34\n",
      "Moving model.layers.31.mlp.gate_proj.weight to 34\n",
      "Moving model.layers.31.mlp.up_proj.weight to 34\n",
      "Moving model.layers.31.post_attention_layernorm.weight to 34\n",
      "Moving model.layers.31.self_attn.k_proj.weight to 34\n",
      "Moving model.layers.31.self_attn.o_proj.weight to 34\n",
      "Moving model.layers.31.self_attn.q_proj.weight to 34\n",
      "Moving model.layers.31.self_attn.rotary_emb.inv_freq to 34\n",
      "Moving model.layers.31.self_attn.v_proj.weight to 34\n"
     ]
    }
   ],
   "source": [
    "expanded_weights = {}\n",
    "for filename in files:\n",
    "    weights = safetensors.torch.load_file(filename)\n",
    "    for k,v in iter(weights.items()):\n",
    "        if 'layers' in k:\n",
    "            layer_no = int(k.split('layers.')[1].split('.')[0])\n",
    "            # shift existing layers by previously added layers\n",
    "            new_layer_no = layer_no + layer_no // split\n",
    "            if new_layer_no != layer_no:\n",
    "                print(f\"Moving {k} to {new_layer_no}\")\n",
    "            new_k = k.replace(f'layers.{layer_no}', f'layers.{new_layer_no}')\n",
    "            expanded_weights[new_k] = v\n",
    "            # add new layers\n",
    "            if (layer_no+1) % split == 0:\n",
    "                new_layer_no += 1\n",
    "                new_k = k.replace(f'layers.{layer_no}', f'layers.{new_layer_no}')\n",
    "                if 'down_proj' in k or 'o_proj' in k:\n",
    "                    expanded_weights[new_k] = torch.zeros_like(v)     \n",
    "                else:\n",
    "                    expanded_weights[new_k] = v\n",
    "        else:\n",
    "            expanded_weights[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4271c8aa-a555-4f55-a9cb-024c443b6d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(expanded_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81cb2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set([k.split(\"layers.\")[1].split(\".\")[0] for k in expanded_weights.keys() if \"layers\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d303fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(expanded_weights['model.layers.31.mlp.up_proj.weight'],expanded_weights['model.layers.32.mlp.up_proj.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40ccf811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., dtype=torch.float16), tensor(0., dtype=torch.float16))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_weights['model.layers.32.mlp.down_proj.weight'].min(), expanded_weights['model.layers.32.mlp.down_proj.weight'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2936f60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., dtype=torch.float16), tensor(0., dtype=torch.float16))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_weights['model.layers.32.self_attn.o_proj.weight'].min(), expanded_weights['model.layers.32.self_attn.o_proj.weight'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa740f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pro_path = Path(\"/weka/home-keremturgutlu/models/meta-llama/Llama-2-7b-hf_blk_exp-32-35\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37444511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/weka/home-keremturgutlu/models/meta-llama/Llama-2-7b-hf_blk_exp-32-35/model-00002-of-00002.safetensors',\n",
       " '/weka/home-keremturgutlu/models/meta-llama/Llama-2-7b-hf_blk_exp-32-35/model-00001-of-00002.safetensors']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = glob(str(llama_pro_path/\"*.safetensors\")); filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f4c609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_nums = set()\n",
    "for filename in filenames:\n",
    "    keys = list(safetensors.torch.load_file(str(filename)).keys())\n",
    "    layer_nums = layer_nums.union(set([int(k.split(\"layers.\")[1].split(\".\")[0]) for k in keys if 'layers' in k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e26663",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_original_layers, num_expanded_layers = llama_pro_path.name.split(\"blk_exp-\")[1].split(\"-\")\n",
    "num_original_layers, num_expanded_layers = int(num_original_layers), int(num_expanded_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c109767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = int(num_original_layers / (num_expanded_layers - num_original_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c64c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_new_layers = num_expanded_layers - num_original_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0048593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer_ids = [split + (split + 1)*n for n in range(total_new_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20b0d820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 10 is new\n",
      "Layer 21 is new\n",
      "Layer 32 is new\n"
     ]
    }
   ],
   "source": [
    "for layer_no in range(num_expanded_layers):\n",
    "    if layer_no in new_layer_ids:\n",
    "        print(f\"Layer {layer_no} is new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d2fb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_weights = {}\n",
    "for filename in filenames:\n",
    "    weights = safetensors.torch.load_file(str(filename))\n",
    "    for k,v in iter(weights.items()):\n",
    "        if any(((f\"layers.{i}\" in k) or (f\"layers.{i-1}\" in k) for i in new_layer_ids)):\n",
    "            verify_weights[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d6e2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in verify_weights.items():\n",
    "    if any(((f\"layers.{i}\" in k) for i in new_layer_ids)):\n",
    "        if 'down_proj' in k or 'o_proj' in k:\n",
    "            assert torch.equal(v, torch.zeros_like(v))\n",
    "        else:\n",
    "            lid = int(k.split(\"layers.\")[1].split(\".\")[0])\n",
    "            assert torch.equal(verify_weights[k.replace(f\"layers.{lid}\", f\"layers.{lid-1}\")], v)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dabe7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(args[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d81ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "457300ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_children(model):\n",
    "    for n,c in model.named_children():\n",
    "        # print(n,  c.__class__)\n",
    "        if n in [str(i) for i in [10,21,32]] + [\"lm_head\"]: \n",
    "            print(\"skipped\", n, c.__class__)\n",
    "            continue\n",
    "        if len(list(model.children())) > 0:\n",
    "            # if n in [str(i) for i in range(32)]: \n",
    "            #     print(\"replaced:\", n)\n",
    "            recursive_children(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "585d11f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 10 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "skipped 21 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "skipped lm_head <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "recursive_children(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97746ec-ca8c-43bc-945b-c8676b3d2513",
   "metadata": {},
   "source": [
    "### ORCA-Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69d2d0c0-aff4-4973-93b3-410b11e72333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b00deca1-6201-4324-93df-7f9f2953bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba8ac98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"microsoft/orca-math-word-problems-200k\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "644bc3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ebe1b4-088b-4db0-b0b2-da3dec6575c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 10000\n",
    "train_ds = ds.select(range(0, ntrain))\n",
    "valid_ds = ds.select(range(ntrain, len(ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49d4634c-1e1d-4a91-82ad-3bba110e0d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 10000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 190035\n",
       " }))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff099efd-131f-4305-8a8e-fe9fa9214b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Sally had 13 peaches at her roadside fruit dish.  She went to the orchard and picked peaches to stock up. She picked 55 peaches. There are _____ peaches now.',\n",
       " 'answer': 'Sally originally had 13 peaches. She picked 55 more peaches. To find out the total number of peaches she has now, we add the two amounts together:\\n\\n13 (original peaches) + 55 (picked peaches) = 68 peaches\\n\\nSo, there are 68 peaches now.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75488776-e427-4089-8608-336506b87f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, style=\"alpaca\"):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.style = style\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        IGNORE_INDEX = -100  # The default setting in CrossEntropyLoss\n",
    "        if self.style == \"guanaco\":\n",
    "            prompt = self.dataset[index][\"text\"].split(\"### Assistant: \")[0]\n",
    "            example = self.dataset[index][\"text\"]\n",
    "        elif self.style == \"qna\":\n",
    "            prompt_template = \"###Context:\\n{context}\\n###Question:\\n{question}\\n###Answer:\\n\"\n",
    "            sample = self.dataset[index]\n",
    "            prompt = prompt_template.format_map(sample)\n",
    "            example = prompt + sample['answer']\n",
    "        elif self.style == \"qna_no_ctx\":\n",
    "            prompt_template = \"###Question:\\n{question}\\n###Answer:\\n\"\n",
    "            sample = self.dataset[index]\n",
    "            prompt = prompt_template.format_map(sample)\n",
    "            example = prompt + sample['answer']            \n",
    "        else: # Alpaca\n",
    "            ann = self.dataset[index]\n",
    "            if ann.get(\"input\", \"\") == \"\":\n",
    "                prompt = PROMPT_DICT[\"prompt_no_input\"].format_map(ann)\n",
    "            else:\n",
    "                prompt = PROMPT_DICT[\"prompt_input\"].format_map(ann)\n",
    "            example = prompt + ann[\"output\"]\n",
    "\n",
    "        prompt = torch.tensor(\n",
    "            self.tokenizer.encode(prompt), dtype=torch.int64\n",
    "        )\n",
    "        example = self.tokenizer.encode(example)\n",
    "        example.append(self.tokenizer.eos_token_id)\n",
    "        example = torch.tensor(\n",
    "            example, dtype=torch.int64\n",
    "        )\n",
    "        labels = copy.deepcopy(example)\n",
    "        labels[: len(prompt)] = -1\n",
    "        example_mask = example.ge(0)\n",
    "        label_mask = labels.ge(0)\n",
    "        example[~example_mask] = 0\n",
    "        labels[~label_mask] = IGNORE_INDEX\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": example.tolist(),\n",
    "            \"labels\": labels.tolist(),\n",
    "            \"attention_mask\":example_mask.tolist(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e61b72f0-d6ba-47cb-96af-0037601c1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InstructionDataset(train_ds, tokenizer, style=\"qna_no_ctx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47ca94ee-bfc9-401b-ba30-213a4ad34dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlens = [len(d['input_ids'] + d['labels']) for d in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c04d473e-deed-446d-9cd6-6b4373dbf90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1019614/3341377964.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(seqlens, bins=10).sort_index()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(65.425, 325.4]      844\n",
       "(325.4, 582.8]      2189\n",
       "(582.8, 840.2]      2846\n",
       "(840.2, 1097.6]     2114\n",
       "(1097.6, 1355.0]    1203\n",
       "(1355.0, 1612.4]     503\n",
       "(1612.4, 1869.8]     202\n",
       "(1869.8, 2127.2]      70\n",
       "(2127.2, 2384.6]      20\n",
       "(2384.6, 2642.0]       9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(seqlens, bins=10).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78d410-d38a-434c-b2f4-3fb0cc520d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afa337-b931-43ee-b803-b67199561e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
