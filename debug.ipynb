{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/py_venvs/qdora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from accelerate import init_empty_weights\n",
    "from scripts.dataset_utils import get_dataloader, InstructionDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_8b = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "tokenizer_70b = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same tokenizer for 3.1 models\n",
    "assert tokenizer_8b.special_tokens_map == tokenizer_70b.special_tokens_map\n",
    "assert tokenizer_8b.vocab == tokenizer_70b.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<|begin_of_text|>', '<|eot_id|>'], 128006)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_8b.all_special_tokens, tokenizer_8b.vocab['<|start_header_id|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " '<|end_of_text|>',\n",
       " '<|reserved_special_token_0|>',\n",
       " '<|reserved_special_token_1|>',\n",
       " '<|finetune_right_pad_id|>',\n",
       " '<|reserved_special_token_2|>',\n",
       " '<|start_header_id|>',\n",
       " '<|end_header_id|>',\n",
       " '<|eom_id|>',\n",
       " '<|eot_id|>',\n",
       " '<|python_tag|>',\n",
       " '<|reserved_special_token_3|>',\n",
       " '<|reserved_special_token_4|>',\n",
       " '<|reserved_special_token_5|>',\n",
       " '<|reserved_special_token_6|>',\n",
       " '<|reserved_special_token_7|>',\n",
       " '<|reserved_special_token_8|>',\n",
       " '<|reserved_special_token_9|>',\n",
       " '<|reserved_special_token_10|>',\n",
       " '<|reserved_special_token_11|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_8b.convert_ids_to_tokens([tokenizer_8b.vocab_size+i for i in range(len(tokenizer_8b.vocab) - tokenizer_8b.vocab_size)])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"/workspace/data/llama_large_mix_dataset_v0_1536\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'output_text', 'input_token_ids', 'output_token_ids', 'token_length'],\n",
       "    num_rows: 90417\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "sample = ds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the effect of decreasing the pH on the formation of the copper(II) EDTA complex in a solution containing a fixed concentration of copper(II) ion and excess EDTA? Provide a complete explanation using relevant chemical equations and equilibrium constants.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample['input_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the pH of a solution containing a fixed concentration of copper(II) ion and excess EDTA is decreased, the formation of the copper(II) EDTA complex is affected. To understand this, let's first look at the relevant chemical equations and equilibrium constants.\n",
      "\n",
      "The formation of the copper(II) EDTA complex can be represented by the following equation:\n",
      "\n",
      "Cu²⁺ + EDTA⁴⁻ ⇌ CuEDTA²⁻\n",
      "\n",
      "The equilibrium constant for this reaction is given by:\n",
      "\n",
      "Kf = [CuEDTA²⁻] / ([Cu²⁺][EDTA⁴⁻])\n",
      "\n",
      "Now, let's consider the effect of decreasing the pH on this equilibrium. When the pH is decreased, the concentration of H⁺ ions in the solution increases. EDTA is a polyprotic ligand, meaning it can lose multiple protons. The fully deprotonated form of EDTA is EDTA⁴⁻, which is the form that binds to the Cu²⁺ ions. However, when the pH is decreased, some of the EDTA⁴⁻ will pick up protons and form its protonated species (HEDTA³⁻, H₂EDTA²⁻, H₃EDTA⁻, and H₄EDTA). The equilibria for these protonation reactions can be represented as:\n",
      "\n",
      "1) H⁺ + EDTA⁴⁻ ⇌ HEDTA³⁻\n",
      "2) H⁺ + HEDTA³⁻ ⇌ H₂EDTA²⁻\n",
      "3) H⁺ + H₂EDTA²⁻ ⇌ H₃EDTA⁻\n",
      "4) H⁺ + H₃EDTA⁻ ⇌ H₄EDTA\n",
      "\n",
      "As the pH decreases, the equilibria for these reactions will shift to the right, resulting in a decrease in the concentration of the fully deprotonated EDTA⁴⁻ species. This will affect the formation of the copper(II) EDTA complex as follows:\n",
      "\n",
      "Since the concentration of EDTA⁴⁻ decreases, according to the equilibrium constant expression for the formation of the CuEDTA²⁻ complex (Kf), the concentration of the CuEDTA²⁻ complex will also decrease. This is because the reaction will shift to the left to maintain the equilibrium constant value, resulting in less CuEDTA²⁻ complex being formed.\n",
      "\n",
      "In summary, decreasing the pH of a solution containing a fixed concentration of copper(II) ion and excess EDTA will result in a decrease in the formation of the copper(II) EDTA complex. This is due to the increased protonation of the EDTA⁴⁻ species, which reduces its concentration and thus shifts the equilibrium of the complex formation reaction to the left.\n"
     ]
    }
   ],
   "source": [
    "print(sample['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"dataset\": \"/workspace/data/llama_large_mix_dataset_v0_1536\",\n",
    "        \"seed\":42, \n",
    "        \"batch_size\":8, \n",
    "        \"gradient_accumulation_steps\":1, \n",
    "        \"context_length\":2048}\n",
    "sft_ds = InstructionDataset(ds, tokenizer_8b, style='local')\n",
    "\n",
    "tokenizer_8b.pad_token_id = tokenizer_8b.eos_token_id\n",
    "dl = get_dataloader(tokenizer_8b, args, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([len(sft_ds[i]['input_ids']) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_8b.decode(sft_ds[0]['labels'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb in dl: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many multiples of X are between 9 and 101?\n",
      "If we know the answer to the above question is 10, what is the value of unknown variable X?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "We want to find the value of $X$ in the given situation.\n",
      "To find the number of multiples of $X$ between 9 and 101, we can subtract the largest multiple of $X$ that is less than or equal to 101 from the smallest multiple of $X$ that is greater than or equal to 9, and then divide the result by $X$.\n",
      "Let's call the smallest multiple of $X$ that is greater than or equal to 9 as $a$, and the largest multiple of $X$ that is less than or equal to 101 as $b$.\n",
      "So, we have:\n",
      "$a = X$\n",
      "$b = X * n$\n",
      "where $n$ is the number of multiples between 9 and 101.\n",
      "To find the value of $X$, we need to determine the value of $n$.\n",
      "Given that the number of multiples is 10, we can write:\n",
      "$n = 10$\n",
      "Substituting the value of $n$ into the equation for $b$, we have:\n",
      "$b = X * 10$\n",
      "We are also given that $b$ is the largest multiple of $X$ that is less than or equal to 101, so we can write:\n",
      "$b \\leq 101$\n",
      "$X * 10 \\leq 101$\n",
      "Dividing both sides of the equation by 10 to solve for $X$, we get:\n",
      "$X \\leq 10.1$\n",
      "Since $X$ is a positive integer, the largest possible value for $X$ is 10.\n",
      "Therefore, the value of $X$ is 10.\n",
      "The answer is: 10<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_8b.decode(xb['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to find the value of $X$ in the given situation.\n",
      "To find the number of multiples of $X$ between 9 and 101, we can subtract the largest multiple of $X$ that is less than or equal to 101 from the smallest multiple of $X$ that is greater than or equal to 9, and then divide the result by $X$.\n",
      "Let's call the smallest multiple of $X$ that is greater than or equal to 9 as $a$, and the largest multiple of $X$ that is less than or equal to 101 as $b$.\n",
      "So, we have:\n",
      "$a = X$\n",
      "$b = X * n$\n",
      "where $n$ is the number of multiples between 9 and 101.\n",
      "To find the value of $X$, we need to determine the value of $n$.\n",
      "Given that the number of multiples is 10, we can write:\n",
      "$n = 10$\n",
      "Substituting the value of $n$ into the equation for $b$, we have:\n",
      "$b = X * 10$\n",
      "We are also given that $b$ is the largest multiple of $X$ that is less than or equal to 101, so we can write:\n",
      "$b \\leq 101$\n",
      "$X * 10 \\leq 101$\n",
      "Dividing both sides of the equation by 10 to solve for $X$, we get:\n",
      "$X \\leq 10.1$\n",
      "Since $X$ is a positive integer, the largest possible value for $X$ is 10.\n",
      "Therefore, the value of $X$ is 10.\n",
      "The answer is: 10<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_8b.decode(xb['labels'][0][xb['labels'][0] != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90417, dict_keys(['input_ids', 'labels', 'attention_mask']))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sft_ds), sft_ds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Provide the best response to a given instruction. Follow the following steps to craft your response:\n",
      "1. reason about the given instruction\n",
      "2. provide a high-quality solution\n",
      "3. offer a concise explanation\n",
      "4. write tests to verify the correctness your solution\n",
      "\n",
      "## Example 1\n",
      "### Instruction\n",
      "Here are two special formulas:\n",
      "\n",
      "$$\n",
      "f_1(a, b) = (a + 1) \\cdot (b + 1) - 1\n",
      "$$\n",
      "\n",
      "$$\n",
      "f_2(k) = \\begin{cases}\n",
      "    \\frac{(k + 1)^2}{2} + k + 1 & \\text{if } k \\text{ is odd} \\\\\n",
      "    \\frac{k^2}{2} + 2k + 1 & \\text{if } k \\text{ is even}\n",
      "\\end{cases}\n",
      "$$\n",
      "\n",
      "Write a Python function to return $f_2(f_1(a, b))$ for given `a` and `b`.\n",
      "\n",
      "### Response\n",
      "[Reasoning]\n",
      "Based on the formulas you provided, we can define two Python functions, `f1(a, b)` and `f2(k)`, respectively, and then combine them to calculate $f2(f1(a, b))$ for given `a` and `b`.\n",
      "\n",
      "[Implementation]\n",
      "Here is the implementation:\n",
      "\n",
      "```python\n",
      "def f1(a, b):\n",
      "    return (a + 1) * (b + 1) - 1\n",
      "\n",
      "def f2(k):\n",
      "    if k % 2 == 0:  # Check if k is even\n",
      "        return (k**2) / 2 + 2 * k + 1\n",
      "    else:  # k is odd\n",
      "        return ((k + 1)**2) / 2 + k + 1\n",
      "\n",
      "def f1_f2_combined(a, b):\n",
      "    return f2(f1(a, b))\n",
      "```\n",
      "\n",
      "[Explanation]\n",
      "In this implementation:\n",
      "- `f1(a, b)` calculates the first formula you've given.\n",
      "- `f2(k)` calculates the second formula, with a conditional branch for whether `k` is even or odd.\n",
      "- `f1_f2_combined(a, b)` uses the output of `f1(a, b)` as the input for `f2(k)` and returns the result.\n",
      "\n",
      "[Tests]\n",
      "To test this combined function, you can simply compare it with the direct application of `f2` to the result of `f1` for different values of `a` and `b`:\n",
      "\n",
      "```python\n",
      "assert f1_f2_combined(2, 3) == f2(f1(2, 3))\n",
      "assert f1_f2_combined(4, 5) == f2(f1(4, 5))\n",
      "assert f1_f2_combined(6, 7) == f2(f1(6, 7))\n",
      "```\n",
      "\n",
      "## Example 2\n",
      "### Instruction\n",
      "Create a Python function to match a filename in a given input string. The function should take a string and return the filename if it matches the filename pattern, or an empty string otherwise. The filename pattern is defined by the regular expression pattern, `r'(\\w|.|-)+\\.(\\w+)'`, which means any combination of alphanumeric characters, dots, and hyphens followed by a dot and any combination of alphanumeric characters. For example, `my_file.txt` and `1.2-alpha.py` match the pattern, whereas `test.py.png` and `my$file.txt` do not. The function should use `re.match` to perform pattern matching.\n",
      "\n",
      "### Response<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "We can use the `re.match` function to check if the input string matches the given filename pattern, which is defined by the regular expression pattern, `r\"(\\w|.|-)+\\.(\\w+)\"`. This regular expression pattern matches any combination of alphanumeric characters, dots, and hyphens followed by a dot and any combination of alphanumeric characters.\n",
      "\n",
      "Here is the implementation of the Python function, `filename_matcher`, using `re.match`:\n",
      "\n",
      "```python\n",
      "import re\n",
      "\n",
      "def filename_matcher(input_string):\n",
      "    filename_pattern = r\"(\\w|.|-)+\\.(\\w+)\"\n",
      "    match = re.match(filename_pattern, input_string)\n",
      "    if match:\n",
      "        return match.group(0)  # Return the matched filename\n",
      "    else:\n",
      "        return \"\"  # Return an empty string if no match\n",
      "```\n",
      "\n",
      "To test this function, we can define a sample input string and then check whether the function returns the expected output:\n",
      "\n",
      "```python\n",
      "input_string = \"my_file.txt\"\n",
      "expected_output = \"my_file.txt\"\n",
      "\n",
      "actual_output = filename_matcher(input_string)\n",
      "assert actual_output == expected_output\n",
      "```<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_8b.decode(sft_ds[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreeze Layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "with init_empty_weights():\n",
    "\tmodel = AutoModelForCausalLM.from_config(cfg)\n",
    "\tmodel_type = cfg.model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(cfg.num_hidden_layers * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.input_layernorm.weight 4096\n",
      "model.layers.0.post_attention_layernorm.weight 4096\n",
      "model.layers.1.input_layernorm.weight 4096\n",
      "model.layers.1.post_attention_layernorm.weight 4096\n",
      "model.layers.2.input_layernorm.weight 4096\n",
      "model.layers.2.post_attention_layernorm.weight 4096\n",
      "model.layers.3.input_layernorm.weight 4096\n",
      "model.layers.3.post_attention_layernorm.weight 4096\n",
      "model.layers.4.input_layernorm.weight 4096\n",
      "model.layers.4.post_attention_layernorm.weight 4096\n",
      "model.layers.5.input_layernorm.weight 4096\n",
      "model.layers.5.post_attention_layernorm.weight 4096\n",
      "model.layers.6.input_layernorm.weight 4096\n",
      "model.layers.6.post_attention_layernorm.weight 4096\n",
      "model.layers.7.input_layernorm.weight 4096\n",
      "model.layers.7.post_attention_layernorm.weight 4096\n",
      "model.layers.8.input_layernorm.weight 4096\n",
      "model.layers.8.post_attention_layernorm.weight 4096\n",
      "model.layers.9.input_layernorm.weight 4096\n",
      "model.layers.9.post_attention_layernorm.weight 4096\n",
      "model.layers.10.input_layernorm.weight 4096\n",
      "model.layers.10.post_attention_layernorm.weight 4096\n",
      "model.layers.11.input_layernorm.weight 4096\n",
      "model.layers.11.post_attention_layernorm.weight 4096\n",
      "model.layers.12.input_layernorm.weight 4096\n",
      "model.layers.12.post_attention_layernorm.weight 4096\n",
      "model.layers.13.input_layernorm.weight 4096\n",
      "model.layers.13.post_attention_layernorm.weight 4096\n",
      "model.layers.14.input_layernorm.weight 4096\n",
      "model.layers.14.post_attention_layernorm.weight 4096\n",
      "model.layers.15.input_layernorm.weight 4096\n",
      "model.layers.15.post_attention_layernorm.weight 4096\n",
      "model.layers.16.input_layernorm.weight 4096\n",
      "model.layers.16.post_attention_layernorm.weight 4096\n",
      "model.layers.17.input_layernorm.weight 4096\n",
      "model.layers.17.post_attention_layernorm.weight 4096\n",
      "model.layers.18.input_layernorm.weight 4096\n",
      "model.layers.18.post_attention_layernorm.weight 4096\n",
      "model.layers.19.input_layernorm.weight 4096\n",
      "model.layers.19.post_attention_layernorm.weight 4096\n",
      "model.layers.20.input_layernorm.weight 4096\n",
      "model.layers.20.post_attention_layernorm.weight 4096\n",
      "model.layers.21.input_layernorm.weight 4096\n",
      "model.layers.21.post_attention_layernorm.weight 4096\n",
      "model.layers.22.input_layernorm.weight 4096\n",
      "model.layers.22.post_attention_layernorm.weight 4096\n",
      "model.layers.23.input_layernorm.weight 4096\n",
      "model.layers.23.post_attention_layernorm.weight 4096\n",
      "model.layers.24.input_layernorm.weight 4096\n",
      "model.layers.24.post_attention_layernorm.weight 4096\n",
      "model.layers.25.input_layernorm.weight 4096\n",
      "model.layers.25.post_attention_layernorm.weight 4096\n",
      "model.layers.26.input_layernorm.weight 4096\n",
      "model.layers.26.post_attention_layernorm.weight 4096\n",
      "model.layers.27.input_layernorm.weight 4096\n",
      "model.layers.27.post_attention_layernorm.weight 4096\n",
      "model.layers.28.input_layernorm.weight 4096\n",
      "model.layers.28.post_attention_layernorm.weight 4096\n",
      "model.layers.29.input_layernorm.weight 4096\n",
      "model.layers.29.post_attention_layernorm.weight 4096\n",
      "model.layers.30.input_layernorm.weight 4096\n",
      "model.layers.30.post_attention_layernorm.weight 4096\n",
      "model.layers.31.input_layernorm.weight 4096\n",
      "model.layers.31.post_attention_layernorm.weight 4096\n",
      "model.norm.weight 4096\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    if \"norm\" in n:\n",
    "        print(n, p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import safetensors\n",
    "import safetensors.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(\"/workspace/models/llama-3-1-8b-instruct-hqq-4bit/model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.input_layernorm.weight 9.0\n",
      "model.layers.0.post_attention_layernorm.weight 8.625\n"
     ]
    }
   ],
   "source": [
    "for n,p in weights.items(): print(n,p.norm().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"/workspace/models/llama-3-1-8b-instruct-hqq-4bit/config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lora_target_modules': ['q_proj',\n",
       "  'k_proj',\n",
       "  'v_proj',\n",
       "  'o_proj',\n",
       "  'gate_proj',\n",
       "  'up_proj',\n",
       "  'down_proj'],\n",
       " 'compute_dtype': 'bfloat16',\n",
       " 'lora_rank': 64,\n",
       " 'layer_nbits': {'q_proj': 4,\n",
       "  'k_proj': 4,\n",
       "  'v_proj': 4,\n",
       "  'o_proj': 4,\n",
       "  'gate_proj': 2,\n",
       "  'up_proj': 2,\n",
       "  'down_proj': 2},\n",
       " 'layer_groupsizes': {'q_proj': 128,\n",
       "  'k_proj': 128,\n",
       "  'v_proj': 128,\n",
       "  'o_proj': 128,\n",
       "  'gate_proj': 64,\n",
       "  'up_proj': 64,\n",
       "  'down_proj': 64},\n",
       " 'skip_dora_all': 1,\n",
       " 'skip_dora_4bit': False,\n",
       " 'train_layernorms': 1}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['qlora_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
