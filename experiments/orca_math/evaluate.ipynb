{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3299605-c781-4901-bf93-eedaa75e2634",
   "metadata": {},
   "source": [
    "Evaluations with zero-shot, 5-shot, full fine-tune, merged LoRA, merged DoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79535af-bfff-48d6-a79d-3ae911876451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json\n",
    "from safetensors.torch import save_file\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import safetensors\n",
    "import safetensors.torch\n",
    "from glob import glob\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME\n",
    "from hqq.core.quantize import HQQLinear, HQQBackend, BaseQuantizeConfig, Quantizer\n",
    "from fastcore.script import *\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.nn.modules import Params4bit\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ee869-34b9-4240-866f-fccdc15e1bb5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef736afa-7e62-46ac-8e92-3e854b2810fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "from fastcore.parallel import parallel\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36eecac2-2842-4b94-b40a-622ca9c1b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_number_or_ratio(s):\n",
    "    # Find all sequences of digits, possibly with leading currency symbols, decimal points, and ratios\n",
    "    patterns = re.findall(r'[\\$€£]?\\d+(?:\\.\\d+)?(?:\\:\\d+(?:\\.\\d+)?)?', s)\n",
    "    \n",
    "    # Return the last pattern found, or None if there are no matches\n",
    "    if patterns:\n",
    "        return patterns[-1]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bb6844-0eb0-46e5-979a-098a8622c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_score(preds, labels):\n",
    "    return sum(p==g for p,g in zip(preds, labels))/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437681e0-1da3-427a-998c-a204728b6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"microsoft/orca-math-word-problems-200k\")['train'].shuffle(seed=42)\n",
    "dataset = dataset.select(range(len(dataset)-5000,len(dataset)))\n",
    "short_answers_gt = parallel(extract_last_number_or_ratio, dataset['answer'], progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921271da-4b66-429c-9e7a-5616a791aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = dataset.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eeb9fc8-715b-4929-bb59-7c1d21a25476",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in valid_dataset['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df75559-19fb-4255-af25-27720773d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = short_answers_gt[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccd93133-3d75-47de-bbf5-4e15c6eb604a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ce90fce-28d4-4037-967e-539c5db97ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUS = torch.cuda.device_count(); NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7f5540f-731f-4b3c-817a-8c49d36f67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f3a0eaf-684c-4732-8884-466d5ba9422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db22768-f3a4-41c3-9db4-efa7bf4a5045",
   "metadata": {},
   "source": [
    "#### FINETUNED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30396b6b-c97b-41ca-98b8-4a8404fabdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/workspace/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52df5f9-1026-4591-a935-c5ec502e551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"llama-3-8b-orca-math-10k-full\" # 0.4\n",
    "# MODEL_NAME = \"llama-3-8b-orca-math-10k-bnb-qlora-merged\" # 0.276\n",
    "# MODEL_NAME = \"llama-3-8b-orca-math-10k-bnb-qdora-merged\" # 0.458\n",
    "\n",
    "MODEL_NAME = \"llama-3-8b-orca-math-100k-bnb-qlora-merged\"\n",
    "# MODEL_NAME = \"llama-3-8b-orca-math-100k-bnb-qdora-merged\" # 0.558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b5708-0704-4b2a-b81b-725abcc7df1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = LLM(model=os.path.join(model_dir,MODEL_NAME), tokenizer=TOKENIZER, \n",
    "          tensor_parallel_size=NUM_GPUS, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed93c58d-a5e5-4b4b-9f29-8c50276bdfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [01:19<00:00,  6.33it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs, SamplingParams(temperature=0.0, max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "704e3218-7a1f-449c-a5f0-cc235c1543bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers_pred = [extract_last_number_or_ratio(o.outputs[0].text) for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d14af3-9dce-4f85-b49d-862f60ea860b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.322"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match_score(short_answers_pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c113080-4f09-49ca-b787-2c592cf4da1c",
   "metadata": {},
   "source": [
    "#### N-SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a43c053-4c1b-42bb-bcf8-e6c7eb1a2ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 12:10:54,776\tWARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2024-05-02 12:10:54,780\tWARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 32.3 to 32.\n",
      "2024-05-02 12:10:55,912\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 12:10:57 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='meta-llama/Meta-Llama-3-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 12:11:22 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=208091)\u001b[0m INFO 05-02 12:11:22 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-02 12:11:23 selector.py:28] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=208310)\u001b[0m INFO 05-02 12:11:27 selector.py:28] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=208310)\u001b[0m INFO 05-02 12:11:22 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "INFO 05-02 12:11:28 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=208091)\u001b[0m INFO 05-02 12:11:28 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "WARNING 05-02 12:11:30 custom_all_reduce.py:65] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=208091)\u001b[0m WARNING 05-02 12:11:30 custom_all_reduce.py:65] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 05-02 12:11:31 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=208196)\u001b[0m INFO 05-02 12:11:31 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 05-02 12:11:32 model_runner.py:173] Loading model weights took 3.7417 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=208196)\u001b[0m INFO 05-02 12:11:32 model_runner.py:173] Loading model weights took 3.7417 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=208196)\u001b[0m INFO 05-02 12:11:27 selector.py:28] Using FlashAttention backend.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 05-02 12:11:35 ray_gpu_executor.py:217] # GPU blocks: 70645, # CPU blocks: 8192\n",
      "INFO 05-02 12:11:37 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-02 12:11:37 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=208091)\u001b[0m INFO 05-02 12:11:37 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=208091)\u001b[0m INFO 05-02 12:11:37 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=208310)\u001b[0m INFO 05-02 12:11:28 pynccl_utils.py:43] vLLM is using nccl==2.18.1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=208310)\u001b[0m WARNING 05-02 12:11:30 custom_all_reduce.py:65] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=208310)\u001b[0m INFO 05-02 12:11:31 weight_utils.py:193] Using model weights format ['*.safetensors']\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 05-02 12:11:47 model_runner.py:1057] Graph capturing finished in 10 secs.\n",
      "\u001b[36m(RayWorkerWrapper pid=208091)\u001b[0m INFO 05-02 12:11:47 model_runner.py:1057] Graph capturing finished in 10 secs.\n",
      "\u001b[36m(RayWorkerWrapper pid=208310)\u001b[0m INFO 05-02 12:11:33 model_runner.py:173] Loading model weights took 3.7417 GB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=208310)\u001b[0m INFO 05-02 12:11:37 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=208310)\u001b[0m INFO 05-02 12:11:37 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "llm = LLM(model=MODEL_NAME, tensor_parallel_size=NUM_GPUS, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe71037b-5068-4437-af5d-65be722780c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [01:20<00:00,  6.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.228"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero-shot\n",
    "inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in valid_dataset['question']]\n",
    "outputs = llm.generate(inputs, SamplingParams(temperature=0.0, max_tokens=1024))\n",
    "short_answers_pred = [extract_last_number_or_ratio(o.outputs[0].text) for o in outputs]\n",
    "exact_match_score(short_answers_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac82c9c-7663-4e22-b483-6b75964b18a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   9%|▉         | 47/500 [01:06<02:45,  2.73it/s] "
     ]
    }
   ],
   "source": [
    "# 5-shot\n",
    "few_shot_examples = [f\"###Question:\\n{ex['question']}\\n###Answer:\\n{ex['answer']}<stop>\" for ex in \n",
    "                     dataset.select(range(len(dataset)-5,len(dataset)))]\n",
    "few_shot_prompt = \"\\n\\n\".join(few_shot_examples)\n",
    "inputs = [few_shot_prompt + \"\\n\\n\" + f\"###Question:\\n{question}\\n###Answer:\\n\" for question in valid_dataset['question']]\n",
    "outputs = llm.generate(inputs, SamplingParams(temperature=0.0, \n",
    "                                              stop_token_ids=[tokenizer.eos_token_id], \n",
    "                                              stop=[\"<stop>\"], \n",
    "                                              max_tokens=1024))\n",
    "short_answers_pred = [extract_last_number_or_ratio(o.outputs[0].text) for o in outputs]\n",
    "exact_match_score(short_answers_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f57468-06c1-4866-9f38-907e2cc5c537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15177345-bca7-49ad-af35-068538787c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b936f5-8a6e-4ebc-ac1a-989ff41f128e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1979a2-ad54-4f46-9247-28a9d0de18bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b467a1-9f3a-4d09-b0f9-2de46c979762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b02810-3d5d-4d8a-8ca7-e43d59722311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
