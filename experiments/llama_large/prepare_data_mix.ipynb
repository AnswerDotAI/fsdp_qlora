{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88954bae-0756-4fc5-9b77-a9e6e2ce9fb3",
   "metadata": {},
   "source": [
    "Prepare IFT datasets with `{\"input_text\":, ... \"output_text\":...}` format. Prepare in chat format to finetune chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0d988a-baed-44c9-a83b-e4b4858d87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import concatenate_datasets, load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb21107-ce34-463b-9755-bdcaf101dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_text(tokenizer, user_prompt, system=\"You are a helpful AI assistant.\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return messages, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc853a14-fa20-49de-b173-ef9ad30c0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c25ab-14a6-4e22-bc64-a7c352972da3",
   "metadata": {},
   "source": [
    "### oasst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0426a8db-f0aa-484c-bd84-1994f9666d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    t = example[\"text\"]\n",
    "    \n",
    "    human_prefix, asst_prefix = \"### Human: \", \"### Assistant: \"\n",
    "    human_matches = list(re.finditer(human_prefix, t))\n",
    "    asst_matches = list(re.finditer(asst_prefix, t))\n",
    "    human_start_idx, _ = human_matches[0].span(0)\n",
    "    asst_start_idx, _ = asst_matches[0].span(0)\n",
    "    if len(human_matches) > 1:\n",
    "        asst_end_idx = human_matches[1].span(0)[0]\n",
    "    else:\n",
    "        asst_end_idx = len(t)\n",
    "    \n",
    "    human_prompt = t[human_start_idx+len(human_prefix):asst_start_idx]\n",
    "    asst_prompt = t[asst_start_idx+len(asst_prefix):asst_end_idx]\n",
    "    \n",
    "    _,input_text = create_chat_text(tokenizer, human_prompt)\n",
    "    output_text = asst_prompt\n",
    "    return {\"input_text\":input_text, \"output_text\":output_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21607f69-ac2c-48be-ac7c-25aef1f1d4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 395/395 [00:00<00:00, 1.53MB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 20.9M/20.9M [00:00<00:00, 54.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.11M/1.11M [00:00<00:00, 8.81MB/s]\n",
      "Generating train split: 100%|██████████| 9846/9846 [00:00<00:00, 142261.10 examples/s]\n",
      "Generating test split: 100%|██████████| 518/518 [00:00<00:00, 89560.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds1 = load_dataset(\"timdettmers/openassistant-guanaco\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c10d8fb-65c8-4f70-ad0c-77c06d8cce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9846/9846 [00:01<00:00, 7352.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds1_processed = train_ds1.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910cfceb-a66c-4efe-93b7-7585ceb88690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9846"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds1_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "622a13ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds1_processed[0]['input_text'])\n",
    "print(train_ds1_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befe39e-e090-4b6b-b980-fe1566dbdbc1",
   "metadata": {},
   "source": [
    "### orca-math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c2520a-e078-497c-b9db-a82047325fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    _,input_text = create_chat_text(tokenizer, example['question'])\n",
    "    return {\"input_text\":input_text, \"output_text\":example['answer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11937249-cc2a-4e04-a33f-534629317ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 6.91k/6.91k [00:00<00:00, 14.5MB/s]\n",
      "Downloading data: 100%|██████████| 84.2M/84.2M [00:01<00:00, 69.3MB/s]\n",
      "Generating train split: 100%|██████████| 200035/200035 [00:01<00:00, 193137.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds2 = load_dataset(\"microsoft/orca-math-word-problems-200k\")\n",
    "train_ds2 = train_ds2['train'].shuffle(42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccbdd733-d301-4cfe-8d8e-0ce7ee2f6e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 5633.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds2_processed = train_ds2.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54a5325-80c2-4884-8b4b-b4533c3baaf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Quentin, Skylar, and Colten have a total of 383 chickens. Quentin has 25 more than double the chickens that Skylar has. Skylar has 4 less than a certain multiple of the number of chickens that Colten has. Colten has 37 chickens. What is the multiple of the number of chickens Colten has that Skylar has 4 less than?',\n",
       " 'answer': \"Let's denote the number of chickens that Skylar has as S and the number of chickens that Quentin has as Q. We know that Colten has 37 chickens.\\n\\nAccording to the information given:\\n\\n1) Quentin has 25 more than double the chickens that Skylar has:\\nQ = 2S + 25\\n\\n2) The total number of chickens is 383:\\nQ + S + 37 = 383\\n\\nNow, let's substitute the expression for Q from the first equation into the second equation:\\n\\n(2S + 25) + S + 37 = 383\\n3S + 62 = 383\\n3S = 383 - 62\\n3S = 321\\nS = 321 / 3\\nS = 107\\n\\nNow we know Skylar has 107 chickens.\\n\\nThe problem states that Skylar has 4 less than a certain multiple of the number of chickens that Colten has. Let's denote the multiple as M. So:\\n\\nS = M * 37 - 4\\n\\nWe already know S is 107, so:\\n\\n107 = M * 37 - 4\\n111 = M * 37\\nM = 111 / 37\\nM = 3\\n\\nTherefore, the multiple of the number of chickens Colten has that Skylar has 4 less than is 3.\",\n",
       " 'input_text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuentin, Skylar, and Colten have a total of 383 chickens. Quentin has 25 more than double the chickens that Skylar has. Skylar has 4 less than a certain multiple of the number of chickens that Colten has. Colten has 37 chickens. What is the multiple of the number of chickens Colten has that Skylar has 4 less than?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       " 'output_text': \"Let's denote the number of chickens that Skylar has as S and the number of chickens that Quentin has as Q. We know that Colten has 37 chickens.\\n\\nAccording to the information given:\\n\\n1) Quentin has 25 more than double the chickens that Skylar has:\\nQ = 2S + 25\\n\\n2) The total number of chickens is 383:\\nQ + S + 37 = 383\\n\\nNow, let's substitute the expression for Q from the first equation into the second equation:\\n\\n(2S + 25) + S + 37 = 383\\n3S + 62 = 383\\n3S = 383 - 62\\n3S = 321\\nS = 321 / 3\\nS = 107\\n\\nNow we know Skylar has 107 chickens.\\n\\nThe problem states that Skylar has 4 less than a certain multiple of the number of chickens that Colten has. Let's denote the multiple as M. So:\\n\\nS = M * 37 - 4\\n\\nWe already know S is 107, so:\\n\\n107 = M * 37 - 4\\n111 = M * 37\\nM = 111 / 37\\nM = 3\\n\\nTherefore, the multiple of the number of chickens Colten has that Skylar has 4 less than is 3.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds2_processed[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4be20282-b79c-42bd-b152-d9614954a911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds2_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1741d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Sally had 13 peaches at her roadside fruit dish.  She went to the orchard and picked peaches to stock up. She picked 55 peaches. There are _____ peaches now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Sally originally had 13 peaches. She picked 55 more peaches. To find out the total number of peaches she has now, we add the two amounts together:\n",
      "\n",
      "13 (original peaches) + 55 (picked peaches) = 68 peaches\n",
      "\n",
      "So, there are 68 peaches now.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds2_processed[0]['input_text'])\n",
    "print(train_ds2_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc438a-d654-48de-ab93-ab4e22b4647c",
   "metadata": {},
   "source": [
    "### meta-math-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50d080e0-ce10-494d-aed7-790067a854c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    _,input_text = create_chat_text(tokenizer, example['query'])\n",
    "    return {\"input_text\":input_text, \"output_text\":example['response']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08a3022d-46ca-4a8f-b33b-45a52ba109e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.45k/4.45k [00:00<00:00, 11.9MB/s]\n",
      "Downloading data: 100%|██████████| 396M/396M [00:10<00:00, 36.2MB/s] \n",
      "Generating train split: 100%|██████████| 395000/395000 [00:16<00:00, 24205.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds3 = load_dataset(\"meta-math/MetaMathQA\")\n",
    "train_ds3 = train_ds3['train'].shuffle(42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5c70ea5-5b0b-471b-b558-2604f1448514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 5123.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds3_processed = train_ds3.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ab6be5-8645-456d-b71e-93b9f60b7c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'MATH_SV',\n",
       " 'query': 'Compute the sum of the squares of the roots of the equation \\\\[x^{2018} +44x^{2015} + 3x^3 + 404 = X The answer is 0. What is the value of unknown variable X?',\n",
       " 'original_question': 'Compute the sum of the squares of the roots of the equation \\\\[x^{2018} +44x^{2015} + 3x^3 + 404 = 0.\\\\]',\n",
       " 'response': 'To solve this problem, we need to determine the value of x that will make the sum of the squares of the roots of the equation equal to 0.\\nThe equation is given as:\\n\\\\[x^{2018} + 44x^{2015} + 3x^3 + 404 = X\\\\]\\nTo find the sum of the squares of the roots, we need to find the roots of the equation and then square them.\\nSince the answer is 0, it means that the sum of the squares of the roots must be equal to 0.\\nTo make the sum of squares 0, we need to find the values of x that will make each square equal to 0.\\nIf a square is equal to 0, it means that the value inside the square must be 0.\\nIn this case, we need to set each term inside the square to 0 and solve for x.\\nSetting each term to 0, we have:\\n\\\\[x^{2018} = 0\\\\]\\n\\\\[44x^{2015} = 0\\\\]\\n\\\\[3x^3 = 0\\\\]\\n\\\\[404 = 0\\\\]\\nThe last equation, 404 = 0, does not have a solution because it is a contradiction.\\nThe third equation, 3x^3 = 0, has a solution of x = 0.\\nThe second equation, 44x^{2015} = 0, also has a solution of x = 0.\\nThe first equation, x^{2018} = 0, has a solution of x = 0.\\nTherefore, the value of unknown variable x is 0.\\nThe answer is: 0',\n",
       " 'input_text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nCompute the sum of the squares of the roots of the equation \\\\[x^{2018} +44x^{2015} + 3x^3 + 404 = X The answer is 0. What is the value of unknown variable X?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       " 'output_text': 'To solve this problem, we need to determine the value of x that will make the sum of the squares of the roots of the equation equal to 0.\\nThe equation is given as:\\n\\\\[x^{2018} + 44x^{2015} + 3x^3 + 404 = X\\\\]\\nTo find the sum of the squares of the roots, we need to find the roots of the equation and then square them.\\nSince the answer is 0, it means that the sum of the squares of the roots must be equal to 0.\\nTo make the sum of squares 0, we need to find the values of x that will make each square equal to 0.\\nIf a square is equal to 0, it means that the value inside the square must be 0.\\nIn this case, we need to set each term inside the square to 0 and solve for x.\\nSetting each term to 0, we have:\\n\\\\[x^{2018} = 0\\\\]\\n\\\\[44x^{2015} = 0\\\\]\\n\\\\[3x^3 = 0\\\\]\\n\\\\[404 = 0\\\\]\\nThe last equation, 404 = 0, does not have a solution because it is a contradiction.\\nThe third equation, 3x^3 = 0, has a solution of x = 0.\\nThe second equation, 44x^{2015} = 0, also has a solution of x = 0.\\nThe first equation, x^{2018} = 0, has a solution of x = 0.\\nTherefore, the value of unknown variable x is 0.\\nThe answer is: 0'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds3_processed[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eefadd93-8ca7-41d6-a4f9-4d3f237a6deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds3_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7ce57b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "If Anna wants to create a smiley face shape using red and yellow tulips, she requires 8 red tulips for each eye and 18 red tulips for the smile. Additionally, she needs 9 times the number of tulips in the smile to create the yellow background of the face. What is the total number of tulips that Anna needs?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Anna needs 8 red tulips for each eye, so for both eyes she needs 8 * 2 = 16 red tulips.\n",
      "She also needs 18 red tulips for the smile.\n",
      "The total number of red tulips she needs is 16 + 18 = 34 red tulips.\n",
      "For the yellow background, she needs 9 times the number of tulips in the smile, so she needs 9 * 18 = 162 yellow tulips.\n",
      "The total number of tulips she needs is 34 + 162 = 196 tulips.\n",
      "#### 196\n",
      "The answer is: 196\n"
     ]
    }
   ],
   "source": [
    "print(train_ds3_processed[0]['input_text'])\n",
    "print(train_ds3_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1706e-ed2b-4ba9-9a4c-92f3268f9a47",
   "metadata": {},
   "source": [
    "### ultrafeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "349d5080-bbac-4906-ab7d-484bcb7bdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    chosen = example['chosen']\n",
    "    _,input_text = create_chat_text(tokenizer, chosen[0]['content'])\n",
    "    return {\"input_text\":input_text, \"output_text\":chosen[1]['content']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54487bae-7126-44c7-b9ed-eb7c40191adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 6.77k/6.77k [00:00<00:00, 18.5MB/s]\n",
      "Downloading data: 100%|██████████| 226M/226M [00:02<00:00, 98.6MB/s] \n",
      "Downloading data: 100%|██████████| 226M/226M [00:02<00:00, 109MB/s]  \n",
      "Downloading data: 100%|██████████| 7.29M/7.29M [00:00<00:00, 15.4MB/s]\n",
      "Downloading data: 100%|██████████| 3.72M/3.72M [00:00<00:00, 13.2MB/s]\n",
      "Downloading data: 100%|██████████| 184M/184M [00:01<00:00, 97.1MB/s] \n",
      "Downloading data: 100%|██████████| 3.02M/3.02M [00:00<00:00, 12.9MB/s]\n",
      "Generating train_prefs split: 100%|██████████| 61135/61135 [00:01<00:00, 52191.18 examples/s]\n",
      "Generating train_sft split: 100%|██████████| 61135/61135 [00:01<00:00, 52801.08 examples/s]\n",
      "Generating test_prefs split: 100%|██████████| 2000/2000 [00:00<00:00, 52181.27 examples/s]\n",
      "Generating test_sft split: 100%|██████████| 1000/1000 [00:00<00:00, 51043.60 examples/s]\n",
      "Generating train_gen split: 100%|██████████| 61135/61135 [00:01<00:00, 58880.05 examples/s]\n",
      "Generating test_gen split: 100%|██████████| 1000/1000 [00:00<00:00, 46869.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds4 = load_dataset('HuggingFaceH4/ultrafeedback_binarized')\n",
    "train_ds4 = train_ds4['train_sft'].shuffle(42).select(range(10000))\n",
    "train_ds4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eb22bbc-2513-4ad6-9476-2eb69618eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:03<00:00, 3315.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds4_processed = train_ds4.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea200f8f-a07b-4a79-8047-b17b006ea674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds4_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a793ebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Do you know something about crystallography and structure factor?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Crystallography is the science of the arrangement of atoms in solids. It is a vast and interdisciplinary field that has applications in physics, chemistry, materials science, biology, and engineering.\n",
      "\n",
      "The structure factor is a mathematical function that is used to describe the diffraction of waves by a crystal. It is a complex number that is related to the atomic positions in the crystal.\n",
      "\n",
      "The structure factor can be used to calculate the intensity of the diffracted waves. This information can be used to determine the atomic positions in the crystal and to study the structure of materials.\n",
      "\n",
      "Crystallography is a powerful tool for understanding the structure of materials. It has been used to determine the structures of many important materials, including metals, semiconductors, and pharmaceuticals. It is also used to study the structure of biological materials, such as proteins and DNA.\n",
      "\n",
      "The structure factor is a key concept in crystallography. It is used to describe the diffraction of waves by a crystal and to calculate the intensity of the diffracted waves. This information can be used to determine the atomic positions in the crystal and to study the structure of materials.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds4_processed[0]['input_text'])\n",
    "print(train_ds4_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684a568",
   "metadata": {},
   "source": [
    "### ultrachat-200k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3af94d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = example['messages']\n",
    "    _,input_text = create_chat_text(tokenizer, messages[0]['content'])\n",
    "    return {\"input_text\":input_text, \"output_text\":messages[1]['content']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30de8364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.44k/4.44k [00:00<00:00, 6.47MB/s]\n",
      "Downloading data: 100%|██████████| 244M/244M [00:02<00:00, 114MB/s]  \n",
      "Downloading data: 100%|██████████| 244M/244M [00:01<00:00, 127MB/s]  \n",
      "Downloading data: 100%|██████████| 244M/244M [00:02<00:00, 97.3MB/s] \n",
      "Downloading data: 100%|██████████| 81.2M/81.2M [00:01<00:00, 66.8MB/s]\n",
      "Downloading data: 100%|██████████| 244M/244M [00:02<00:00, 109MB/s]  \n",
      "Downloading data: 100%|██████████| 243M/243M [00:02<00:00, 105MB/s]  \n",
      "Downloading data: 100%|██████████| 243M/243M [00:02<00:00, 115MB/s]  \n",
      "Downloading data: 100%|██████████| 80.4M/80.4M [00:00<00:00, 83.1MB/s]\n",
      "Generating train_sft split: 100%|██████████| 207865/207865 [00:07<00:00, 28681.99 examples/s]\n",
      "Generating test_sft split: 100%|██████████| 23110/23110 [00:00<00:00, 28862.87 examples/s]\n",
      "Generating train_gen split: 100%|██████████| 256032/256032 [00:06<00:00, 38169.04 examples/s]\n",
      "Generating test_gen split: 100%|██████████| 28304/28304 [00:00<00:00, 37343.84 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds5 = load_dataset('HuggingFaceH4/ultrachat_200k', name='default')\n",
    "train_ds5 = train_ds5['train_sft'].shuffle(42).select(range(10000))\n",
    "train_ds5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e15c1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:03<00:00, 3304.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds5_processed = train_ds5.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bcb026b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds5_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42891290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does the location of the Sydney Conservatorium of Music impact the academic and professional opportunities available to music students, and how does the conservatorium support student engagement with the music industry in Australia?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "The location of the Sydney Conservatorium of Music, which is situated in the heart of Sydney's cultural precinct, impacts both the academic and professional opportunities available to music students. The conservatorium is located near several major performing arts venues and organizations, including the Sydney Opera House, the Australian Broadcasting Corporation, and the Sydney Symphony Orchestra, providing students with easy access to performances, rehearsals, and networking opportunities.\n",
      "\n",
      "One of the primary ways the conservatorium supports student engagement with the music industry in Australia is through its strong industry connections. The conservatorium has established partnerships with numerous professional music organizations, such as the Australian Chamber Orchestra, Opera Australia, and the Melbourne Symphony Orchestra. These partnerships provide students with the opportunity to work with industry professionals and gain practical experience in their chosen field.\n",
      "\n",
      "The conservatorium also offers a range of academic and extra-curricular programs that prepare students for careers in the music industry. For example, the Bachelor of Music (Performance) program includes specialized pathways in instrumental/vocal performance, composition, and musicology, while the Artist-In-Residence program allows students to work with internationally renowned musicians.\n",
      "\n",
      "Additionally, the conservatorium has a dedicated Career Development Office that provides students with assistance in finding employment, internships, and performance opportunities. The office also hosts regular industry events, such as masterclasses and networking sessions, to help students connect with industry professionals and gain a deeper understanding of the music industry in Australia.\n",
      "\n",
      "Overall, the location of the Sydney Conservatorium of Music and its strong industry connections provide students with a range of academic and professional opportunities that prepare them for successful careers in the music industry.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds5_processed[0]['input_text'])\n",
    "print(train_ds5_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f61f9",
   "metadata": {},
   "source": [
    "### open hermes 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c897a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = example['conversations']\n",
    "    _,input_text = create_chat_text(tokenizer, messages[0]['value'])\n",
    "    return {\"input_text\":input_text, \"output_text\":messages[1]['value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ef29c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.75k/4.75k [00:00<00:00, 11.0MB/s]\n",
      "Downloading data: 100%|██████████| 1.94G/1.94G [00:15<00:00, 127MB/s] \n",
      "Generating train split: 100%|██████████| 1001551/1001551 [00:36<00:00, 27747.60 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['custom_instruction', 'topic', 'model_name', 'model', 'skip_prompt_formatting', 'category', 'conversations', 'views', 'language', 'id', 'title', 'idx', 'hash', 'avatarUrl', 'system_prompt', 'source'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds6 = load_dataset('teknium/OpenHermes-2.5')\n",
    "train_ds6 = train_ds6['train'].shuffle(42).select(range(10000))\n",
    "train_ds6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6171a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:03<00:00, 2668.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds6_processed = train_ds6.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6dcdbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds6_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e07464b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A Sierpinski arrowhead curve is created by dividing an equilateral triangle into three smaller equilateral triangles and removing the central one, then doing the same for each remaining smaller triangle. If the length of the original triangle is 6cm, what is the perimeter of the Sierpinski arrowhead curve after the third iteration?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "After each iteration, the perimeter of the Sierpinski arrowhead curve increases by a factor of 2. Let's calculate the perimeter after the third iteration:\n",
      "\n",
      "1st iteration:\n",
      "Perimeter = 6 cm * 2 = 12 cm\n",
      "\n",
      "2nd iteration:\n",
      "Perimeter = 12 cm * 2 = 24 cm\n",
      "\n",
      "3rd iteration:\n",
      "Perimeter = 24 cm * 2 = 48 cm\n",
      "\n",
      "So, the perimeter of the Sierpinski arrowhead curve after the third iteration is 48 cm.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds6_processed[0]['input_text'])\n",
    "print(train_ds6_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e21abc",
   "metadata": {},
   "source": [
    "### starcoder-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dbc7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    _,input_text = create_chat_text(tokenizer, example['prompt'])\n",
    "    return {\"input_text\":input_text, \"output_text\":example['response']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "810e4845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 922/922 [00:00<00:00, 3.53MB/s]\n",
      "Downloading data: 100%|██████████| 90.1M/90.1M [00:02<00:00, 42.3MB/s]\n",
      "Generating train split: 100%|██████████| 50661/50661 [00:00<00:00, 57192.55 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fingerprint', 'sha1', 'seed', 'response', 'concepts', 'prompt', 'instruction', 'id'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds7 = load_dataset('bigcode/self-oss-instruct-sc2-exec-filter-50k')\n",
    "train_ds7 = train_ds7['train'].shuffle(42).select(range(10000))\n",
    "train_ds7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32d5cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 3742.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds7_processed = train_ds7.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be4dc0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds7_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fd97968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Provide the best response to a given instruction. Follow the following steps to craft your response:\n",
      "1. reason about the given instruction\n",
      "2. provide a high-quality solution\n",
      "3. offer a concise explanation\n",
      "4. write tests to verify the correctness your solution\n",
      "\n",
      "## Example 1\n",
      "### Instruction\n",
      "Design a Python function that takes a sorted array and a target value, and return a valid index where target can be inserted to maintain the array's sorted order. Optimize the function to run in logarithmic time complexity.\n",
      "\n",
      "For example, given `array = [1, 3, 5, 5, 6]` and `target = 5`, the function should return either 2 or 3 because 5 presents at both indices 2 and 3.\n",
      "\n",
      "### Response\n",
      "[Reasoning]\n",
      "To solve this problem efficiently and ensure logarithmic time complexity, we can use a binary search algorithm. Compared with a standard binary search that looks for an exact match, we can modify the algorithm such that when the target is not found, we return the `left` bound, which represents the index where the target can be inserted to maintain the array's sorted order. Since any valid index for insertion is acceptable, we can direclty return the index if there is an exact match.\n",
      "\n",
      "[Implementation]\n",
      "Here is a Python function that implements this approach:\n",
      "\n",
      "```python\n",
      "from typing import List\n",
      "\n",
      "def search_insert_position(nums: List[int], target: int) -> int:\n",
      "    \"\"\"\n",
      "    Finds the index to insert `target` into `nums` to maintain sorted order. For example,\n",
      "    given `nums = [1, 3, 5, 5, 6]` and `target = 5`, this function returns either 2 or 3,\n",
      "    as `target` can be inserted at index 2 or 3 to maintain the sorted order.\n",
      "    \"\"\"\n",
      "    left, right = 0, len(nums) - 1\n",
      "\n",
      "    while left <= right:\n",
      "        mid = (left + right) // 2\n",
      "\n",
      "        # Directly return the index if there's an exact match\n",
      "        if nums[mid] == target:\n",
      "            return mid\n",
      "        elif nums[mid] < target:\n",
      "            left = mid + 1\n",
      "        else:\n",
      "            right = mid - 1\n",
      "\n",
      "    # At this point, `left` is the correct insertion index\n",
      "    return left\n",
      "```\n",
      "\n",
      "[Explanation]\n",
      "This implementation ensures that when `target` is not found, `left` represents the correct insertion index. This is because at the end of the loop, all elements to the left of `left` are smaller than `target`, all elements to the right of `left` are equal to or larger than `target`, and `left > right`. Therefore, `left` is the correct index for insertion to maintain the sorted order.\n",
      "\n",
      "[Tests]\n",
      "To test this function, you can use the example you provided:\n",
      "\n",
      "```python\n",
      "# Provided example\n",
      "assert search_insert_position([1, 3, 5, 5, 6], 5) in [2, 3]\n",
      "# Additional cases\n",
      "assert search_insert_position([1, 3, 5, 5, 6], 2) == 1\n",
      "assert search_insert_position([1, 3, 5, 5, 6], 7) == 5\n",
      "```\n",
      "\n",
      "These tests cover the scenario where the target is found at multiple indices, as well as cases where the target is not present in the array but needs to be inserted at the correct position to maintain the sorted order.\n",
      "\n",
      "## Example 2\n",
      "### Instruction\n",
      "Define a Python function that takes a list of objects and returns a list of dictionaries representing the objects. Each dictionary should have the object's class name as key, and a list of dictionaries representing the object's attributes as value. Each attribute dictionary should have the attribute name as key and the attribute value as value. The function should use dynamic class construction and object serialization.\n",
      "\n",
      "### Response<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Here is a Python function that implements this approach:\n",
      "\n",
      "```python\n",
      "def serialize_objects(objects):\n",
      "    \"\"\"\n",
      "    Serialize a list of objects into a list of dictionaries.\n",
      "    Each dictionary represents the object's class name and attributes.\n",
      "    \"\"\"\n",
      "    serialized_objects = []\n",
      "    for obj in objects:\n",
      "        serialized_object = {\n",
      "            \"class_name\": obj.__class__.__name__,\n",
      "            \"attributes\": {}\n",
      "        }\n",
      "        for attr_name, attr_value in obj.__dict__.items():\n",
      "            attr_dict = serialize_value(attr_value)\n",
      "            serialized_object[\"attributes\"][attr_name] = attr_dict\n",
      "        serialized_objects.append(serialized_object)\n",
      "    return serialized_objects\n",
      "\n",
      "def serialize_value(value):\n",
      "    \"\"\"\n",
      "    Serialize a value into a dictionary.\n",
      "    \"\"\"\n",
      "    if isinstance(value, list):\n",
      "        serialized_value = [serialize_value(item) for item in value]\n",
      "    elif isinstance(value, dict):\n",
      "        serialized_value = {key: serialize_value(value) for key, value in value.items()}\n",
      "    else:\n",
      "        serialized_value = value\n",
      "    return serialized_value\n",
      "```\n",
      "\n",
      "This implementation uses dynamic class construction to create a dictionary for each object in the list, and then uses object serialization to convert the object's attributes into a dictionary. The `serialize_value` function is used recursively to serialize any nested lists or dictionaries.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds7_processed[0]['input_text'])\n",
    "print(train_ds7_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7c5e9",
   "metadata": {},
   "source": [
    "### The Tome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20e56f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = example['conversations']\n",
    "    _,input_text = create_chat_text(tokenizer, messages[0]['value'])\n",
    "    return {\"input_text\":input_text, \"output_text\":messages[1]['value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.75k/1.75k [00:00<00:00, 7.14MB/s]\n",
      "Downloading data: 100%|██████████| 4.58G/4.58G [02:03<00:00, 37.0MB/s]\n",
      "Generating train split: 100%|██████████| 1752473/1752473 [00:15<00:00, 114745.85 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'dataset'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds8 = load_dataset('arcee-ai/The-Tome')\n",
    "train_ds8 = train_ds8['train'].shuffle(42).select(range(10000))\n",
    "train_ds8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1374edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 3990.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds8_processed = train_ds8.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ac571d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds8_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e6f1b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Google’s recent announcement to create 10,000 new jobs in Europe, 6,000 of which will be in the UK, has been welcomed by the Business Secretary. What is the main focus of Google's expansion in Europe and how does it impact the economies of the countries it operates in?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Google's recent announcement to create 10,000 new jobs in Europe, with 6,000 of those positions located in the UK, is a significant development that has been welcomed by the Business Secretary. This expansion is primarily focused on bolstering Google's presence in key areas such as engineering, sales, and marketing, as well as in new initiatives like artificial intelligence and cloud computing.\n",
      "\n",
      "The impact of this expansion on the economies of the countries it operates in is multifaceted:\n",
      "\n",
      "1. **Job Creation**: The creation of 10,000 new jobs directly contributes to reducing unemployment rates and stimulating local economies. These positions often come with competitive salaries and benefits, which can lead to increased consumer spending and economic growth.\n",
      "\n",
      "2. **Skill Development and Training**: Google's presence often encourages the development of local talent through training programs and partnerships with educational institutions. This not only enhances the skills of the workforce but also fosters a culture of innovation and technology adoption.\n",
      "\n",
      "3. **Economic Diversification**: By investing in sectors like AI and cloud computing, Google helps diversify the economies of the countries it operates in. This diversification can reduce reliance on traditional industries and make economies more resilient to global economic fluctuations.\n",
      "\n",
      "4. **Increased Tax Revenue**: The influx of new employees leads to higher tax revenues for the government, which can be used to fund public services, infrastructure, and other economic development initiatives.\n",
      "\n",
      "5. **Boost to Local Businesses**: Google's operations often lead to a multiplier effect, benefiting local businesses through increased demand for goods and services. This can include everything from office supplies to catering services, contributing to a broader economic ecosystem.\n",
      "\n",
      "6. **International Investment**: Google's decision to expand in Europe signals confidence in the region's economic stability and potential for growth. This can attract further investment from other companies, enhancing the attractiveness of the region as a business hub.\n",
      "\n",
      "7. **Cultural and Technological Exchange**: The presence of a global tech giant like Google can facilitate the exchange of ideas and technologies, contributing to the development of local tech ecosystems and fostering a more connected and innovative community.\n",
      "\n",
      "Overall, Google's expansion in Europe is expected to have a positive impact on the economies of the countries involved, driving growth, creating jobs, and promoting technological advancement.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds8_processed[0]['input_text'])\n",
    "print(train_ds8_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061de6e1",
   "metadata": {},
   "source": [
    "### openhermes 200k unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d34b2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = example['conversations']\n",
    "    _,input_text = create_chat_text(tokenizer, messages[0]['value'])\n",
    "    return {\"input_text\":input_text, \"output_text\":messages[1]['value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f4403df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 31.0/31.0 [00:00<00:00, 143kB/s]\n",
      "Downloading data: 100%|██████████| 335M/335M [00:10<00:00, 30.8MB/s] \n",
      "Generating train split: 100%|██████████| 184892/184892 [00:01<00:00, 159088.72 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'relevance_score', 'id'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds9 = load_dataset(\"Crystalcareai/openhermes_200k_unfiltered\")\n",
    "train_ds9 = train_ds9['train'].sort(\"relevance_score\").select(range(10000))\n",
    "train_ds9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7097246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4153.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds9_processed = train_ds9.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c019658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds9_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed62cb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "If a company produces 5000 units of a product per day and sells them for $20 each, how much additional revenue would they generate if they increased their production by 10%?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "First, let's calculate the current daily revenue generated by selling 5000 units at $20 each:\n",
      "\n",
      "Current daily revenue = Units sold * Price per unit\n",
      "                                  = 5000 * $20\n",
      "                                  = $100,000\n",
      "\n",
      "Now, we need to find out how many additional units will be produced if the company increases its production by 10%. To do this, we multiply the current production (5000 units) by 10%:\n",
      "\n",
      "Additional units = Current production * 10%\n",
      "                            = 5000 * 0.1\n",
      "                            = 500 units\n",
      "\n",
      "So, the company will produce an additional 500 units per day.\n",
      "\n",
      "Next, we need to calculate the additional revenue generated from selling these extra 500 units at $20 each:\n",
      "\n",
      "Additional revenue = Additional units * Price per unit\n",
      "                             = 500 * $20\n",
      "                             = $10,000\n",
      "\n",
      "Therefore, if the company increases its production by 10%, it would generate an additional $10,000 in daily revenue.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds9_processed[0]['input_text'])\n",
    "print(train_ds9_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96115d75",
   "metadata": {},
   "source": [
    "### Synth reasoning alpaca combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4510ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    _,input_text = create_chat_text(tokenizer, example['instruction'])\n",
    "    return {\"input_text\":input_text, \"output_text\":example['output']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "854b74af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 4.20M/4.20M [00:00<00:00, 19.2MB/s]\n",
      "Generating train split: 100%|██████████| 11000/11000 [00:00<00:00, 428459.20 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds10 = load_dataset(\"Crystalcareai/synthetic_reasoning_natural_Alpaca_Combined\")\n",
    "train_ds10 = train_ds10['train'].shuffle(42).select(range(1000))\n",
    "train_ds10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38c98191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5268.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds10_processed = train_ds10.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87583a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds10_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0442e6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "If Bob is hot, then Bob is big.\n",
      "If Bob is cold, then Bob is bad.\n",
      "If Bob is slow and blue, then Bob is beautiful.\n",
      "If person is round, then person is clean.\n",
      "If Bob is strong, then Bob is happy.\n",
      "Fact:\n",
      "Bob is nice and cool.\n",
      "The following can be determined about Bob:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Bob is bad.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds10_processed[0]['input_text'])\n",
    "print(train_ds10_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67197f2a",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f534d8d-7960-4a1b-a9b6-cc927c5e74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = concatenate_datasets([train_ds1_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        train_ds2_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        train_ds3_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds4_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds5_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        train_ds6_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds7_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds8_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        train_ds9_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds10_processed.select_columns(['input_text', 'output_text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db49791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = concat_train_ds.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a506d9c6-a722-47f6-8b6b-46de80feea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 90846/90846 [00:01<00:00, 89638.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5aefe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = load_from_disk(\"/workspace/data/llama_large_mix_dataset_v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70562ccc-17b6-494b-8ed1-8a14e3f84744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(exs):\n",
    "    return {\"input_token_ids\" : tokenizer(exs['input_text'])['input_ids'],\n",
    "            \"output_token_ids\" : tokenizer(exs['output_text'])['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "497017d0-c564-4173-9353-14091e155b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/90846 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90846/90846 [00:23<00:00, 3921.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_ds = concat_train_ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43dcb5b8-2d35-45a4-9c13-ba89d836bb34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def token_lenghts(ex):\n",
    "    return {\"token_length\": len(ex['input_token_ids']) + len(ex['output_token_ids'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f39266-84af-4c78-b603-81b7c47b8d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90846/90846 [00:30<00:00, 2994.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_ds = concat_train_ds.map(token_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24bb79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = load_from_disk(\"/workspace/data/llama_large_mix_dataset_v0_1536\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3f99f1-31de-4f8a-a210-7ff677e642be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63978/790154069.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(concat_train_ds['token_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.001, 512.0]     0.687703\n",
       "(512.0, 1024.0]     0.926319\n",
       "(1024.0, 1536.0]    1.000000\n",
       "(1536.0, 2048.0]    1.000000\n",
       "(2048.0, 4096.0]    1.000000\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(concat_train_ds['token_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6110d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 90846/90846 [00:17<00:00, 5123.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_filtered_ds = concat_train_ds.filter(lambda ex: ex[\"token_length\"] < 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdb983f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 84289/84289 [00:00<00:00, 84994.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_filtered_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v0_1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2141f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 90846/90846 [00:17<00:00, 5145.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_filtered_ds = concat_train_ds.filter(lambda ex: ex[\"token_length\"] < 1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56bd5a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 90456/90456 [00:01<00:00, 74953.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_filtered_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v0_1536\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a839ca6",
   "metadata": {},
   "source": [
    "### Dataset Mixture v1\n",
    "\n",
    "- Dedup at prompt level.\n",
    "- Add training sets of academic benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93985b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94679995",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = load_from_disk(\"/workspace/data/llama_large_mix_dataset_v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7174af14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'output_text'],\n",
       "    num_rows: 90846\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b68b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def compute_hash(text):\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Create a dictionary to store unique hashes\n",
    "unique_hashes = {}\n",
    "\n",
    "# Function to check if a row should be kept\n",
    "def should_keep(row):\n",
    "    hash_value = compute_hash(row['input_text'])\n",
    "    if hash_value not in unique_hashes:\n",
    "        unique_hashes[hash_value] = True\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Apply the deduplication\n",
    "deduplicated_dataset = concat_train_ds.filter(should_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'output_text'],\n",
       "    num_rows: 84078\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2244d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Provide the best response to a given instruction. Follow the following steps to craft your response:\n",
      "1. reason about the given instruction\n",
      "2. provide a high-quality solution\n",
      "3. offer a concise explanation\n",
      "4. write tests to verify the correctness your solution\n",
      "\n",
      "## Example 1\n",
      "### Instruction\n",
      "Here are two special formulas:\n",
      "\n",
      "$$\n",
      "f_1(a, b) = (a + 1) \\cdot (b + 1) - 1\n",
      "$$\n",
      "\n",
      "$$\n",
      "f_2(k) = \\begin{cases}\n",
      "    \\frac{(k + 1)^2}{2} + k + 1 & \\text{if } k \\text{ is odd} \\\\\n",
      "    \\frac{k^2}{2} + 2k + 1 & \\text{if } k \\text{ is even}\n",
      "\\end{cases}\n",
      "$$\n",
      "\n",
      "Write a Python function to return $f_2(f_1(a, b))$ for given `a` and `b`.\n",
      "\n",
      "### Response\n",
      "[Reasoning]\n",
      "Based on the formulas you provided, we can define two Python functions, `f1(a, b)` and `f2(k)`, respectively, and then combine them to calculate $f2(f1(a, b))$ for given `a` and `b`.\n",
      "\n",
      "[Implementation]\n",
      "Here is the implementation:\n",
      "\n",
      "```python\n",
      "def f1(a, b):\n",
      "    return (a + 1) * (b + 1) - 1\n",
      "\n",
      "def f2(k):\n",
      "    if k % 2 == 0:  # Check if k is even\n",
      "        return (k**2) / 2 + 2 * k + 1\n",
      "    else:  # k is odd\n",
      "        return ((k + 1)**2) / 2 + k + 1\n",
      "\n",
      "def f1_f2_combined(a, b):\n",
      "    return f2(f1(a, b))\n",
      "```\n",
      "\n",
      "[Explanation]\n",
      "In this implementation:\n",
      "- `f1(a, b)` calculates the first formula you've given.\n",
      "- `f2(k)` calculates the second formula, with a conditional branch for whether `k` is even or odd.\n",
      "- `f1_f2_combined(a, b)` uses the output of `f1(a, b)` as the input for `f2(k)` and returns the result.\n",
      "\n",
      "[Tests]\n",
      "To test this combined function, you can simply compare it with the direct application of `f2` to the result of `f1` for different values of `a` and `b`:\n",
      "\n",
      "```python\n",
      "assert f1_f2_combined(2, 3) == f2(f1(2, 3))\n",
      "assert f1_f2_combined(4, 5) == f2(f1(4, 5))\n",
      "assert f1_f2_combined(6, 7) == f2(f1(6, 7))\n",
      "```\n",
      "\n",
      "## Example 2\n",
      "### Instruction\n",
      "Create a Python function to match a filename in a given input string. The function should take a string and return the filename if it matches the filename pattern, or an empty string otherwise. The filename pattern is defined by the regular expression pattern, `r'(\\w|.|-)+\\.(\\w+)'`, which means any combination of alphanumeric characters, dots, and hyphens followed by a dot and any combination of alphanumeric characters. For example, `my_file.txt` and `1.2-alpha.py` match the pattern, whereas `test.py.png` and `my$file.txt` do not. The function should use `re.match` to perform pattern matching.\n",
      "\n",
      "### Response<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(deduplicated_dataset[0]['input_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0821a4c",
   "metadata": {},
   "source": [
    "#### ARC-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c65ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a5cecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ai2_arc\", name=\"ARC-Easy\")\n",
    "arc_e_train_samples = ds['train'].shuffle(42).select(range(25,len(ds['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8b71ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choices', 'answerKey'],\n",
       "    num_rows: 2226\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_e_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1f6af606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arc_example(ex):\n",
    "\t\n",
    "\tchoice_texts = ex['choices']['text']\n",
    "\tchoice_labels = ex['choices']['label']\n",
    "\tchoices_list = [f\"{l}. {c}\" for c,l in zip(ex[\"choices\"][\"text\"], ex['choices']['label'])]\n",
    "\n",
    "\t\n",
    "\tchoices = \"\\n\".join(choices_list)\n",
    "\tquestion = ex[\"question\"] + f\"\\n\\n{choices}\"\n",
    "\n",
    "\tanswer_idx = {l:i for i,l in enumerate(ex['choices']['label'])}[ex['answerKey']]\n",
    "\tanswer = choices_list[answer_idx]\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "169772ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68cdfefa6574f34af3ac3ed20bd25dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arc_e_train_ds_processed = (arc_e_train_samples.shuffle(42).select(range(1000)).map(process_arc_example)\n",
    "                            .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77cfd89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Living organisms require energy for biological processes. Chemical energy in a plant cell is\n",
      "\n",
      "A. produced in vacuoles.\n",
      "B. converted from solar energy.\n",
      "C. developed by centrioles.\n",
      "D. stored as kinetic energy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "B. converted from solar energy.\n"
     ]
    }
   ],
   "source": [
    "print(arc_e_train_ds_processed[0]['input_text'])\n",
    "print(arc_e_train_ds_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f7853",
   "metadata": {},
   "source": [
    "#### ARC-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fddea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ai2_arc\", name=\"ARC-Challenge\")\n",
    "arc_c_train_samples = ds['train'].shuffle(42).select(range(25,len(ds['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1ed049d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choices', 'answerKey'],\n",
       "    num_rows: 1094\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_c_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a8047647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5196ee82f94cb9bd1b1c97bba4e7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arc_c_train_ds_processed = (arc_c_train_samples.shuffle(42).select(range(1000)).map(process_arc_example)\n",
    "                            .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Carbon on Earth is found in both living and nonliving matter. In order for carbon to be continuously available, it must be recycled. Through which process is carbon made available in the atmosphere?\n",
      "\n",
      "A. formation of fossil fuels\n",
      "B. layering of soil\n",
      "C. plant photosynthesis\n",
      "D. forest fires<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "D. forest fires\n"
     ]
    }
   ],
   "source": [
    "print(arc_c_train_ds_processed[0]['input_text'])\n",
    "print(arc_c_train_ds_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb6ff0",
   "metadata": {},
   "source": [
    "### BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d1ca50e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'passage'],\n",
       "    num_rows: 9427\n",
       "})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"boolq\")\n",
    "boolq_train_samples = ds['train']\n",
    "boolq_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3c1b388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_boolq_example(ex):\n",
    "\tquestion = \"Passage: \" + ex[\"passage\"] +\"\\n\\nQuestion: According to the passage, \" + ex[\"question\"] + \"?\"\n",
    "\tanswer = \"Yes\" if ex['answer'] else \"No\"\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "794c0c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62f7465733444d7b1000d49a6b1990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boolq_train_ds_processed = (boolq_train_samples.shuffle(42).select(range(1000)).map(process_boolq_example)\n",
    "                            .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e3ac694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Passage: Henry Daniel Mills is a fictional character in ABC's television series Once Upon a Time. Henry is the boy Emma Swan gave up to adoption; Regina Mills adopted him. Henry was originally portrayed as a child by Jared S. Gilmore, who won the Young Artist Award for Best Performance in a TV Series -- Leading Young Actor in 2012. For the show's seventh and final season, Andrew J. West later took over the role of Henry as an adult and father to a eight-year-old girl named Lucy, with Gilmore also making three appearances as Henry during the season.\n",
      "\n",
      "Question: According to the passage, did henry die in once upon a time?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "print(boolq_train_ds_processed[0]['input_text'])\n",
    "print(boolq_train_ds_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd475625",
   "metadata": {},
   "source": [
    "### Commonsense QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "940a8798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
       "    num_rows: 9734\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"commonsense_qa\")\n",
    "commonsenseqa_train_samples = ds['train'].shuffle(42).select(range(7,len(ds['train'])))\n",
    "commonsenseqa_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d0cc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_commonsenseqa_example(ex):\n",
    "\t\n",
    "\tchoice_texts = ex['choices']['text']\n",
    "\tchoice_labels = ex['choices']['label']\n",
    "\tchoices_list = [f\"{l}. {c}\" for c,l in zip(ex[\"choices\"][\"text\"], ex['choices']['label'])]\n",
    "\n",
    "\t\n",
    "\tchoices = \"\\n\".join(choices_list)\n",
    "\tquestion = ex[\"question\"] + f\"\\n\\n{choices}\"\n",
    "\n",
    "\tanswer_idx = {l:i for i,l in enumerate(ex['choices']['label'])}[ex['answerKey']]\n",
    "\tanswer = choices_list[answer_idx]\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "66151515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefb4b01b9554952b725e087d06626ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "commonsenseqa_train_ds_processed = (commonsenseqa_train_samples.shuffle(42).select(range(1000)).map(process_commonsenseqa_example)\n",
    "                                    \t\t\t\t.select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50acdc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "As the counterweight began to rise the elevator began to what into the mine?\n",
      "\n",
      "A. park\n",
      "B. reduce\n",
      "C. descend\n",
      "D. fall\n",
      "E. set<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "C. descend\n"
     ]
    }
   ],
   "source": [
    "print(commonsenseqa_train_ds_processed[0]['input_text'])\n",
    "print(commonsenseqa_train_ds_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83cfd1c",
   "metadata": {},
   "source": [
    "#### HellaSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4c794e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label'],\n",
       "    num_rows: 39905\n",
       "})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"hellaswag\", name=None)\n",
    "hellaswag_train_samples = ds['train']\n",
    "hellaswag_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "580f7653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ind': 4,\n",
       " 'activity_label': 'Removing ice from car',\n",
       " 'ctx_a': 'Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles.',\n",
       " 'ctx_b': 'then',\n",
       " 'ctx': 'Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then',\n",
       " 'endings': [', the man adds wax to the windshield and cuts it.',\n",
       "  ', a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.',\n",
       "  ', the man puts on a christmas coat, knitted with netting.',\n",
       "  ', the man continues removing the snow on his car.'],\n",
       " 'source_id': 'activitynet~v_-1IBHYS3L-Y',\n",
       " 'split': 'train',\n",
       " 'split_type': 'indomain',\n",
       " 'label': '3'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellaswag_train_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "281cbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess_hellaswag(text):\n",
    "\ttext = text.strip()\n",
    "\t# NOTE: Brackets are artifacts of the WikiHow dataset portion of HellaSwag.\n",
    "\ttext = text.replace(\" [title]\", \". \")\n",
    "\ttext = re.sub(\"\\\\[.*?\\\\]\", \"\", text)\n",
    "\ttext = text.replace(\"  \", \" \")\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7372eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hellaswag_example(ex):\n",
    "\t\n",
    "\tctx = ex[\"ctx_a\"] + \" \" + ex[\"ctx_b\"].capitalize()\n",
    "\tquery = preprocess_hellaswag(ex['activity_label'] + \". \" + ctx)\n",
    "\tchoices = [preprocess_hellaswag(ending) for ending in ex['endings']]\n",
    "\tchoices_list = [f\"{l}. {c}\" for c, l in zip(choices, string.ascii_uppercase)]\n",
    " \n",
    "\tchoices = \"\\n\".join(choices_list)\n",
    "\tquestion = query + f\"\\n\\n{choices}\"\n",
    "\n",
    "\tanswer = choices_list[int(ex['label'])]\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f8a09a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b6a8e8ace74d0bb386c4a63dabf47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hellaswag_train_ds_processed = (hellaswag_train_samples.shuffle(42).select(range(1000)).map(process_hellaswag_example)\n",
    "                                    \t\t\t\t.select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6a980b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Health. How to choose superfoods good for your heart. Eat dark, leafy greens. Kale is a green that can be used in a variety of ways and paired with almost any food. Dark, leafy greens like swiss chard or collard, mustard, and turnip greens are superfoods that are good for your heart.\n",
      "\n",
      "A. However, those with a higher cholesterol content may use some other vegetables. One ingredient that has a higher cholesterol content is nuts, seeds, chickpeas, and fruit.\n",
      "B. You can eat these greens whole whole, cut in half, or feed them whole. Leafy greens tend to be full of fiber and can calm the stomach, so try to eat steamed greens cooked whole.\n",
      "C. They are full of antioxidants and have anti-inflammatory properties, which helps promote heart health. Use kale and other dark leafy greens to make salads or side dishes.\n",
      "D. Fruit is an excellent nutrient for your heart, as you don't have to eat a ton of it. Broccoli, green beans, cucumber, carrots, and lemon are good choices.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "C. They are full of antioxidants and have anti-inflammatory properties, which helps promote heart health. Use kale and other dark leafy greens to make salads or side dishes.\n"
     ]
    }
   ],
   "source": [
    "print(hellaswag_train_ds_processed[4]['input_text'])\n",
    "print(hellaswag_train_ds_processed[4]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843af3da",
   "metadata": {},
   "source": [
    "#### Winogrande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "73cf58c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'option1', 'option2', 'answer'],\n",
       "    num_rows: 40393\n",
       "})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"winogrande\", name=\"winogrande_xl\")\n",
    "winogrande_train_samples = ds['train'].shuffle(42).select(range(5,len(ds['train'])))\n",
    "winogrande_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c11efc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_winogrande_example(ex):\n",
    "\tidx = ex['sentence'].index(\"_\")\n",
    "\tquery = ex['sentence'][:idx].strip()\n",
    "\tremaining = ex['sentence'][idx+1:].strip()\n",
    "\n",
    "\tchoices = [ex['option1'] + \" \" + remaining, ex['option2'] + \" \" + remaining]\n",
    "\tchoices_list = [f\"{l}. {c}\" for c, l in zip(choices, string.ascii_uppercase)]\n",
    "\tchoices = \"\\n\".join(choices_list)\n",
    "\n",
    "\tquestion = f\"Sentence: {query}\\n\\nWhich ones is the most likely continuation?\\n\\n{choices}\"\n",
    "\t\n",
    "\tanswer = choices_list[int(ex['answer'])-1]\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5e83aa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf59047218747ff855a7460a7f31392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wino_train_ds_processed = (winogrande_train_samples.shuffle(42).select(range(1000)).map(process_winogrande_example)\n",
    "                           .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Sentence: Paying the electricity bill is an adult responsibility for Joseph but not for Jeffrey because\n",
      "\n",
      "Which ones is the most likely continuation?\n",
      "\n",
      "A. Joseph lives on his own.\n",
      "B. Jeffrey lives on his own.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "A. Joseph lives on his own.\n"
     ]
    }
   ],
   "source": [
    "print(wino_train_ds_processed[1]['input_text'])\n",
    "print(wino_train_ds_processed[1]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951f5a2",
   "metadata": {},
   "source": [
    "#### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4babf58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7465\n",
       "})"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"gsm8k\", name=\"main\")\n",
    "gsm8k_train_samples = ds['train'].shuffle(42).select(range(8, len(ds['train'])))\t\n",
    "gsm8k_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "39c4c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gsm8k_example(ex):\n",
    "\tquestion = ex['question']\n",
    "\tanswer = ex['answer']\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_train_ds_processed = (gsm8k_train_samples.shuffle(42).select(range(1000)).map(process_gsm8k_example)\n",
    "                           .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3d84b6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Each member of Greg’s softball team needs to buy one uniform made up of a shirt, a pair of pants, and socks. A shirt costs $7.50, a pair of pants cost $15, and socks cost $4.50 each if each team member buys the uniform items on their own. If they buy the items as a group, they are given a discount. A discounted shirt cost $6.75, a discounted pair of pants cost $13.50, and discounted socks cost $3.75. How much would their team of 12 save with the group discount?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "The cost of a regularly-priced uniform is $7.50 for the shirt + $15 for pants + $4.50 for socks = $<<7.5+15+4.5=27>>27.\n",
      "The cost of a discounted uniform is $6.75 for the shirt + $13.50 for pants + $3.75 for socks = $<<6.75+13.5+3.75=24>>24.\n",
      "By purchasing the discounted uniform, each team member would save $27 for full price - $24 for discounted= $<<27-24=3>>3.\n",
      "So, for the team of 12, they would save $3 * 12 = $<<3*12=36>>36\n",
      "#### 36\n"
     ]
    }
   ],
   "source": [
    "print(gsm8k_train_ds_processed[1]['input_text'])\n",
    "print(gsm8k_train_ds_processed[1]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dd210",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "human eval, mmlu, mmlu-pro, bbhard, agieval don't have train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only test\n",
    "# ds = load_dataset(\"openai_humaneval\")\n",
    "# ds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "472070dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU_SUBJECTS = [\n",
    "# \t\"abstract_algebra\",\n",
    "# \t\"anatomy\",\n",
    "# \t\"astronomy\",\n",
    "# \t\"business_ethics\",\n",
    "# \t\"clinical_knowledge\",\n",
    "# \t\"college_biology\",\n",
    "# \t\"college_chemistry\",\n",
    "# \t\"college_computer_science\",\n",
    "# \t\"college_mathematics\",\n",
    "# \t\"college_medicine\",\n",
    "# \t\"college_physics\",\n",
    "# \t\"computer_security\",\n",
    "# \t\"conceptual_physics\",\n",
    "# \t\"econometrics\",\n",
    "# \t\"electrical_engineering\",\n",
    "# \t\"elementary_mathematics\",\n",
    "# \t\"formal_logic\",\n",
    "# \t\"global_facts\",\n",
    "# \t\"high_school_biology\",\n",
    "# \t\"high_school_chemistry\",\n",
    "# \t\"high_school_computer_science\",\n",
    "# \t\"high_school_european_history\",\n",
    "# \t\"high_school_geography\",\n",
    "# \t\"high_school_government_and_politics\",\n",
    "# \t\"high_school_macroeconomics\",\n",
    "# \t\"high_school_mathematics\",\n",
    "# \t\"high_school_microeconomics\",\n",
    "# \t\"high_school_physics\",\n",
    "# \t\"high_school_psychology\",\n",
    "# \t\"high_school_statistics\",\n",
    "# \t\"high_school_us_history\",\n",
    "# \t\"high_school_world_history\",\n",
    "# \t\"human_aging\",\n",
    "# \t\"human_sexuality\",\n",
    "# \t\"international_law\",\n",
    "# \t\"jurisprudence\",\n",
    "# \t\"logical_fallacies\",\n",
    "# \t\"machine_learning\",\n",
    "# \t\"management\",\n",
    "# \t\"marketing\",\n",
    "# \t\"medical_genetics\",\n",
    "# \t\"miscellaneous\",\n",
    "# \t\"moral_disputes\",\n",
    "# \t\"moral_scenarios\",\n",
    "# \t\"nutrition\",\n",
    "# \t\"philosophy\",\n",
    "# \t\"prehistory\",\n",
    "# \t\"professional_accounting\",\n",
    "# \t\"professional_law\",\n",
    "# \t\"professional_medicine\",\n",
    "# \t\"professional_psychology\",\n",
    "# \t\"public_relations\",\n",
    "# \t\"security_studies\",\n",
    "# \t\"sociology\",\n",
    "# \t\"us_foreign_policy\",\n",
    "# \t\"virology\",\n",
    "# \t\"world_religions\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "73b0166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"cais/mmlu\", name=MMLU_SUBJECTS[0])\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
    "# MMLU_PRO_SUBJECTS = set(ds['validation']['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "789e5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGIEVAL_DATASETS = [\"dmayhem93/agieval-aqua-rat\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-gaokao-english\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-logiqa-en\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-lsat-ar\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-lsat-lr\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-lsat-rc\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-sat-en-without-passage\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-sat-en\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-sat-math\",\n",
    "# \t\t\t\t\t\"hails/agieval-math\"\n",
    "# ]\n",
    "\n",
    "# ds = load_dataset(AGIEVAL_DATASETS[0])\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIGBENCH_HARD_TASK_NAMES =  ['boolean_expressions', \n",
    "# \t\t\t\t\t\t'causal_judgement', \n",
    "# \t\t\t\t\t\t'date_understanding', \n",
    "# \t\t\t\t\t\t'disambiguation_qa', \n",
    "# \t\t\t\t\t\t'dyck_languages',\n",
    "# \t\t\t\t\t\t'formal_fallacies',\n",
    "# \t\t\t\t\t\t'geometric_shapes',\n",
    "# \t\t\t\t\t\t'hyperbaton',\n",
    "# \t\t\t\t\t\t'logical_deduction_three_objects',\n",
    "# \t\t\t\t\t\t'logical_deduction_five_objects',\n",
    "# \t\t\t\t\t\t'logical_deduction_seven_objects',\n",
    "# \t\t\t\t\t\t'movie_recommendation',\n",
    "# \t\t\t\t\t\t'multistep_arithmetic_two',\n",
    "# \t\t\t\t\t\t'navigate',\n",
    "# \t\t\t\t\t\t'object_counting',\n",
    "# \t\t\t\t\t\t'penguins_in_a_table',\n",
    "# \t\t\t\t\t\t'reasoning_about_colored_objects',\n",
    "# \t\t\t\t\t\t'ruin_names',\n",
    "# \t\t\t\t\t\t'salient_translation_error_detection',\n",
    "# \t\t\t\t\t\t'snarks', \n",
    "# \t\t\t\t\t\t'sports_understanding', \n",
    "# \t\t\t\t\t\t'temporal_sequences',\n",
    "# \t\t\t\t\t\t'tracking_shuffled_objects_three_objects',\t\t\t\t\t\n",
    "# \t\t\t\t\t\t'tracking_shuffled_objects_five_objects',\n",
    "# \t\t\t\t\t\t'tracking_shuffled_objects_seven_objects',\n",
    "# \t\t\t\t\t\t'web_of_lies',\n",
    "# \t\t\t\t\t\t'word_sorting']\n",
    "\n",
    "# ds = load_dataset(\"maveriq/bigbenchhard\", name=BIGBENCH_HARD_TASK_NAMES[0])\n",
    "# ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c58e6",
   "metadata": {},
   "source": [
    "#### SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9e5c39c7-7d59-42e5-82fd-68a304ef216e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 130319\n",
       "})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"rajpurkar/squad_v2\")\n",
    "squad_train_samples = ds['train']\n",
    "squad_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a642d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_squad_example(ex):\n",
    "\tcontext = ex['context']\n",
    "\tquestion =ex['question']\n",
    "\tquestion = f\"Context: {context}\" + \"\\n\\n\" + f\"Question: {question}\"\n",
    "\tanswer = ex['answers']['text']\n",
    "\tif answer:\n",
    "\t\tanswer = answer[0].capitalize()\n",
    "\telse:\n",
    "\t\tanswer = 'No answer can be found in the context.'\n",
    " \n",
    "\treturn {\"input_text\":question, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "67bf82ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b79dee3bb28416c8b26eea4f362a035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad_train_ds_processed = (squad_train_samples.shuffle(42).select(range(1000)).map(process_squad_example)\n",
    "                           .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Alexandria was the most important trade center in the whole empire during Athanasius's boyhood. Intellectually, morally, and politically—it epitomized the ethnically diverse Graeco-Roman world, even more than Rome or Constantinople, Antioch or Marseilles. Its famous catechetical school, while sacrificing none of its famous passion for orthodoxy since the days of Pantaenus, Clement of Alexandria, Origen of Alexandria, Dionysius and Theognostus, had begun to take on an almost secular character in the comprehensiveness of its interests, and had counted influential pagans among its serious auditors.\n",
      "\n",
      "Question: What was Alexandria known for?\n",
      "Important trade center\n"
     ]
    }
   ],
   "source": [
    "print(squad_train_ds_processed[1]['input_text'])\n",
    "print(squad_train_ds_processed[1]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bd2ac",
   "metadata": {},
   "source": [
    "### DROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0c20a544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['section_id', 'query_id', 'passage', 'question', 'answers_spans'],\n",
       "    num_rows: 77400\n",
       "})"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"ucinlp/drop\")\n",
    "drop_train_samples = ds['train']\n",
    "drop_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4f0b5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_drop_example(ex):\n",
    "\tcontext = ex['passage']\n",
    "\tquestion = ex['question']\n",
    "\tquestion = f\"Context: {context}\" + \"\\n\\n\" + f\"Question: {question}\"\n",
    "\tanswer = ex['answers_spans']['spans']\n",
    "\tif answer:\n",
    "\t\tanswer = answer[0].capitalize()\n",
    "\telse:\n",
    "\t\tanswer = 'No answer can be found in the context.'\n",
    " \n",
    "\treturn {\"input_text\":question, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train_ds_processed = (drop_train_samples.shuffle(42).select(range(1000)).map(process_drop_example)\n",
    "                           .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "da119588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: There are 1,068,573 households in the municipality, giving an average household size of 3.3 people. Of those households, 78.4% are in formal structures (houses or apartment), while 20.5% are in informal structures (Shanty town). 94.0% of households use mains electricity for lighting. 87.3% of households have water supply to the dwelling, while 12.0% have piped water through a communal tap. 94.9% of households have regular refuse collection service. 91.4% of households have a flush toilet or chemical toilet, while 4.5% still use a bucket toilet. 82.1% of households have a refrigerator, 87.3% have a television and 70.1% have a radio. Only 34.0% have a landline telephone, but 91.3% have a cellphone. 37.9% have a computer, and 49.3% have access to the Internet (either through a computer or a cellphone).\n",
      "\n",
      "Question: How many more percent of residents lived in formal structures than informal structures?\n",
      "57.9\n"
     ]
    }
   ],
   "source": [
    "print(drop_train_ds_processed[4]['input_text'])\n",
    "print(drop_train_ds_processed[4]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd12ca",
   "metadata": {},
   "source": [
    "### Create Dataset v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "086a2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = concatenate_datasets([deduplicated_dataset.select_columns(['input_text', 'output_text']), \n",
    "                                        arc_e_train_ds_processed,\n",
    "                                        arc_c_train_ds_processed,\n",
    "                                        boolq_train_ds_processed,\n",
    "                                        commonsenseqa_train_ds_processed,\n",
    "                                        hellaswag_train_ds_processed,\n",
    "                                        wino_train_ds_processed,\n",
    "                                        gsm8k_train_ds_processed,\n",
    "                                        squad_train_ds_processed,\n",
    "                                        drop_train_ds_processed]).shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07066ecb4cd447c9bf374756f6b00d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bad64715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'output_text'],\n",
       "    num_rows: 93078\n",
       "})"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2424afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(exs):\n",
    "    return {\"input_token_ids\" : tokenizer(exs['input_text'])['input_ids'],\n",
    "            \"output_token_ids\" : tokenizer(exs['output_text'])['input_ids']}\n",
    "    \n",
    "def token_lenghts(ex):\n",
    "    return {\"token_length\": len(ex['input_token_ids']) + len(ex['output_token_ids'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e3ef931c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149e8841e14c43209a954e8364dd2d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a248a8694b84d62a1bfa118230639b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_ds = concat_train_ds.map(tokenize, batched=True)\n",
    "concat_train_ds = concat_train_ds.map(token_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "527219e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3327/790154069.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(concat_train_ds['token_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.001, 512.0]     0.699199\n",
       "(512.0, 1024.0]     0.925278\n",
       "(1024.0, 1536.0]    0.995885\n",
       "(1536.0, 2048.0]    0.999194\n",
       "(2048.0, 4096.0]    0.999914\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(concat_train_ds['token_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bc9c5a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca2b8fe8d5f496e915f2125488892b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_filtered_ds = concat_train_ds.filter(lambda ex: ex[\"token_length\"] < 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "08a5a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08bfa5b169f492985481640a11b5eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/86090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_filtered_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v1_1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f6c3181c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ce33f940e64e63b8697c05b7d515b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_filtered_ds = concat_train_ds.filter(lambda ex: ex[\"token_length\"] < 1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ff3165a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8340a7c9ce834e608c67a18d1f96b4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/92693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_filtered_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v1_1536\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0227556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac24b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09663a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafdd892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bce9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
