{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88954bae-0756-4fc5-9b77-a9e6e2ce9fb3",
   "metadata": {},
   "source": [
    "Prepare IFT datasets with `{\"input_text\":, ... \"output_text\":...}` format. Prepare in chat format to finetune chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d988a-baed-44c9-a83b-e4b4858d87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import concatenate_datasets, load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5800486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb21107-ce34-463b-9755-bdcaf101dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_text(tokenizer, user_prompt, system=\"You are a helpful AI assistant.\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return messages, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc853a14-fa20-49de-b173-ef9ad30c0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c25ab-14a6-4e22-bc64-a7c352972da3",
   "metadata": {},
   "source": [
    "### oasst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426a8db-f0aa-484c-bd84-1994f9666d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    t = example[\"text\"]\n",
    "    \n",
    "    human_prefix, asst_prefix = \"### Human: \", \"### Assistant: \"\n",
    "    human_matches = list(re.finditer(human_prefix, t))\n",
    "    asst_matches = list(re.finditer(asst_prefix, t))\n",
    "    human_start_idx, _ = human_matches[0].span(0)\n",
    "    asst_start_idx, _ = asst_matches[0].span(0)\n",
    "    if len(human_matches) > 1:\n",
    "        asst_end_idx = human_matches[1].span(0)[0]\n",
    "    else:\n",
    "        asst_end_idx = len(t)\n",
    "    \n",
    "    human_prompt = t[human_start_idx+len(human_prefix):asst_start_idx]\n",
    "    asst_prompt = t[asst_start_idx+len(asst_prefix):asst_end_idx]\n",
    "    \n",
    "    _,input_text = create_chat_text(tokenizer, human_prompt)\n",
    "    output_text = asst_prompt\n",
    "    return {\"input_text\":input_text, \"output_text\":output_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21607f69-ac2c-48be-ac7c-25aef1f1d4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|█████████████████████████████████████████████████████████| 9846/9846 [00:00<00:00, 159085.93 examples/s]\n",
      "Generating test split: 100%|█████████████████████████████████████████████████████████████| 518/518 [00:00<00:00, 75183.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds1 = load_dataset(\"timdettmers/openassistant-guanaco\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10d8fb-65c8-4f70-ad0c-77c06d8cce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████| 9846/9846 [00:00<00:00, 10268.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds1_processed = train_ds1.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cfceb-a66c-4efe-93b7-7585ceb88690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9846"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds1_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a13ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Siento un dolor punzante en la zona superior izquierda de mi abdomen, ¿qué podría estar causándolo?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Lo siento, no puedo contestar a esa pregunta, ya que no tengo la suficiente información cómo para dictaminar un resultado. Si de verdad padece de algún problema físico, recomiendo ir inmediatamente a un hospital cercano.\n"
     ]
    }
   ],
   "source": [
    "idx = 105\n",
    "print(train_ds1_processed[idx]['input_text'])\n",
    "print(train_ds1_processed[idx]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befe39e-e090-4b6b-b980-fe1566dbdbc1",
   "metadata": {},
   "source": [
    "### orca-math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2520a-e078-497c-b9db-a82047325fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    _,input_text = create_chat_text(tokenizer, example['question'])\n",
    "    return {\"input_text\":input_text, \"output_text\":example['answer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11937249-cc2a-4e04-a33f-534629317ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|█████████████████████████████████████████████████████| 200035/200035 [00:01<00:00, 148710.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds2 = load_dataset(\"microsoft/orca-math-word-problems-200k\")\n",
    "train_ds2, eval_ds2 = train_ds2['train'].shuffle(42).select(range(10000)), train_ds2['train'].shuffle(42).select(range(10000, 10100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdd733-d301-4cfe-8d8e-0ce7ee2f6e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 7449.35 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 5816.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds2_processed = train_ds2.map(process_example)\n",
    "eval_ds2_processed = eval_ds2.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be20282-b79c-42bd-b152-d9614954a911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds2_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1741d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Sally had 13 peaches at her roadside fruit dish.  She went to the orchard and picked peaches to stock up. She picked 55 peaches. There are _____ peaches now.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Sally originally had 13 peaches. She picked 55 more peaches. To find out the total number of peaches she has now, we add the two amounts together:\n",
      "\n",
      "13 (original peaches) + 55 (picked peaches) = 68 peaches\n",
      "\n",
      "So, there are 68 peaches now.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds2_processed[0]['input_text'])\n",
    "print(train_ds2_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "cara leaves her home and walks toward don ' s house . two hours later , don leaves his home and walks toward cara ' s house . the distance between their homes is 45 kilometers , cara ' s walking speed is 6 km / h , and don ' s walking speed is 5 km / h . how many kilometers will cara walk before she meets don ?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Let's denote the distance Cara walks before meeting Don as \\( x \\) kilometers.\n",
      "\n",
      "Since Cara walks at a speed of 6 km/h, she will have walked for \\( \\frac{x}{6} \\) hours before meeting Don.\n",
      "\n",
      "Don starts walking 2 hours after Cara, so by the time they meet, Don will have been walking for \\( \\frac{x}{6} - 2 \\) hours.\n",
      "\n",
      "Don walks at a speed of 5 km/h, so the distance Don walks before meeting Cara is \\( 5 \\times \\left( \\frac{x}{6} - 2 \\right) \\) kilometers.\n",
      "\n",
      "The sum of the distances walked by Cara and Don before they meet should equal the total distance between their homes, which is 45 kilometers. Therefore, we can set up the following equation:\n",
      "\n",
      "\\[ x + 5 \\left( \\frac{x}{6} - 2 \\right) = 45 \\]\n",
      "\n",
      "Now, let's solve for \\( x \\):\n",
      "\n",
      "\\[ x + \\frac{5x}{6} - 10 = 45 \\]\n",
      "\n",
      "To combine like terms, we need a common denominator, which is 6:\n",
      "\n",
      "\\[ \\frac{6x}{6} + \\frac{5x}{6} - 10 = 45 \\]\n",
      "\n",
      "\\[ \\frac{11x}{6} - 10 = 45 \\]\n",
      "\n",
      "Now, add 10 to both sides:\n",
      "\n",
      "\\[ \\frac{11x}{6} = 55 \\]\n",
      "\n",
      "Next, multiply both sides by 6 to solve for \\( x \\):\n",
      "\n",
      "\\[ 11x = 330 \\]\n",
      "\n",
      "Finally, divide both sides by 11:\n",
      "\n",
      "\\[ x = 30 \\]\n",
      "\n",
      "So, Cara will walk 30 kilometers before she meets Don.\n"
     ]
    }
   ],
   "source": [
    "print(eval_ds2_processed[0]['input_text'])\n",
    "print(eval_ds2_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc438a-d654-48de-ab93-ab4e22b4647c",
   "metadata": {},
   "source": [
    "### meta-math-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d080e0-ce10-494d-aed7-790067a854c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    _,input_text = create_chat_text(tokenizer, example['query'])\n",
    "    return {\"input_text\":input_text, \"output_text\":example['response']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3022d-46ca-4a8f-b33b-45a52ba109e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████████████████████████████████████████████████| 395000/395000 [00:09<00:00, 40243.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds3 = load_dataset(\"meta-math/MetaMathQA\")\n",
    "train_ds3, eval_ds3 = train_ds3['train'].shuffle(42).select(range(10000)), train_ds3['train'].shuffle(42).select(range(10000, 10100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c70ea5-5b0b-471b-b558-2604f1448514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 6783.65 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 5691.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds3_processed = train_ds3.map(process_example)\n",
    "eval_ds3_processed = eval_ds3.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab6be5-8645-456d-b71e-93b9f60b7c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'MATH_SV',\n",
       " 'query': 'Compute the sum of the squares of the roots of the equation \\\\[x^{2018} +44x^{2015} + 3x^3 + 404 = X The answer is 0. What is the value of unknown variable X?',\n",
       " 'original_question': 'Compute the sum of the squares of the roots of the equation \\\\[x^{2018} +44x^{2015} + 3x^3 + 404 = 0.\\\\]',\n",
       " 'response': 'To solve this problem, we need to determine the value of x that will make the sum of the squares of the roots of the equation equal to 0.\\nThe equation is given as:\\n\\\\[x^{2018} + 44x^{2015} + 3x^3 + 404 = X\\\\]\\nTo find the sum of the squares of the roots, we need to find the roots of the equation and then square them.\\nSince the answer is 0, it means that the sum of the squares of the roots must be equal to 0.\\nTo make the sum of squares 0, we need to find the values of x that will make each square equal to 0.\\nIf a square is equal to 0, it means that the value inside the square must be 0.\\nIn this case, we need to set each term inside the square to 0 and solve for x.\\nSetting each term to 0, we have:\\n\\\\[x^{2018} = 0\\\\]\\n\\\\[44x^{2015} = 0\\\\]\\n\\\\[3x^3 = 0\\\\]\\n\\\\[404 = 0\\\\]\\nThe last equation, 404 = 0, does not have a solution because it is a contradiction.\\nThe third equation, 3x^3 = 0, has a solution of x = 0.\\nThe second equation, 44x^{2015} = 0, also has a solution of x = 0.\\nThe first equation, x^{2018} = 0, has a solution of x = 0.\\nTherefore, the value of unknown variable x is 0.\\nThe answer is: 0',\n",
       " 'input_text': '<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nCompute the sum of the squares of the roots of the equation \\\\[x^{2018} +44x^{2015} + 3x^3 + 404 = X The answer is 0. What is the value of unknown variable X?<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " 'output_text': 'To solve this problem, we need to determine the value of x that will make the sum of the squares of the roots of the equation equal to 0.\\nThe equation is given as:\\n\\\\[x^{2018} + 44x^{2015} + 3x^3 + 404 = X\\\\]\\nTo find the sum of the squares of the roots, we need to find the roots of the equation and then square them.\\nSince the answer is 0, it means that the sum of the squares of the roots must be equal to 0.\\nTo make the sum of squares 0, we need to find the values of x that will make each square equal to 0.\\nIf a square is equal to 0, it means that the value inside the square must be 0.\\nIn this case, we need to set each term inside the square to 0 and solve for x.\\nSetting each term to 0, we have:\\n\\\\[x^{2018} = 0\\\\]\\n\\\\[44x^{2015} = 0\\\\]\\n\\\\[3x^3 = 0\\\\]\\n\\\\[404 = 0\\\\]\\nThe last equation, 404 = 0, does not have a solution because it is a contradiction.\\nThe third equation, 3x^3 = 0, has a solution of x = 0.\\nThe second equation, 44x^{2015} = 0, also has a solution of x = 0.\\nThe first equation, x^{2018} = 0, has a solution of x = 0.\\nTherefore, the value of unknown variable x is 0.\\nThe answer is: 0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds3_processed[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefadd93-8ca7-41d6-a4f9-4d3f237a6deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds3_processed), len(eval_ds3_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce57b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "If Anna wants to create a smiley face shape using red and yellow tulips, she requires 8 red tulips for each eye and 18 red tulips for the smile. Additionally, she needs 9 times the number of tulips in the smile to create the yellow background of the face. What is the total number of tulips that Anna needs?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Anna needs 8 red tulips for each eye, so for both eyes she needs 8 * 2 = 16 red tulips.\n",
      "She also needs 18 red tulips for the smile.\n",
      "The total number of red tulips she needs is 16 + 18 = 34 red tulips.\n",
      "For the yellow background, she needs 9 times the number of tulips in the smile, so she needs 9 * 18 = 162 yellow tulips.\n",
      "The total number of tulips she needs is 34 + 162 = 196 tulips.\n",
      "#### 196\n",
      "The answer is: 196\n"
     ]
    }
   ],
   "source": [
    "print(train_ds3_processed[0]['input_text'])\n",
    "print(train_ds3_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c53fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "After waking up to find the thermostat set at 40 degrees, Jerry doubles the initial temperature. However, his dad reduces the temperature by 30 degrees after noticing. Later, Jerry's mother further reduces the temperature by 30%, and then his sister increases it by 24 degrees. What is the final temperature?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "The initial temperature was 40 degrees. \n",
      "Jerry doubles the initial temperature, so it becomes 40 * 2 = 80 degrees. \n",
      "His dad reduces the temperature by 30 degrees, so it becomes 80 - 30 = 50 degrees. \n",
      "Jerry's mother reduces the temperature by 30%, so she reduces it by 50 * 0.30 = 15 degrees. \n",
      "After his mother's reduction, the temperature becomes 50 - 15 = 35 degrees. \n",
      "Jerry's sister increases the temperature by 24 degrees, so it becomes 35 + 24 = 59 degrees. \n",
      "The final temperature is 59 degrees. \n",
      "#### 59\n",
      "The answer is: 59\n"
     ]
    }
   ],
   "source": [
    "print(eval_ds3_processed[0]['input_text'])\n",
    "print(eval_ds3_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1706e-ed2b-4ba9-9a4c-92f3268f9a47",
   "metadata": {},
   "source": [
    "### ultrafeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d5080-bbac-4906-ab7d-484bcb7bdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    chosen = example['chosen']\n",
    "    _,input_text = create_chat_text(tokenizer, chosen[0]['content'])\n",
    "    return {\"input_text\":input_text, \"output_text\":chosen[1]['content']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54487bae-7126-44c7-b9ed-eb7c40191adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train_prefs split: 100%|██████████████████████████████████████████████████| 61135/61135 [00:01<00:00, 35980.10 examples/s]\n",
      "Generating train_sft split: 100%|████████████████████████████████████████████████████| 61135/61135 [00:01<00:00, 33563.17 examples/s]\n",
      "Generating test_prefs split: 100%|█████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 31326.61 examples/s]\n",
      "Generating test_sft split: 100%|███████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 33861.08 examples/s]\n",
      "Generating train_gen split: 100%|████████████████████████████████████████████████████| 61135/61135 [00:01<00:00, 37584.17 examples/s]\n",
      "Generating test_gen split: 100%|███████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 33052.56 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:02<00:00, 4156.69 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 3930.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds4 = load_dataset('HuggingFaceH4/ultrafeedback_binarized')\n",
    "train_ds4, eval_ds4 = train_ds4['train_sft'].shuffle(42).select(range(10000)), train_ds4['train_sft'].shuffle(42).select(range(10000, 10100))\n",
    "train_ds4_processed = train_ds4.map(process_example)\n",
    "eval_ds4_processed = eval_ds4.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea200f8f-a07b-4a79-8047-b17b006ea674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds4_processed), len(eval_ds4_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793ebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Do you know something about crystallography and structure factor?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Crystallography is the science of the arrangement of atoms in solids. It is a vast and interdisciplinary field that has applications in physics, chemistry, materials science, biology, and engineering.\n",
      "\n",
      "The structure factor is a mathematical function that is used to describe the diffraction of waves by a crystal. It is a complex number that is related to the atomic positions in the crystal.\n",
      "\n",
      "The structure factor can be used to calculate the intensity of the diffracted waves. This information can be used to determine the atomic positions in the crystal and to study the structure of materials.\n",
      "\n",
      "Crystallography is a powerful tool for understanding the structure of materials. It has been used to determine the structures of many important materials, including metals, semiconductors, and pharmaceuticals. It is also used to study the structure of biological materials, such as proteins and DNA.\n",
      "\n",
      "The structure factor is a key concept in crystallography. It is used to describe the diffraction of waves by a crystal and to calculate the intensity of the diffracted waves. This information can be used to determine the atomic positions in the crystal and to study the structure of materials.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds4_processed[0]['input_text'])\n",
    "print(train_ds4_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10348f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "instruction:\n",
      "The task is to read a passage and convert numbers in their text forms into digit forms. The text numbers can be used to represent Quantity, Dates and Time, Addresses or Positions.\n",
      "question:\n",
      "was the case of the forgeries upon the Bank of England discovered in eighteen sixty three,\n",
      "answer:\n",
      "was the case of the forgeries upon the Bank of England discovered in 1863,\n",
      "\n",
      "\n",
      "question:\n",
      "The Chronicles of Newgate, Volume two. By Arthur Griffiths. Section twenty: Newgate Notorieties, part one.\n",
      "answer:\n",
      "The Chronicles of Newgate, Volume 2. By Arthur Griffiths. Section 20: Newgate Notorieties, part 1.\n",
      "\n",
      "\n",
      "question:\n",
      "If Oswald left the Depository Building at twelve:thirty three p.m.,\n",
      "answer:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "If Oswald left the Depository Building at 12:33 p.m.,\n"
     ]
    }
   ],
   "source": [
    "print(eval_ds4_processed[0]['input_text'])\n",
    "print(eval_ds4_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684a568",
   "metadata": {},
   "source": [
    "### ultrachat-200k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af94d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = example['messages']\n",
    "    _,input_text = create_chat_text(tokenizer, messages[0]['content'])\n",
    "    return {\"input_text\":input_text, \"output_text\":messages[1]['content']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de8364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train_sft split: 100%|██████████████████████████████████████████████████| 207865/207865 [00:10<00:00, 20130.02 examples/s]\n",
      "Generating test_sft split: 100%|█████████████████████████████████████████████████████| 23110/23110 [00:01<00:00, 20678.60 examples/s]\n",
      "Generating train_gen split: 100%|██████████████████████████████████████████████████| 256032/256032 [00:09<00:00, 26269.86 examples/s]\n",
      "Generating test_gen split: 100%|█████████████████████████████████████████████████████| 28304/28304 [00:01<00:00, 24686.44 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:02<00:00, 4279.21 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4067.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds5 = load_dataset('HuggingFaceH4/ultrachat_200k', name='default')\n",
    "train_ds5, eval_ds5 = train_ds5['train_sft'].shuffle(42).select(range(10000)), train_ds5['train_sft'].shuffle(42).select(range(10000, 10100))\n",
    "train_ds5_processed = train_ds5.map(process_example)\n",
    "eval_ds5_processed = eval_ds5.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb026b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds5_processed), len(eval_ds5_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42891290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "How does the location of the Sydney Conservatorium of Music impact the academic and professional opportunities available to music students, and how does the conservatorium support student engagement with the music industry in Australia?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "The location of the Sydney Conservatorium of Music, which is situated in the heart of Sydney's cultural precinct, impacts both the academic and professional opportunities available to music students. The conservatorium is located near several major performing arts venues and organizations, including the Sydney Opera House, the Australian Broadcasting Corporation, and the Sydney Symphony Orchestra, providing students with easy access to performances, rehearsals, and networking opportunities.\n",
      "\n",
      "One of the primary ways the conservatorium supports student engagement with the music industry in Australia is through its strong industry connections. The conservatorium has established partnerships with numerous professional music organizations, such as the Australian Chamber Orchestra, Opera Australia, and the Melbourne Symphony Orchestra. These partnerships provide students with the opportunity to work with industry professionals and gain practical experience in their chosen field.\n",
      "\n",
      "The conservatorium also offers a range of academic and extra-curricular programs that prepare students for careers in the music industry. For example, the Bachelor of Music (Performance) program includes specialized pathways in instrumental/vocal performance, composition, and musicology, while the Artist-In-Residence program allows students to work with internationally renowned musicians.\n",
      "\n",
      "Additionally, the conservatorium has a dedicated Career Development Office that provides students with assistance in finding employment, internships, and performance opportunities. The office also hosts regular industry events, such as masterclasses and networking sessions, to help students connect with industry professionals and gain a deeper understanding of the music industry in Australia.\n",
      "\n",
      "Overall, the location of the Sydney Conservatorium of Music and its strong industry connections provide students with a range of academic and professional opportunities that prepare them for successful careers in the music industry.\n"
     ]
    }
   ],
   "source": [
    "print(train_ds5_processed[0]['input_text'])\n",
    "print(train_ds5_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d7cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Could you paraphrase the line \"Oh mark the words that I do say, Who know? Tomorrow could be your day\"?: From Those Around, I hear a cry, a mouthful, a hopeless sigh. I hear their footsteps leaving slow and then I know my soul must fly!\n",
      "Oh mark the words that I do say, Who know? Tomorrow could be your day. At last, it comes to Heaven or Hell. Decide which now, do not delay! Come on my brother let's pray. Decide which now, do not delay!\n",
      "Oh God! Oh God: I can not see! My eyes are blind! Am I still me?! Or has my soul been led astray. And forced to pay a priceless Fee?!\n",
      "Alas to dust we all return, some shall rejoice, while others burn. If only I knew that before, the line grew short, and came my turn!\n",
      "Oh mark my words That I don't say, who knows? Tomorrow could be your day. At last, it come to Heaven and Hell. Decide which now, do not delay. Come on my brothers, let's pray. Decide which now, do not delay.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Listen closely to my words, for tomorrow could be your fate. As I hear the cries of those around me, I know my time has come to soar away. It's up to you to make your choice, whether heaven or hell will be your stay. Don't wait, decide now and let us pray. But I fear I'm lost, my vision gone, my soul lost in a costly game. For we all must return to dust in the end, either rejoicing or in flames. So heed my warning, tomorrow is uncertain. Choose wisely and don't delay, come on my brothers let us pray.\n"
     ]
    }
   ],
   "source": [
    "print(eval_ds5_processed[0]['input_text'])\n",
    "print(eval_ds5_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f61f9",
   "metadata": {},
   "source": [
    "### open hermes 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = example['conversations']\n",
    "    _,input_text = create_chat_text(tokenizer, messages[0]['value'])\n",
    "    return {\"input_text\":input_text, \"output_text\":messages[1]['value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef29c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split:   0%|                                                                     | 0/1001551 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "train_ds6 = load_dataset('teknium/OpenHermes-2.5')\n",
    "train_ds6, eval_ds6 = train_ds6['train'].shuffle(42).select(range(10000)), train_ds6['train'].shuffle(42).select(range(10000, 10100))\n",
    "train_ds6_processed = train_ds6.map(process_example)\n",
    "eval_ds6_processed = eval_ds6.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcdbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds6_processed), len(eval_ds6_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07464b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds6_processed[0]['input_text'])\n",
    "print(train_ds6_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_ds6_processed[0]['input_text'])\n",
    "print(eval_ds6_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e21abc",
   "metadata": {},
   "source": [
    "### starcoder-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    _,input_text = create_chat_text(tokenizer, example['prompt'])\n",
    "    return {\"input_text\":input_text, \"output_text\":example['response']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds7 = load_dataset('bigcode/self-oss-instruct-sc2-exec-filter-50k')\n",
    "train_ds7, eval_ds7 = train_ds7['train'].shuffle(42).select(range(10000)), train_ds7['train'].shuffle(42).select(range(10000, 10100))\n",
    "train_ds7_processed = train_ds7.map(process_example)\n",
    "eval_ds7_processed = eval_ds7.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds7_processed), len(eval_ds7_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd97968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds7_processed[0]['input_text'])\n",
    "print(train_ds7_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_ds7_processed[0]['input_text'])\n",
    "print(eval_ds7_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7c5e9",
   "metadata": {},
   "source": [
    "### The Tome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e56f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = example['conversations']\n",
    "    _,input_text = create_chat_text(tokenizer, messages[0]['value'])\n",
    "    return {\"input_text\":input_text, \"output_text\":messages[1]['value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba562d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds8 = load_dataset('arcee-ai/The-Tome')\n",
    "train_ds8, eval_ds8 = train_ds8['train'].shuffle(42).select(range(10000)), train_ds8['train'].shuffle(42).select(range(10000, 10100))\n",
    "train_ds8_processed = train_ds8.map(process_example)\n",
    "eval_ds8_processed = eval_ds8.map(process_example)\n",
    "len(train_ds8_processed), len(eval_ds8_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds8_processed[0]['input_text'])\n",
    "print(train_ds8_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_ds8_processed[0]['input_text'])\n",
    "print(eval_ds8_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061de6e1",
   "metadata": {},
   "source": [
    "### openhermes 200k unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = example['conversations']\n",
    "    _,input_text = create_chat_text(tokenizer, messages[0]['value'])\n",
    "    return {\"input_text\":input_text, \"output_text\":messages[1]['value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4403df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds9 = load_dataset(\"Crystalcareai/openhermes_200k_unfiltered\")\n",
    "train_ds9, eval_ds9 = train_ds9['train'].sort(\"relevance_score\").select(range(10000)), train_ds9['train'].sort(\"relevance_score\").select(range(10000, 10100))\n",
    "train_ds9_processed = train_ds9.map(process_example)\n",
    "eval_ds9_processed = eval_ds9.map(process_example)\n",
    "len(train_ds9_processed), len(eval_ds9_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds9_processed[0]['input_text'])\n",
    "print(train_ds9_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_ds9_processed[0]['input_text'])\n",
    "print(eval_ds9_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96115d75",
   "metadata": {},
   "source": [
    "### Synth reasoning alpaca combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4510ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    _,input_text = create_chat_text(tokenizer, example['instruction'])\n",
    "    return {\"input_text\":input_text, \"output_text\":example['output']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds10 = load_dataset(\"Crystalcareai/synthetic_reasoning_natural_Alpaca_Combined\")\n",
    "train_ds10, eval_ds10 = train_ds10['train'].shuffle(42).select(range(1000)), train_ds10['train'].shuffle(42).select(range(1000, 1010))\n",
    "train_ds10_processed = train_ds10.map(process_example)\n",
    "eval_ds10_processed = eval_ds10.map(process_example)\n",
    "len(train_ds10_processed), len(eval_ds10_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds10_processed[0]['input_text'])\n",
    "print(train_ds10_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df57521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_ds10_processed[0]['input_text'])\n",
    "print(eval_ds10_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67197f2a",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f534d8d-7960-4a1b-a9b6-cc927c5e74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = concatenate_datasets([train_ds1_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        train_ds2_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        train_ds3_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds4_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds5_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        train_ds6_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds7_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds8_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        train_ds9_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        train_ds10_processed.select_columns(['input_text', 'output_text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_eval_ds = concatenate_datasets([eval_ds2_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        eval_ds3_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        eval_ds4_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        eval_ds5_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        eval_ds6_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        eval_ds7_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        eval_ds8_processed.select_columns(['input_text', 'output_text']), \n",
    "                                        eval_ds9_processed.select_columns(['input_text', 'output_text']),\n",
    "                                        eval_ds10_processed.select_columns(['input_text', 'output_text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concat_train_ds), len(concat_eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = concat_train_ds.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506d9c6-a722-47f6-8b6b-46de80feea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████| 90846/90846 [00:01<00:00, 48230.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_ds.save_to_disk(\"/home/k/data/qwen_large_mix_dataset_v0\")\n",
    "concat_train_ds = load_from_disk(\"/home/k/data/qwen_large_mix_dataset_v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d79640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████| 810/810 [00:00<00:00, 58053.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_eval_ds.save_to_disk(\"/home/k/data/qwen_large_mix_dataset_v0_eval\")\n",
    "concat_eval_ds = load_from_disk(\"/home/k/data/qwen_large_mix_dataset_v0_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70562ccc-17b6-494b-8ed1-8a14e3f84744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(exs):\n",
    "    return {\"inp_ids\" : tokenizer(exs['input_text'])['input_ids'], \n",
    "            \"out_ids\" : tokenizer(exs['output_text'])['input_ids']}\n",
    "def token_lenghts(ex):\n",
    "    return {\"input_length\": len(ex['inp_ids']), \n",
    "            \"output_length\": len(ex['out_ids']),\n",
    "            \"token_length\": len(ex['inp_ids']) + len(ex['out_ids'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497017d0-c564-4173-9353-14091e155b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 90846/90846 [00:21<00:00, 4143.99 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 90846/90846 [00:29<00:00, 3100.60 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████| 810/810 [00:00<00:00, 3850.00 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████| 810/810 [00:00<00:00, 3034.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_ds = concat_train_ds.map(tokenize, batched=True).map(token_lenghts)\n",
    "concat_eval_ds  = concat_eval_ds.map(tokenize, batched=True).map(token_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb79bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████| 90846/90846 [00:16<00:00, 5549.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████| 84037/84037 [00:01<00:00, 75841.31 examples/s]\n",
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████| 90846/90846 [00:16<00:00, 5557.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████| 90421/90421 [00:01<00:00, 71518.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "concat_train_filtered_ds = concat_train_ds.filter(lambda ex: ex[\"token_length\"] < 1024)\n",
    "concat_train_filtered_ds.save_to_disk(\"/home/k/data/qwen_large_mix_dataset_v0_1024\")\n",
    "\n",
    "concat_train_filtered_ds = concat_train_ds.filter(lambda ex: ex[\"token_length\"] < 1536)\n",
    "concat_train_filtered_ds.save_to_disk(\"/home/k/qwen_large_mix_dataset_v0_1536\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f99f1-31de-4f8a-a210-7ff677e642be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Length Bins\n",
      "(-0.001, 512.0]     0.867820\n",
      "(512.0, 1024.0]     0.984710\n",
      "(1024.0, 1536.0]    0.999318\n",
      "(1536.0, 2048.0]    0.999758\n",
      "(2048.0, 4096.0]    1.000000\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "Output Length Bins\n",
      "(-0.001, 512.0]     0.898730\n",
      "(512.0, 1024.0]     0.993329\n",
      "(1024.0, 1536.0]    0.998888\n",
      "(1536.0, 2048.0]    0.999538\n",
      "(2048.0, 4096.0]    0.999989\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "Total Length Bins\n",
      "(-0.001, 512.0]     0.695165\n",
      "(512.0, 1024.0]     0.925357\n",
      "(1024.0, 1536.0]    0.995366\n",
      "(1536.0, 2048.0]    0.998888\n",
      "(2048.0, 4096.0]    0.999824\n",
      "Name: proportion, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_length_counts = pd.value_counts(concat_train_ds['input_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n",
    "input_length_counts.index.name = 'Input Length Bins'\n",
    "\n",
    "output_length_counts = pd.value_counts(concat_train_ds['output_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n",
    "output_length_counts.index.name = 'Output Length Bins'\n",
    "\n",
    "token_length_counts = pd.value_counts(concat_train_ds['token_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n",
    "token_length_counts.index.name = 'Total Length Bins'\n",
    "\n",
    "print(input_length_counts, \"\\n\") \n",
    "print(output_length_counts, \"\\n\") \n",
    "print(token_length_counts, \"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911c5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Length Bins\n",
      "(-0.001, 512.0]     0.845679\n",
      "(512.0, 1024.0]     0.986420\n",
      "(1024.0, 1536.0]    1.000000\n",
      "(1536.0, 2048.0]    1.000000\n",
      "(2048.0, 4096.0]    1.000000\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "Output Length Bins\n",
      "(-0.001, 512.0]     0.872840\n",
      "(512.0, 1024.0]     0.988889\n",
      "(1024.0, 1536.0]    0.998765\n",
      "(1536.0, 2048.0]    0.998765\n",
      "(2048.0, 4096.0]    1.000000\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "Total Length Bins\n",
      "(-0.001, 512.0]     0.654321\n",
      "(512.0, 1024.0]     0.922222\n",
      "(1024.0, 1536.0]    0.993827\n",
      "(1536.0, 2048.0]    0.998765\n",
      "(2048.0, 4096.0]    1.000000\n",
      "Name: proportion, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_length_counts = pd.value_counts(concat_eval_ds['input_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n",
    "input_length_counts.index.name = 'Input Length Bins'\n",
    "\n",
    "output_length_counts = pd.value_counts(concat_eval_ds['output_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n",
    "output_length_counts.index.name = 'Output Length Bins'\n",
    "\n",
    "token_length_counts = pd.value_counts(concat_eval_ds['token_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n",
    "token_length_counts.index.name = 'Total Length Bins'\n",
    "\n",
    "print(input_length_counts, \"\\n\") \n",
    "print(output_length_counts, \"\\n\") \n",
    "print(token_length_counts, \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████| 90846/90846 [00:08<00:00, 10449.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from datasets import Dataset\n",
    "\n",
    "def compute_hash(text): return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "# Create a set to store unique hashes from the training dataset\n",
    "unique_hashes = set(concat_train_ds.map(lambda row: {\"hash\":compute_hash(row['input_text'])})['hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50154d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████| 810/810 [00:00<00:00, 5076.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the deduplication to the eval dataset using the filter function\n",
    "deduplicated_eval_dataset = concat_eval_ds.filter(lambda row: compute_hash(row['input_text']) not in unique_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8f828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████| 736/736 [00:00<00:00, 25996.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# save \n",
    "deduplicated_eval_dataset.save_to_disk(\"/home/k/data/qwen_large_mix_dataset_v0_dedup_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a839ca6",
   "metadata": {},
   "source": [
    "### Dataset Mixture v1\n",
    "\n",
    "- Dedup at prompt level.\n",
    "- Add training sets of academic benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93985b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94679995",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = load_from_disk(\"/home/k/data/qwen_large_mix_dataset_v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174af14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'output_text', 'inp_ids', 'out_ids', 'input_length', 'output_length', 'token_length'],\n",
       "    num_rows: 90846\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b68b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████| 90846/90846 [00:16<00:00, 5519.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "def compute_hash(text):\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Create a dictionary to store unique hashes\n",
    "unique_hashes = {}\n",
    "\n",
    "# Function to check if a row should be kept\n",
    "def should_keep(row):\n",
    "    hash_value = compute_hash(row['input_text'])\n",
    "    if hash_value not in unique_hashes:\n",
    "        unique_hashes[hash_value] = True\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Apply the deduplication\n",
    "deduplicated_dataset = concat_train_ds.filter(should_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e7ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'output_text', 'inp_ids', 'out_ids', 'input_length', 'output_length', 'token_length'],\n",
       "    num_rows: 84088\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03805aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████| 84088/84088 [00:16<00:00, 4964.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████| 77421/77421 [00:01<00:00, 76641.38 examples/s]\n",
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████| 84088/84088 [00:17<00:00, 4871.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████| 83706/83706 [00:01<00:00, 60328.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dedup_ds = deduplicated_dataset.filter(lambda ex: ex[\"token_length\"] < 1024)\n",
    "dedup_ds.save_to_disk(\"/home/k/data/qwen_large_mix_dataset_v0_dedup_1024\")\n",
    "\n",
    "dedup_ds = deduplicated_dataset.filter(lambda ex: ex[\"token_length\"] < 1536)\n",
    "dedup_ds.save_to_disk(\"/home/k/data/qwen_large_mix_dataset_v0_dedup_1536\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0821a4c",
   "metadata": {},
   "source": [
    "### ARC-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5cecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ai2_arc\", name=\"ARC-Easy\")\n",
    "arc_e_train_samples = ds['train'].shuffle(42).select(range(25,len(ds['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b71ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choices', 'answerKey'],\n",
       "    num_rows: 2226\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_e_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6af606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arc_example(ex):\n",
    "\t\n",
    "\tchoice_texts = ex['choices']['text']\n",
    "\tchoice_labels = ex['choices']['label']\n",
    "\tchoices_list = [f\"{l}. {c}\" for c,l in zip(ex[\"choices\"][\"text\"], ex['choices']['label'])]\n",
    "\n",
    "\t\n",
    "\tchoices = \"\\n\".join(choices_list)\n",
    "\tquestion = ex[\"question\"] + f\"\\n\\n{choices}\"\n",
    "\n",
    "\tanswer_idx = {l:i for i,l in enumerate(ex['choices']['label'])}[ex['answerKey']]\n",
    "\tanswer = choices_list[answer_idx]\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169772ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68cdfefa6574f34af3ac3ed20bd25dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arc_e_train_ds_processed = (arc_e_train_samples.shuffle(42).select(range(1000)).map(process_arc_example)\n",
    "                            .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cfd89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Living organisms require energy for biological processes. Chemical energy in a plant cell is\n",
      "\n",
      "A. produced in vacuoles.\n",
      "B. converted from solar energy.\n",
      "C. developed by centrioles.\n",
      "D. stored as kinetic energy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "B. converted from solar energy.\n"
     ]
    }
   ],
   "source": [
    "print(arc_e_train_ds_processed[0]['input_text'])\n",
    "print(arc_e_train_ds_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f7853",
   "metadata": {},
   "source": [
    "### ARC-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ai2_arc\", name=\"ARC-Challenge\")\n",
    "arc_c_train_samples = ds['train'].shuffle(42).select(range(25,len(ds['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed049d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choices', 'answerKey'],\n",
       "    num_rows: 1094\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_c_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8047647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5196ee82f94cb9bd1b1c97bba4e7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arc_c_train_ds_processed = (arc_c_train_samples.shuffle(42).select(range(1000)).map(process_arc_example)\n",
    "                            .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc85815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Carbon on Earth is found in both living and nonliving matter. In order for carbon to be continuously available, it must be recycled. Through which process is carbon made available in the atmosphere?\n",
      "\n",
      "A. formation of fossil fuels\n",
      "B. layering of soil\n",
      "C. plant photosynthesis\n",
      "D. forest fires<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "D. forest fires\n"
     ]
    }
   ],
   "source": [
    "print(arc_c_train_ds_processed[0]['input_text'])\n",
    "print(arc_c_train_ds_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb6ff0",
   "metadata": {},
   "source": [
    "### BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca50e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'passage'],\n",
       "    num_rows: 9427\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"boolq\")\n",
    "boolq_train_samples = ds['train']\n",
    "boolq_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_boolq_example(ex):\n",
    "\tquestion = \"Passage: \" + ex[\"passage\"] +\"\\n\\nQuestion: According to the passage, \" + ex[\"question\"] + \"?\"\n",
    "\tanswer = \"Yes\" if ex['answer'] else \"No\"\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c0c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62f7465733444d7b1000d49a6b1990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boolq_train_ds_processed = (boolq_train_samples.shuffle(42).select(range(1000)).map(process_boolq_example)\n",
    "                            .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Passage: Henry Daniel Mills is a fictional character in ABC's television series Once Upon a Time. Henry is the boy Emma Swan gave up to adoption; Regina Mills adopted him. Henry was originally portrayed as a child by Jared S. Gilmore, who won the Young Artist Award for Best Performance in a TV Series -- Leading Young Actor in 2012. For the show's seventh and final season, Andrew J. West later took over the role of Henry as an adult and father to a eight-year-old girl named Lucy, with Gilmore also making three appearances as Henry during the season.\n",
      "\n",
      "Question: According to the passage, did henry die in once upon a time?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "print(boolq_train_ds_processed[0]['input_text'])\n",
    "print(boolq_train_ds_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd475625",
   "metadata": {},
   "source": [
    "### Commonsense QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a8798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
       "    num_rows: 9734\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"commonsense_qa\")\n",
    "commonsenseqa_train_samples = ds['train'].shuffle(42).select(range(7,len(ds['train'])))\n",
    "commonsenseqa_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_commonsenseqa_example(ex):\n",
    "\t\n",
    "\tchoice_texts = ex['choices']['text']\n",
    "\tchoice_labels = ex['choices']['label']\n",
    "\tchoices_list = [f\"{l}. {c}\" for c,l in zip(ex[\"choices\"][\"text\"], ex['choices']['label'])]\n",
    "\n",
    "\t\n",
    "\tchoices = \"\\n\".join(choices_list)\n",
    "\tquestion = ex[\"question\"] + f\"\\n\\n{choices}\"\n",
    "\n",
    "\tanswer_idx = {l:i for i,l in enumerate(ex['choices']['label'])}[ex['answerKey']]\n",
    "\tanswer = choices_list[answer_idx]\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66151515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefb4b01b9554952b725e087d06626ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "commonsenseqa_train_ds_processed = (commonsenseqa_train_samples.shuffle(42).select(range(1000)).map(process_commonsenseqa_example)\n",
    "                                    \t\t\t\t.select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50acdc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "As the counterweight began to rise the elevator began to what into the mine?\n",
      "\n",
      "A. park\n",
      "B. reduce\n",
      "C. descend\n",
      "D. fall\n",
      "E. set<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "C. descend\n"
     ]
    }
   ],
   "source": [
    "print(commonsenseqa_train_ds_processed[0]['input_text'])\n",
    "print(commonsenseqa_train_ds_processed[0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83cfd1c",
   "metadata": {},
   "source": [
    "#### HellaSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c794e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label'],\n",
       "    num_rows: 39905\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"hellaswag\", name=None)\n",
    "hellaswag_train_samples = ds['train']\n",
    "hellaswag_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f7653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ind': 4,\n",
       " 'activity_label': 'Removing ice from car',\n",
       " 'ctx_a': 'Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles.',\n",
       " 'ctx_b': 'then',\n",
       " 'ctx': 'Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then',\n",
       " 'endings': [', the man adds wax to the windshield and cuts it.',\n",
       "  ', a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.',\n",
       "  ', the man puts on a christmas coat, knitted with netting.',\n",
       "  ', the man continues removing the snow on his car.'],\n",
       " 'source_id': 'activitynet~v_-1IBHYS3L-Y',\n",
       " 'split': 'train',\n",
       " 'split_type': 'indomain',\n",
       " 'label': '3'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellaswag_train_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess_hellaswag(text):\n",
    "\ttext = text.strip()\n",
    "\t# NOTE: Brackets are artifacts of the WikiHow dataset portion of HellaSwag.\n",
    "\ttext = text.replace(\" [title]\", \". \")\n",
    "\ttext = re.sub(\"\\\\[.*?\\\\]\", \"\", text)\n",
    "\ttext = text.replace(\"  \", \" \")\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hellaswag_example(ex):\n",
    "\t\n",
    "\tctx = ex[\"ctx_a\"] + \" \" + ex[\"ctx_b\"].capitalize()\n",
    "\tquery = preprocess_hellaswag(ex['activity_label'] + \". \" + ctx)\n",
    "\tchoices = [preprocess_hellaswag(ending) for ending in ex['endings']]\n",
    "\tchoices_list = [f\"{l}. {c}\" for c, l in zip(choices, string.ascii_uppercase)]\n",
    " \n",
    "\tchoices = \"\\n\".join(choices_list)\n",
    "\tquestion = query + f\"\\n\\n{choices}\"\n",
    "\n",
    "\tanswer = choices_list[int(ex['label'])]\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a09a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b6a8e8ace74d0bb386c4a63dabf47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hellaswag_train_ds_processed = (hellaswag_train_samples.shuffle(42).select(range(1000)).map(process_hellaswag_example)\n",
    "                                    \t\t\t\t.select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a980b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Health. How to choose superfoods good for your heart. Eat dark, leafy greens. Kale is a green that can be used in a variety of ways and paired with almost any food. Dark, leafy greens like swiss chard or collard, mustard, and turnip greens are superfoods that are good for your heart.\n",
      "\n",
      "A. However, those with a higher cholesterol content may use some other vegetables. One ingredient that has a higher cholesterol content is nuts, seeds, chickpeas, and fruit.\n",
      "B. You can eat these greens whole whole, cut in half, or feed them whole. Leafy greens tend to be full of fiber and can calm the stomach, so try to eat steamed greens cooked whole.\n",
      "C. They are full of antioxidants and have anti-inflammatory properties, which helps promote heart health. Use kale and other dark leafy greens to make salads or side dishes.\n",
      "D. Fruit is an excellent nutrient for your heart, as you don't have to eat a ton of it. Broccoli, green beans, cucumber, carrots, and lemon are good choices.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "C. They are full of antioxidants and have anti-inflammatory properties, which helps promote heart health. Use kale and other dark leafy greens to make salads or side dishes.\n"
     ]
    }
   ],
   "source": [
    "print(hellaswag_train_ds_processed[4]['input_text'])\n",
    "print(hellaswag_train_ds_processed[4]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843af3da",
   "metadata": {},
   "source": [
    "#### Winogrande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf58c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'option1', 'option2', 'answer'],\n",
       "    num_rows: 40393\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"winogrande\", name=\"winogrande_xl\")\n",
    "winogrande_train_samples = ds['train'].shuffle(42).select(range(5,len(ds['train'])))\n",
    "winogrande_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11efc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_winogrande_example(ex):\n",
    "\tidx = ex['sentence'].index(\"_\")\n",
    "\tquery = ex['sentence'][:idx].strip()\n",
    "\tremaining = ex['sentence'][idx+1:].strip()\n",
    "\n",
    "\tchoices = [ex['option1'] + \" \" + remaining, ex['option2'] + \" \" + remaining]\n",
    "\tchoices_list = [f\"{l}. {c}\" for c, l in zip(choices, string.ascii_uppercase)]\n",
    "\tchoices = \"\\n\".join(choices_list)\n",
    "\n",
    "\tquestion = f\"Sentence: {query}\\n\\nWhich ones is the most likely continuation?\\n\\n{choices}\"\n",
    "\t\n",
    "\tanswer = choices_list[int(ex['answer'])-1]\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83aa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf59047218747ff855a7460a7f31392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wino_train_ds_processed = (winogrande_train_samples.shuffle(42).select(range(1000)).map(process_winogrande_example)\n",
    "                           .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad51da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Sentence: Paying the electricity bill is an adult responsibility for Joseph but not for Jeffrey because\n",
      "\n",
      "Which ones is the most likely continuation?\n",
      "\n",
      "A. Joseph lives on his own.\n",
      "B. Jeffrey lives on his own.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "A. Joseph lives on his own.\n"
     ]
    }
   ],
   "source": [
    "print(wino_train_ds_processed[1]['input_text'])\n",
    "print(wino_train_ds_processed[1]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951f5a2",
   "metadata": {},
   "source": [
    "#### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babf58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7465\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"gsm8k\", name=\"main\")\n",
    "gsm8k_train_samples = ds['train'].shuffle(42).select(range(8, len(ds['train'])))\t\n",
    "gsm8k_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gsm8k_example(ex):\n",
    "\tquestion = ex['question']\n",
    "\tanswer = ex['answer']\n",
    "\t_,input_text = create_chat_text(tokenizer, question)\n",
    "\treturn {\"input_text\":input_text, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766891d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_train_ds_processed = (gsm8k_train_samples.shuffle(42).select(range(1000)).map(process_gsm8k_example)\n",
    "                           .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84b6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Each member of Greg’s softball team needs to buy one uniform made up of a shirt, a pair of pants, and socks. A shirt costs $7.50, a pair of pants cost $15, and socks cost $4.50 each if each team member buys the uniform items on their own. If they buy the items as a group, they are given a discount. A discounted shirt cost $6.75, a discounted pair of pants cost $13.50, and discounted socks cost $3.75. How much would their team of 12 save with the group discount?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "The cost of a regularly-priced uniform is $7.50 for the shirt + $15 for pants + $4.50 for socks = $<<7.5+15+4.5=27>>27.\n",
      "The cost of a discounted uniform is $6.75 for the shirt + $13.50 for pants + $3.75 for socks = $<<6.75+13.5+3.75=24>>24.\n",
      "By purchasing the discounted uniform, each team member would save $27 for full price - $24 for discounted= $<<27-24=3>>3.\n",
      "So, for the team of 12, they would save $3 * 12 = $<<3*12=36>>36\n",
      "#### 36\n"
     ]
    }
   ],
   "source": [
    "print(gsm8k_train_ds_processed[1]['input_text'])\n",
    "print(gsm8k_train_ds_processed[1]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dd210",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "human eval, mmlu, mmlu-pro, bbhard, agieval don't have train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db774912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only test\n",
    "# ds = load_dataset(\"openai_humaneval\")\n",
    "# ds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472070dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU_SUBJECTS = [\n",
    "# \t\"abstract_algebra\",\n",
    "# \t\"anatomy\",\n",
    "# \t\"astronomy\",\n",
    "# \t\"business_ethics\",\n",
    "# \t\"clinical_knowledge\",\n",
    "# \t\"college_biology\",\n",
    "# \t\"college_chemistry\",\n",
    "# \t\"college_computer_science\",\n",
    "# \t\"college_mathematics\",\n",
    "# \t\"college_medicine\",\n",
    "# \t\"college_physics\",\n",
    "# \t\"computer_security\",\n",
    "# \t\"conceptual_physics\",\n",
    "# \t\"econometrics\",\n",
    "# \t\"electrical_engineering\",\n",
    "# \t\"elementary_mathematics\",\n",
    "# \t\"formal_logic\",\n",
    "# \t\"global_facts\",\n",
    "# \t\"high_school_biology\",\n",
    "# \t\"high_school_chemistry\",\n",
    "# \t\"high_school_computer_science\",\n",
    "# \t\"high_school_european_history\",\n",
    "# \t\"high_school_geography\",\n",
    "# \t\"high_school_government_and_politics\",\n",
    "# \t\"high_school_macroeconomics\",\n",
    "# \t\"high_school_mathematics\",\n",
    "# \t\"high_school_microeconomics\",\n",
    "# \t\"high_school_physics\",\n",
    "# \t\"high_school_psychology\",\n",
    "# \t\"high_school_statistics\",\n",
    "# \t\"high_school_us_history\",\n",
    "# \t\"high_school_world_history\",\n",
    "# \t\"human_aging\",\n",
    "# \t\"human_sexuality\",\n",
    "# \t\"international_law\",\n",
    "# \t\"jurisprudence\",\n",
    "# \t\"logical_fallacies\",\n",
    "# \t\"machine_learning\",\n",
    "# \t\"management\",\n",
    "# \t\"marketing\",\n",
    "# \t\"medical_genetics\",\n",
    "# \t\"miscellaneous\",\n",
    "# \t\"moral_disputes\",\n",
    "# \t\"moral_scenarios\",\n",
    "# \t\"nutrition\",\n",
    "# \t\"philosophy\",\n",
    "# \t\"prehistory\",\n",
    "# \t\"professional_accounting\",\n",
    "# \t\"professional_law\",\n",
    "# \t\"professional_medicine\",\n",
    "# \t\"professional_psychology\",\n",
    "# \t\"public_relations\",\n",
    "# \t\"security_studies\",\n",
    "# \t\"sociology\",\n",
    "# \t\"us_foreign_policy\",\n",
    "# \t\"virology\",\n",
    "# \t\"world_religions\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"cais/mmlu\", name=MMLU_SUBJECTS[0])\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5518f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
    "# MMLU_PRO_SUBJECTS = set(ds['validation']['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24625f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGIEVAL_DATASETS = [\"dmayhem93/agieval-aqua-rat\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-gaokao-english\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-logiqa-en\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-lsat-ar\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-lsat-lr\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-lsat-rc\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-sat-en-without-passage\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-sat-en\",\n",
    "# \t\t\t\t\t\"dmayhem93/agieval-sat-math\",\n",
    "# \t\t\t\t\t\"hails/agieval-math\"\n",
    "# ]\n",
    "\n",
    "# ds = load_dataset(AGIEVAL_DATASETS[0])\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f83122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIGBENCH_HARD_TASK_NAMES =  ['boolean_expressions', \n",
    "# \t\t\t\t\t\t'causal_judgement', \n",
    "# \t\t\t\t\t\t'date_understanding', \n",
    "# \t\t\t\t\t\t'disambiguation_qa', \n",
    "# \t\t\t\t\t\t'dyck_languages',\n",
    "# \t\t\t\t\t\t'formal_fallacies',\n",
    "# \t\t\t\t\t\t'geometric_shapes',\n",
    "# \t\t\t\t\t\t'hyperbaton',\n",
    "# \t\t\t\t\t\t'logical_deduction_three_objects',\n",
    "# \t\t\t\t\t\t'logical_deduction_five_objects',\n",
    "# \t\t\t\t\t\t'logical_deduction_seven_objects',\n",
    "# \t\t\t\t\t\t'movie_recommendation',\n",
    "# \t\t\t\t\t\t'multistep_arithmetic_two',\n",
    "# \t\t\t\t\t\t'navigate',\n",
    "# \t\t\t\t\t\t'object_counting',\n",
    "# \t\t\t\t\t\t'penguins_in_a_table',\n",
    "# \t\t\t\t\t\t'reasoning_about_colored_objects',\n",
    "# \t\t\t\t\t\t'ruin_names',\n",
    "# \t\t\t\t\t\t'salient_translation_error_detection',\n",
    "# \t\t\t\t\t\t'snarks', \n",
    "# \t\t\t\t\t\t'sports_understanding', \n",
    "# \t\t\t\t\t\t'temporal_sequences',\n",
    "# \t\t\t\t\t\t'tracking_shuffled_objects_three_objects',\t\t\t\t\t\n",
    "# \t\t\t\t\t\t'tracking_shuffled_objects_five_objects',\n",
    "# \t\t\t\t\t\t'tracking_shuffled_objects_seven_objects',\n",
    "# \t\t\t\t\t\t'web_of_lies',\n",
    "# \t\t\t\t\t\t'word_sorting']\n",
    "\n",
    "# ds = load_dataset(\"maveriq/bigbenchhard\", name=BIGBENCH_HARD_TASK_NAMES[0])\n",
    "# ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c58e6",
   "metadata": {},
   "source": [
    "#### SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c39c7-7d59-42e5-82fd-68a304ef216e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 130319\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"rajpurkar/squad_v2\")\n",
    "squad_train_samples = ds['train']\n",
    "squad_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_squad_example(ex):\n",
    "\tcontext = ex['context']\n",
    "\tquestion =ex['question']\n",
    "\tquestion = f\"Context: {context}\" + \"\\n\\n\" + f\"Question: {question}\"\n",
    "\tanswer = ex['answers']['text']\n",
    "\tif answer:\n",
    "\t\tanswer = answer[0].capitalize()\n",
    "\telse:\n",
    "\t\tanswer = 'No answer can be found in the context.'\n",
    " \n",
    "\treturn {\"input_text\":question, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf82ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b79dee3bb28416c8b26eea4f362a035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad_train_ds_processed = (squad_train_samples.shuffle(42).select(range(1000)).map(process_squad_example)\n",
    "                           .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8717e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Alexandria was the most important trade center in the whole empire during Athanasius's boyhood. Intellectually, morally, and politically—it epitomized the ethnically diverse Graeco-Roman world, even more than Rome or Constantinople, Antioch or Marseilles. Its famous catechetical school, while sacrificing none of its famous passion for orthodoxy since the days of Pantaenus, Clement of Alexandria, Origen of Alexandria, Dionysius and Theognostus, had begun to take on an almost secular character in the comprehensiveness of its interests, and had counted influential pagans among its serious auditors.\n",
      "\n",
      "Question: What was Alexandria known for?\n",
      "Important trade center\n"
     ]
    }
   ],
   "source": [
    "print(squad_train_ds_processed[1]['input_text'])\n",
    "print(squad_train_ds_processed[1]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bd2ac",
   "metadata": {},
   "source": [
    "### DROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20a544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['section_id', 'query_id', 'passage', 'question', 'answers_spans'],\n",
       "    num_rows: 77400\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"ucinlp/drop\")\n",
    "drop_train_samples = ds['train']\n",
    "drop_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_drop_example(ex):\n",
    "\tcontext = ex['passage']\n",
    "\tquestion = ex['question']\n",
    "\tquestion = f\"Context: {context}\" + \"\\n\\n\" + f\"Question: {question}\"\n",
    "\tanswer = ex['answers_spans']['spans']\n",
    "\tif answer:\n",
    "\t\tanswer = answer[0].capitalize()\n",
    "\telse:\n",
    "\t\tanswer = 'No answer can be found in the context.'\n",
    " \n",
    "\treturn {\"input_text\":question, \"output_text\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb39504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train_ds_processed = (drop_train_samples.shuffle(42).select(range(1000)).map(process_drop_example)\n",
    "                           .select_columns(['input_text', 'output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da119588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: There are 1,068,573 households in the municipality, giving an average household size of 3.3 people. Of those households, 78.4% are in formal structures (houses or apartment), while 20.5% are in informal structures (Shanty town). 94.0% of households use mains electricity for lighting. 87.3% of households have water supply to the dwelling, while 12.0% have piped water through a communal tap. 94.9% of households have regular refuse collection service. 91.4% of households have a flush toilet or chemical toilet, while 4.5% still use a bucket toilet. 82.1% of households have a refrigerator, 87.3% have a television and 70.1% have a radio. Only 34.0% have a landline telephone, but 91.3% have a cellphone. 37.9% have a computer, and 49.3% have access to the Internet (either through a computer or a cellphone).\n",
      "\n",
      "Question: How many more percent of residents lived in formal structures than informal structures?\n",
      "57.9\n"
     ]
    }
   ],
   "source": [
    "print(drop_train_ds_processed[4]['input_text'])\n",
    "print(drop_train_ds_processed[4]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd12ca",
   "metadata": {},
   "source": [
    "### Create Dataset v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_ds = concatenate_datasets([deduplicated_dataset.select_columns(['input_text', 'output_text']), \n",
    "                                        arc_e_train_ds_processed,\n",
    "                                        arc_c_train_ds_processed,\n",
    "                                        boolq_train_ds_processed,\n",
    "                                        commonsenseqa_train_ds_processed,\n",
    "                                        hellaswag_train_ds_processed,\n",
    "                                        wino_train_ds_processed,\n",
    "                                        gsm8k_train_ds_processed,\n",
    "                                        squad_train_ds_processed,\n",
    "                                        drop_train_ds_processed]).shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dff72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07066ecb4cd447c9bf374756f6b00d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad64715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'output_text'],\n",
       "    num_rows: 93078\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(exs):\n",
    "    return {\"input_token_ids\" : tokenizer(exs['input_text'])['input_ids'],\n",
    "            \"output_token_ids\" : tokenizer(exs['output_text'])['input_ids']}\n",
    "    \n",
    "def token_lenghts(ex):\n",
    "    return {\"token_length\": len(ex['input_token_ids']) + len(ex['output_token_ids'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef931c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149e8841e14c43209a954e8364dd2d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a248a8694b84d62a1bfa118230639b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_ds = concat_train_ds.map(tokenize, batched=True)\n",
    "concat_train_ds = concat_train_ds.map(token_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527219e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3327/790154069.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(concat_train_ds['token_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.001, 512.0]     0.699199\n",
       "(512.0, 1024.0]     0.925278\n",
       "(1024.0, 1536.0]    0.995885\n",
       "(1536.0, 2048.0]    0.999194\n",
       "(2048.0, 4096.0]    0.999914\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(concat_train_ds['token_length'], normalize=True, bins=[0,512,1024,1536,2048,4096]).sort_index().cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c5a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca2b8fe8d5f496e915f2125488892b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_filtered_ds = concat_train_ds.filter(lambda ex: ex[\"token_length\"] < 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08bfa5b169f492985481640a11b5eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/86090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_filtered_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v1_1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3181c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ce33f940e64e63b8697c05b7d515b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/93078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_filtered_ds = concat_train_ds.filter(lambda ex: ex[\"token_length\"] < 1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3165a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8340a7c9ce834e608c67a18d1f96b4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/92693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_train_filtered_ds.save_to_disk(\"/workspace/data/llama_large_mix_dataset_v1_1536\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0227556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac24b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09663a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82425f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafdd892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869c096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bce9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
