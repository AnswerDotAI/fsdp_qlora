{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0954a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f56a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4af62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME\n",
    "from accelerate import init_empty_weights\n",
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2FlashAttention2, Qwen2SdpaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import HQQLinear, HQQBackend, BaseQuantizeConfig\n",
    "from quant_utils import replace_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b27f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/k/git/transformers/src/transformers/__init__.py', '4.46.0.dev0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__file__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb252b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KV fp8 quantization.\n",
    "# self.k_scale = config.get(\"k_scale\", None)\n",
    "# self.v_scale = config.get(\"v_scale\", None)\n",
    "\n",
    "# # Cross Layer Attention (CLA).\n",
    "# self.compute_new_kv = config.get(\"compute_new_kv\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94792607",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Linear(128,64,bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a436d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-32B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(model_name)\n",
    "cfg._attn_implementation = \"eager\"\n",
    "cfg.num_hidden_layers = 4\n",
    "cfg.hidden_size //= 8\n",
    "cfg.intermediate_size //= 8\n",
    "cfg.num_attention_heads //= 2\n",
    "cfg.num_key_value_heads //= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8752d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"_name_or_path\": \"Qwen/Qwen2.5-32B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"Qwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151645,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 640,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3456,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 70,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 20,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.46.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 152064\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3399842",
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_empty_weights(): model = AutoModelForCausalLM.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a78482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 640)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (k_proj): Linear(in_features=640, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=3456, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=3456, bias=False)\n",
       "          (down_proj): Linear(in_features=3456, out_features=640, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg._attn_implementation = \"sdpa\"\n",
    "cfg.use_fp8_kv_scale = True\n",
    "cfg.cla_kv_cache_map = {0:0, 1:1, 2:1, 3:0}\n",
    "cfg.palu_kv_compression_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee73730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KV fp8 quantization is enabled.\n",
      "Cross Layer Attention (CLA) is enabled.\n"
     ]
    }
   ],
   "source": [
    "with init_empty_weights(): model = AutoModelForCausalLM.from_config(cfg); model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498bda8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_scale torch.Size([])\n",
      "v_scale torch.Size([])\n",
      "q_proj.weight torch.Size([640, 640])\n",
      "q_proj.bias torch.Size([640])\n",
      "k_proj.weight torch.Size([128, 640])\n",
      "k_proj.bias torch.Size([128])\n",
      "v_proj.weight torch.Size([128, 640])\n",
      "v_proj.bias torch.Size([128])\n",
      "o_proj.weight torch.Size([640, 640])\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.model.layers[0].self_attn.named_parameters(): print(n, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29734412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/git/transformers/src/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cfg._attn_implementation = \"eager\"\n",
    "cfg.use_fp8_kv_scale = True\n",
    "cfg.cla_kv_cache_map = {0:0, 1:1, 2:1, 3:0}\n",
    "cfg.palu_kv_compression_enabled = False\n",
    "cfg.use_cache = False\n",
    "cfg.debug_kv_sharing = True\n",
    "cfg.output_attentions = True\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(cfg)\n",
    "model.to(device=\"cuda\", dtype=torch.bfloat16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.layers[0].self_attn.k_scale, model.model.layers[0].self_attn.v_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e725cb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, {0: 0, 1: 1, 2: 1, 3: 0}, False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_fp8_kv_scale, model.config.cla_kv_cache_map, model.config.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n,p in model.model.layers[0].self_attn.named_parameters(): print(n, p.shape, p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf0eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(32, device=\"cuda\").view(1,-1); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_eager = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7011d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6798814",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg._attn_implementation == \"eager\":\n",
    "    assert torch.equal(out_eager.attentions[0], out_eager.attentions[3])\n",
    "    assert torch.equal(out_eager.attentions[1], out_eager.attentions[2])\n",
    "    assert not torch.equal(out_eager.attentions[0], out_eager.attentions[1])\n",
    "    \n",
    "attn_outputs = [l.self_attn.debug_cla_attn_output for l in model.model.layers]\n",
    "assert len(attn_outputs) == 4\n",
    "assert torch.equal(attn_outputs[0], attn_outputs[3])\n",
    "assert torch.equal(attn_outputs[1], attn_outputs[2])\n",
    "assert not torch.equal(attn_outputs[0], attn_outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7717afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg._attn_implementation = \"sdpa\"\n",
    "cfg.output_attentions = False\n",
    "model = AutoModelForCausalLM.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device=\"cuda\", dtype=torch.bfloat16);\n",
    "model.load_state_dict(model_state_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483f597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sdpa = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb0328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 6.0156e-01, -3.9551e-02,  3.9648e-01,  ...,  2.4316e-01,\n",
       "            2.6172e-01,  3.6328e-01],\n",
       "          [ 8.2031e-01, -9.2773e-02,  7.2754e-02,  ...,  9.3359e-01,\n",
       "           -1.4551e-01,  4.7852e-01],\n",
       "          [ 5.2002e-02, -1.1475e-01,  2.6953e-01,  ...,  7.3047e-01,\n",
       "           -1.2061e-01,  2.8516e-01],\n",
       "          ...,\n",
       "          [-6.1512e-05, -2.0898e-01,  8.9062e-01,  ...,  2.9883e-01,\n",
       "           -2.5195e-01, -6.5234e-01],\n",
       "          [ 3.9307e-02, -3.9453e-01,  6.5625e-01,  ...,  6.1719e-01,\n",
       "           -5.0000e-01, -9.2188e-01],\n",
       "          [-1.2305e-01,  1.9434e-01,  4.4922e-01,  ...,  6.0938e-01,\n",
       "           -1.9434e-01, -1.0078e+00]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[[ 0.6016, -0.0396,  0.3965,  ...,  0.2432,  0.2617,  0.3633],\n",
       "          [ 0.8242, -0.0962,  0.0830,  ...,  0.9297, -0.1270,  0.4883],\n",
       "          [ 0.0569, -0.1094,  0.2754,  ...,  0.7305, -0.1099,  0.2988],\n",
       "          ...,\n",
       "          [-0.0019, -0.2090,  0.8906,  ...,  0.3066, -0.2490, -0.6562],\n",
       "          [ 0.0376, -0.3965,  0.6562,  ...,  0.6211, -0.4902, -0.9219],\n",
       "          [-0.1245,  0.1973,  0.4473,  ...,  0.6211, -0.1963, -0.9961]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_eager.logits, out_sdpa.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c16ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_outputs = [l.self_attn.debug_cla_attn_output for l in model.model.layers]\n",
    "assert len(attn_outputs) == 4\n",
    "assert torch.equal(attn_outputs[0], attn_outputs[3])\n",
    "assert torch.equal(attn_outputs[1], attn_outputs[2])\n",
    "assert not torch.equal(attn_outputs[0], attn_outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c550113",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isclose(out_eager.logits, out_sdpa.logits, rtol=0.1, atol=0.1).float().mean().item() > 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, attn_outputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg._attn_implementation = \"sdpa\"\n",
    "cfg.use_fp8_kv_scale = False\n",
    "cfg.cla_kv_cache_map = {0:0, 1:1, 2:2, 3:3}\n",
    "cfg.palu_kv_compression_enabled = False\n",
    "cfg.use_cache = False\n",
    "cfg.debug_kv_sharing = False\n",
    "cfg.output_attentions = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(cfg)\n",
    "model.to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "model_state_dict = model.state_dict()\n",
    "test_output = model(x)\n",
    "\n",
    "cfg.cla_kv_cache_map = None\n",
    "model = AutoModelForCausalLM.from_config(cfg)\n",
    "model.to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "model.load_state_dict(model_state_dict);\n",
    "base_output = model(x)\n",
    "\n",
    "assert torch.equal(test_output.logits, base_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab0fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfd658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d7b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3584d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f17acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfea201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad06939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb50ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp8_quant_dequant(x, scale):\n",
    "    # dtype -> fp8 / fp8 -> dtype\n",
    "    assert not x.isnan().any(), \"key or value states contain NaN before fp8 quantization\"\n",
    "    x_fp8 = (x.to(torch.float32)/scale).to(torch.float8_e4m3fn)\n",
    "    assert not x_fp8.isnan().any(), \"key or value states contain NaN after fp8 quantization\"\n",
    "    x_dequant = (x_fp8.to(torch.float32)*scale).to(x.dtype)\n",
    "    assert not x_dequant.isnan().any(), \"key or value states contain NaN after fp8 dequantization\"\n",
    "    return x_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eeefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai\n",
    "\n",
    "do you have any suggestion for fixing potential NaN values inside `fp8_quant_dequant`. Consider min and max values of `torch.float8_e4m3fn` dtype so that values don't underflow or overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41def19",
   "metadata": {},
   "source": [
    "##### Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167fbfaf",
   "metadata": {},
   "source": [
    "To address potential NaN values in `fp8_quant_dequant`, we should consider the range limitations of `torch.float8_e4m3fn`. Here's an improved version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp8_quant_dequant(x, scale):\n",
    "    assert not x.isnan().any(), \"key or value states contain NaN before fp8 quantization\"\n",
    "    \n",
    "    # Get min/max values for float8_e4m3fn\n",
    "    f8_min, f8_max = torch.finfo(torch.float8_e4m3fn).min, torch.finfo(torch.float8_e4m3fn).max\n",
    "    \n",
    "    # Clamp x/scale to float8 range before conversion\n",
    "    x_fp32 = x.to(torch.float32) / scale\n",
    "    x_fp32_clamped = torch.clamp(x_fp32, f8_min, f8_max)\n",
    "    \n",
    "    x_fp8 = x_fp32_clamped.to(torch.float8_e4m3fn)\n",
    "    assert not x_fp8.isnan().any(), \"key or value states contain NaN after fp8 quantization\"\n",
    "    \n",
    "    x_dequant = (x_fp8.to(torch.float32) * scale).to(x.dtype)\n",
    "    assert not x_dequant.isnan().any(), \"key or value states contain NaN after fp8 dequantization\"\n",
    "    \n",
    "    return x_dequant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa612d",
   "metadata": {},
   "source": [
    "This implementation:\n",
    "\n",
    "1. Converts `x` to float32 and divides by `scale`.\n",
    "2. Clamps values to the float8_e4m3fn range before conversion.\n",
    "3. Converts to float8_e4m3fn, then back to the original dtype.\n",
    "\n",
    "If NaNs still occur, you may need to adjust `scale` to ensure values fall within the float8 range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca68e62",
   "metadata": {},
   "source": [
    "##### User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f8_min, f8_max = torch.finfo(torch.float8_e4m3fn).min, torch.finfo(torch.float8_e4m3fn).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5014188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-448.0, 448.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f8_min, f8_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d8cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020d8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aaa26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model.model.cla_key_value) == 2\n",
    "for k,v in model.model.cla_key_value:\n",
    "    assert k.shape == v.shape\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84e07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.cla_key_value[0][0].isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967141dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1e70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef011d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp8_quant_dequant(x, scale):\n",
    "    # dtype -> fp8 / fp8 -> dtype\n",
    "    x_fp8 = (x.to(torch.float32)/scale).to(torch.float8_e4m3fn)\n",
    "    x_dequant = (x_fp8.to(torch.float32)*scale).to(x.dtype)\n",
    "    return x_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454e699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.0800, -0.1200,  0.0400,  ..., -3.2000,  0.1400,  0.2000],\n",
       "          [-0.8800, -1.6000, -1.6000,  ..., -0.8000,  0.8000, -0.0700],\n",
       "          [-1.2800, -1.2000,  0.0700,  ..., -0.1500,  1.9200, -2.4000],\n",
       "          [-0.2000,  0.8000,  0.2400,  ...,  0.4800, -2.8800, -1.1200],\n",
       "          [ 0.4800, -0.1800, -1.1200,  ...,  0.8000,  0.6000,  1.1200]],\n",
       "\n",
       "         [[-1.7600, -1.7600,  1.7600,  ..., -0.5200,  0.1800, -1.4400],\n",
       "          [-1.2000, -0.6400, -0.1400,  ..., -0.2400, -1.7600,  0.4800],\n",
       "          [-1.9200,  0.5200,  0.1200,  ..., -0.9600, -1.9200,  3.8400],\n",
       "          [-1.2800,  0.3000,  0.0375,  ..., -0.1500,  0.5600, -0.4400],\n",
       "          [-1.6000,  2.5600, -0.6400,  ...,  3.2000, -0.9600, -1.6000]],\n",
       "\n",
       "         [[-2.5600, -0.7200, -1.2000,  ...,  1.6000,  1.7600,  1.0400],\n",
       "          [-1.9200, -1.1200,  2.5600,  ...,  0.4400,  3.2000,  0.8000],\n",
       "          [ 0.4800, -0.4000, -0.7200,  ...,  1.2000, -0.5600,  2.0800],\n",
       "          [ 0.8000, -0.0300,  0.0225,  ...,  0.2600,  1.2000,  0.1300],\n",
       "          [-0.9600,  2.2400,  0.7200,  ..., -2.5600,  0.5200, -1.7600]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0400, -0.7200,  1.9200,  ...,  0.5600, -0.3000, -0.1300],\n",
       "          [ 0.7200, -0.8800, -2.4000,  ...,  1.4400,  2.5600,  1.6000],\n",
       "          [-0.1800,  3.2000,  0.0900,  ...,  4.1600, -0.7200, -0.6400],\n",
       "          [-0.8800,  2.5600,  0.2200,  ...,  0.6400,  1.2800, -0.9600],\n",
       "          [ 0.5200,  0.7200,  0.1100,  ...,  1.2000, -0.5200,  1.2000]],\n",
       "\n",
       "         [[-0.0350,  4.1600,  0.7200,  ...,  1.9200,  0.8800,  2.4000],\n",
       "          [-2.5600,  2.5600,  0.8800,  ..., -0.0250,  0.1400, -0.5200],\n",
       "          [-2.4000, -0.3200,  2.4000,  ..., -0.3600, -0.0225,  0.1400],\n",
       "          [ 4.1600,  0.2200,  1.1200,  ..., -2.0800,  1.6000, -1.9200],\n",
       "          [-1.0400, -0.3200, -0.7200,  ...,  2.0800, -1.9200,  0.1000]],\n",
       "\n",
       "         [[ 0.1000,  1.0400,  1.2800,  ...,  1.6000,  2.4000,  0.8000],\n",
       "          [-2.0800,  1.6000, -0.8800,  ...,  1.2800,  0.4400, -0.8800],\n",
       "          [ 0.1600, -0.2200,  1.2800,  ...,  1.1200,  0.8000,  1.1200],\n",
       "          [-0.9600,  3.2000, -0.8000,  ..., -0.8000, -1.4400, -3.5200],\n",
       "          [-0.3000, -0.7200, -0.0650,  ..., -0.7200, -0.2400, -1.4400]]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3015e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.0800, -0.1200,  0.0400,  ..., -3.2000,  0.1400,  0.2000],\n",
       "          [-0.8800, -1.6000, -1.6000,  ..., -0.8000,  0.8000, -0.0700],\n",
       "          [-1.2800, -1.2000,  0.0700,  ..., -0.1500,  1.9200, -2.4000],\n",
       "          [-0.2000,  0.8000,  0.2400,  ...,  0.4800, -2.8800, -1.1200],\n",
       "          [ 0.4800, -0.1800, -1.1200,  ...,  0.8000,  0.6000,  1.1200]],\n",
       "\n",
       "         [[-1.7600, -1.7600,  1.7600,  ..., -0.5200,  0.1800, -1.4400],\n",
       "          [-1.2000, -0.6400, -0.1400,  ..., -0.2400, -1.7600,  0.4800],\n",
       "          [-1.9200,  0.5200,  0.1200,  ..., -0.9600, -1.9200,  3.8400],\n",
       "          [-1.2800,  0.3000,  0.0375,  ..., -0.1500,  0.5600, -0.4400],\n",
       "          [-1.6000,  2.5600, -0.6400,  ...,  3.2000, -0.9600, -1.6000]],\n",
       "\n",
       "         [[-2.5600, -0.7200, -1.2000,  ...,  1.6000,  1.7600,  1.0400],\n",
       "          [-1.9200, -1.1200,  2.5600,  ...,  0.4400,  3.2000,  0.8000],\n",
       "          [ 0.4800, -0.4000, -0.7200,  ...,  1.2000, -0.5600,  2.0800],\n",
       "          [ 0.8000, -0.0300,  0.0225,  ...,  0.2600,  1.2000,  0.1300],\n",
       "          [-0.9600,  2.2400,  0.7200,  ..., -2.5600,  0.5200, -1.7600]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0400, -0.7200,  1.9200,  ...,  0.5600, -0.3000, -0.1300],\n",
       "          [ 0.7200, -0.8800, -2.4000,  ...,  1.4400,  2.5600,  1.6000],\n",
       "          [-0.1800,  3.2000,  0.0900,  ...,  4.1600, -0.7200, -0.6400],\n",
       "          [-0.8800,  2.5600,  0.2200,  ...,  0.6400,  1.2800, -0.9600],\n",
       "          [ 0.5200,  0.7200,  0.1100,  ...,  1.2000, -0.5200,  1.2000]],\n",
       "\n",
       "         [[-0.0350,  4.1600,  0.7200,  ...,  1.9200,  0.8800,  2.4000],\n",
       "          [-2.5600,  2.5600,  0.8800,  ..., -0.0250,  0.1400, -0.5200],\n",
       "          [-2.4000, -0.3200,  2.4000,  ..., -0.3600, -0.0225,  0.1400],\n",
       "          [ 4.1600,  0.2200,  1.1200,  ..., -2.0800,  1.6000, -1.9200],\n",
       "          [-1.0400, -0.3200, -0.7200,  ...,  2.0800, -1.9200,  0.1000]],\n",
       "\n",
       "         [[ 0.1000,  1.0400,  1.2800,  ...,  1.6000,  2.4000,  0.8000],\n",
       "          [-2.0800,  1.6000, -0.8800,  ...,  1.2800,  0.4400, -0.8800],\n",
       "          [ 0.1600, -0.2200,  1.2800,  ...,  1.1200,  0.8000,  1.1200],\n",
       "          [-0.9600,  3.2000, -0.8000,  ..., -0.8000, -1.4400, -3.5200],\n",
       "          [-0.3000, -0.7200, -0.0650,  ..., -0.7200, -0.2400, -1.4400]]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp8_quant_dequant(k, model.model.layers[0].self_attn.k_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d35fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d9954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10151b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57db3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf8961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa18c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc16ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8595d2df",
   "metadata": {},
   "source": [
    "### bias load and quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31578c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers\n",
      "[]\n",
      "model.layers.0\n",
      "[]\n",
      "model.layers.0.self_attn\n",
      "[]\n",
      "Replacing model.layers.0.self_attn.q_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.0.self_attn.k_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.0.self_attn.v_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.0.self_attn.o_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "model.layers.0.mlp\n",
      "[]\n",
      "Replacing model.layers.0.mlp.gate_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.0.mlp.up_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.0.mlp.down_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "model.layers.1\n",
      "[]\n",
      "model.layers.1.self_attn\n",
      "[]\n",
      "Replacing model.layers.1.self_attn.q_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.1.self_attn.k_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.1.self_attn.v_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.1.self_attn.o_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "model.layers.1.mlp\n",
      "[]\n",
      "Replacing model.layers.1.mlp.gate_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.1.mlp.up_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.1.mlp.down_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "model.layers.2\n",
      "[]\n",
      "model.layers.2.self_attn\n",
      "[]\n",
      "Replacing model.layers.2.self_attn.q_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.2.self_attn.k_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.2.self_attn.v_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.2.self_attn.o_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "model.layers.2.mlp\n",
      "[]\n",
      "Replacing model.layers.2.mlp.gate_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.2.mlp.up_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.2.mlp.down_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "model.layers.3\n",
      "[]\n",
      "model.layers.3.self_attn\n",
      "[]\n",
      "Replacing model.layers.3.self_attn.q_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.3.self_attn.k_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.3.self_attn.v_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.3.self_attn.o_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "model.layers.3.mlp\n",
      "[]\n",
      "Replacing model.layers.3.mlp.gate_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.3.mlp.up_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n",
      "Replacing model.layers.3.mlp.down_proj with <class 'hqq.core.quantize.HQQLinear'> with 4-bit groupsize 128\n"
     ]
    }
   ],
   "source": [
    "quant_config_4bit = BaseQuantizeConfig(nbits=4, group_size=128, quant_zero=False,\n",
    "                                        quant_scale=False, offload_meta=False, view_as_float=True, axis=1)\n",
    "quant_config_2bit = BaseQuantizeConfig(nbits=2, group_size=32, quant_zero=False,\n",
    "                                        quant_scale=False, offload_meta=False, view_as_float=True, axis=1)\n",
    "attn_layers = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "mlp_layers  = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "layers_4bit = attn_layers + mlp_layers\n",
    "layers_2bit = []\n",
    "skip_modules = [\"lm_head\"]\n",
    "block_influence_patterns = []\n",
    "rank = 0\n",
    "compute_dtype = torch.bfloat16\n",
    "model.model = replace_linear(model=model.model, \n",
    "                                linear_replacement=HQQLinear, \n",
    "                                quant_config_4bit=quant_config_4bit, \n",
    "                                quant_config_2bit=quant_config_2bit,\n",
    "                                layers_4bit=layers_4bit, \n",
    "                                layers_2bit=layers_2bit,\n",
    "                                skip_modules=skip_modules,\n",
    "                                block_influence_patterns=block_influence_patterns,\n",
    "                                prefix='model',\n",
    "                                device=rank,\n",
    "                                compute_dtype=compute_dtype, \n",
    "                                del_orig=True, \n",
    "                                initialize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c00a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): HQQLinear(\n",
       "            (linear_layer): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (k_proj): HQQLinear(\n",
       "            (linear_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          )\n",
       "          (v_proj): HQQLinear(\n",
       "            (linear_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          )\n",
       "          (o_proj): HQQLinear(\n",
       "            (linear_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          )\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): HQQLinear(\n",
       "            (linear_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          )\n",
       "          (up_proj): HQQLinear(\n",
       "            (linear_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          )\n",
       "          (down_proj): HQQLinear(\n",
       "            (linear_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = hub.cached_file(model_name, SAFE_WEIGHTS_INDEX_NAME)\n",
    "files, _ = hub.get_checkpoint_shard_files(model_name, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2afb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f58d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_copy = {}\n",
    "for name, param in iter(weights.items()):\n",
    "    bias_param = None\n",
    "    if name.endswith(\".bias\"): continue\n",
    "    if name.endswith(\".weight\"):\n",
    "        bias_name = name.replace(\".weight\", \".bias\")\n",
    "        if bias_name in weights:\n",
    "            bias_param = weights[bias_name]\n",
    "    weights_copy[name] = (param, bias_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc520127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.2.mlp.down_proj.weight torch.Size([5120, 27648]) None\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([27648, 5120]) None\n",
      "model.layers.3.input_layernorm.weight torch.Size([5120]) None\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([5120, 27648]) None\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([27648, 5120]) None\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([27648, 5120]) None\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([5120]) None\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 5120]) torch.Size([1024])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([5120, 5120]) None\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([5120, 5120]) torch.Size([5120])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 5120]) torch.Size([1024])\n",
      "model.layers.4.input_layernorm.weight torch.Size([5120]) None\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([5120, 27648]) None\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([27648, 5120]) None\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([27648, 5120]) None\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([5120]) None\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 5120]) torch.Size([1024])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([5120, 5120]) None\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([5120, 5120]) torch.Size([5120])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 5120]) torch.Size([1024])\n",
      "model.layers.5.input_layernorm.weight torch.Size([5120]) None\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([5120, 27648]) None\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([27648, 5120]) None\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([27648, 5120]) None\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([5120]) None\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 5120]) torch.Size([1024])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([5120, 5120]) None\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([5120, 5120]) torch.Size([5120])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 5120]) torch.Size([1024])\n",
      "model.layers.6.input_layernorm.weight torch.Size([5120]) None\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([27648, 5120]) None\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([5120]) None\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 5120]) torch.Size([1024])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([5120, 5120]) None\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([5120, 5120]) torch.Size([5120])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 5120]) torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for name, (param, bias_param) in weights_copy.items():\n",
    "    print(name, param.shape, bias_param.shape if bias_param is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57e094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0277,  0.0694,  0.0460,  0.0540,  0.0410,  0.0492,  0.0447, -0.0036,\n",
       "        -0.0181,  0.0537,  0.0414,  0.0143,  0.0643, -0.0863, -0.0174,  0.0551,\n",
       "        -0.0195, -0.0147,  0.0242, -0.0850,  0.0644, -0.0038, -0.0637, -0.0624,\n",
       "         0.0111,  0.0059, -0.0147, -0.0552,  0.0768, -0.0064, -0.0505,  0.0064,\n",
       "        -0.0104, -0.0583, -0.0130,  0.0077,  0.0257,  0.0842,  0.0632,  0.0396,\n",
       "        -0.0588, -0.0166,  0.0791, -0.0845, -0.0719, -0.0801, -0.0023, -0.0765,\n",
       "         0.0230, -0.0268, -0.0444,  0.0556,  0.0225, -0.0201, -0.0292,  0.0430,\n",
       "        -0.0149,  0.0852, -0.0632, -0.0257,  0.0270,  0.0235,  0.0110, -0.0479],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Linear(128,64,bias=True).get_parameter(\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce60f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`bias` is not an nn.Parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_quant/lib/python3.10/site-packages/torch/nn/modules/module.py:701\u001b[0m, in \u001b[0;36mModule.get_parameter\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    698\u001b[0m param: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, param_name)\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter):\n\u001b[0;32m--> 701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m param_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` is not an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn.Parameter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m param\n",
      "\u001b[0;31mAttributeError\u001b[0m: `bias` is not an nn.Parameter"
     ]
    }
   ],
   "source": [
    "torch.nn.Linear(128,64,bias=None).get_parameter(\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11ba90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 2., 4., 8.], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.nn.Linear(128,64,bias=None).get_parameter(\"weight\"))(torch.tensor([1.,2.,4.,8.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932bd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7736351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21423216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de804a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98041f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba702ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eaf8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1efd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae43bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907c718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef18f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af26ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b5c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
