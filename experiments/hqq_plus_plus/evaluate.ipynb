{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9eb91b9-f5fc-44ae-8cca-90d243a73884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from fastcore.parallel import parallel\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ab240f-5dfc-4cd6-ab72-310464d11d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0, device='cuda:0'), tensor(0, device='cuda:1'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0).to(0), torch.tensor(0).to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0917d6f5-a6d3-4e34-842e-64931b91b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chat_input(question):\n",
    "\n",
    "    system =  \"\"\"You are an AI assistant, answering multiple choice questions. \n",
    "Only output the letter (A,B,C,D,E,etc..) of the answer and nothing else.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\":system},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c06918-441b-4059-b0b4-7327f7a47ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "137ef2d9-d8d9-40d6-853f-b3c796dd738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUS = torch.cuda.device_count(); NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b645aa8e-95c3-4d13-baf1-16122aeab62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_ds = load_dataset(\"pharaouk/dharma-2\")['dharma_g1i5_shuffled']\n",
    "# set(eval_ds['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6a2693-2371-46e3-8159-336e38fcfca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = load_dataset(\"MixEval/MixEval\", 'MixEval')['free_form']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f2a73be-c365-4338-ac41-f7b1fa2bb5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9',\n",
       " 'problem_type': 'free-form',\n",
       " 'context': 'None',\n",
       " 'prompt': 'In the 1833 Factory Act in Britain what was the minimum age of a child allowed to work in a factory?',\n",
       " 'target': ['Nine years'],\n",
       " 'benchmark_name': 'TriviaQA',\n",
       " 'options': ['None']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f3958c8-8013-4d5f-b4a3-7f59c4912cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_ds = load_dataset(\"MixEval/MixEval\", 'MixEval')['multiple_choice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299b53fc-ccb4-4694-ac83-7f378ce0660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7',\n",
       " 'problem_type': 'multiple-choice',\n",
       " 'context': 'None',\n",
       " 'prompt': 'Which of the following drugs is classified as a stimulant?',\n",
       " 'target': ['1'],\n",
       " 'benchmark_name': 'MMLU',\n",
       " 'options': ['alcohol', 'nicotine', 'heroin', 'phencyclidine']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc65fd0d-99e6-41b9-9df1-d758009c9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIPLE_CHOICE_PROMPT = \"\"\"{context}\n",
    "\n",
    "{prompt}\n",
    "{enumerated_options}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d056092-9069-499f-9bc9-6761cd8b65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2letter = dict(enumerate(\"ABCDEFGH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "451c3d23-1d24-4801-8f2f-8210259d54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ds(example):\n",
    "    if example['context'] == 'None':\n",
    "        example['context'] = ''\n",
    "    example[\"enumerated_options\"] = \"\\n\".join([f\"{num2letter[idx]}) {opt}\" \n",
    "                                           for idx, opt in enumerate(example['options'])])\n",
    "    example[\"output\"] = num2letter[int(example['target'][0])]\n",
    "    example[\"input\"] = MULTIPLE_CHOICE_PROMPT.format_map(example) \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe2c912-d430-4322-adef-e41e92e938f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = eval_ds.map(process_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a3a598-9939-4c69-92c7-02c0da193b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "What is a common use of the mineral graphite?\n",
      "A) to make glass\n",
      "B) as a source of iron\n",
      "C) to make pencil leads\n",
      "D) as a household cleaner\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(eval_ds[14]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d54579-b826-4491-9cce-062571cc540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [convert_to_chat_input(t) for t in eval_ds['input']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b783e36-44af-48d5-a3fc-afa73f307c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an AI assistant, answering multiple choice questions. \n",
      "Only output the letter (A,B,C,D,E,etc..) of the answer and nothing else.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "The book announced an insane world of dehumanization through terror in which the individual was systematically obliterated by an all-powerful elite. Its key phrases‚ÄîBig Brother, doublethink, Newspeak, the Ministry of Peace (devoted to war), the Ministry of Truth (devoted to lies), the Ministry of Love (devoted to torture)‚Äîburned their way at once into the modern consciousness. The passage above discusses\n",
      "A) E.M. Forster's A Passage to India\n",
      "B) Thomas Pynchon's V.\n",
      "C) George Orwell's 1984\n",
      "D) Flannery O'Connor's The Violent Bear It Away\n",
      "\n",
      "Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e63735-095a-43c9-a947-ba34a98c5803",
   "metadata": {},
   "source": [
    "### llama-3-8b-instruct\n",
    "\n",
    "This is the baseline llama chat model.\n",
    "\n",
    "Dharma-2 : `acc:0.58`\n",
    "\n",
    "Mix-Eval (Multiple Choice): `acc:72`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0c132-b998-49d1-b868-56c73f1a45fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-06-13 06:41:10,338\tWARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2024-06-13 06:41:10,339\tWARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 16.15 to 16.\n",
      "2024-06-13 06:41:10,476\tINFO worker.py:1753 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 06:41:11 config.py:623] Defaulting to use mp for distributed inference\n",
      "INFO 06-13 06:41:11 llm_engine.py:161] Initializing an LLM engine (v0.5.0) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m INFO 06-13 06:41:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method init_device: CUDA error: invalid device ordinal\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] , Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226]   File \"/workspace/git/vllm_fork/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226]   File \"/workspace/git/vllm_fork/vllm/worker/worker.py\", line 106, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226]     torch.cuda.set_device(self.device)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 399, in set_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226]     torch._C._cuda_setDevice(device)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] RuntimeError: CUDA error: invalid device ordinal\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m ERROR 06-13 06:41:15 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=34718)\u001b[0;0m INFO 06-13 06:41:22 multiproc_worker_utils.py:237] Worker exiting\n",
      "INFO 06-13 06:41:22 multiproc_worker_utils.py:123] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm = LLM(model=MODEL_NAME, tensor_parallel_size=2, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "24eb59a6-6e2b-4d6f-9d52-36333f725ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2000/2000 [00:45<00:00, 43.61it/s, Generation Speed: 43.61 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, SamplingParams(temperature=0.0, max_tokens=1, stop=[\"<|eot_id|>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c8f33b1a-52b6-4f84-9ba4-506521a093f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fad86cf5-1e6f-4c01-b57c-ac0a22478cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.mean([p==a for p,a in zip(preds, eval_ds['output'])])\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709b3d3-d08c-44df-9fa9-6d37c037d3d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### llama-3-8b-instruct-hqq\n",
    "\n",
    "This is the model with just HQQ quantization.\n",
    "\n",
    "Dharma-2 : `acc:0.6`\n",
    "\n",
    "Mix-Eval (Multiple Choice): `acc:0.6905`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b1b1ec1-6be4-45e0-8418-bd9c11d51522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-12 21:13:35 config.py:187] torchao quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 06-12 21:13:35 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-only-hqq-vllm', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=torchao, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-only-hqq-vllm)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-12 21:13:37 selector.py:37] Using FlashAttention-2 backend.\n",
      "INFO 06-12 21:13:40 model_runner.py:145] Loading model weights took 9.3189 GB\n",
      "INFO 06-12 21:14:10 gpu_executor.py:83] # GPU blocks: 14326, # CPU blocks: 2048\n",
      "INFO 06-12 21:14:11 model_runner.py:818] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-12 21:14:11 model_runner.py:822] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-12 21:15:23 model_runner.py:888] Graph capturing finished in 71 secs.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-only-hqq-vllm\"\n",
    "llm = LLM(model=model_dir, tokenizer=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "            dtype=\"bfloat16\", tensor_parallel_size=NUM_GPUS, enforce_eager=False,\n",
    "            quantization=\"torchao\", gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5ba0f50-f897-4ef5-99f0-5c849b6067ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2000/2000 [01:05<00:00, 30.39it/s, Generation Speed: 30.39 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, SamplingParams(temperature=0.0, max_tokens=1, stop=[\"<|eot_id|>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0817e042-a228-400d-be93-60c763532464",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d9f12ef-20db-44d0-9112-d3e8f7ad9ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6905"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.mean([p==a for p,a in zip(preds, eval_ds['output'])])\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81db1d-42a9-4414-a380-e7eafa99afdb",
   "metadata": {},
   "source": [
    "### llama-3-8b-instruct-hqq-dora\n",
    "\n",
    "This is the model with HQQ quantization and HQQ++ dataset dora finetuning.\n",
    "\n",
    "Dharma-2 : `acc:0.6`\n",
    "\n",
    "Mix-Eval (Multiple Choice): `acc:0.7085`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fdb4c-9c47-435c-a540-3616036f70b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-13 05:30:36 config.py:187] torchao quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 05:30:38,930\tWARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2024-06-13 05:30:38,934\tWARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 16.15 to 16.\n",
      "2024-06-13 05:30:40,050\tINFO worker.py:1753 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 05:30:40 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-qdora-vllm/', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=torchao, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-qdora-vllm/)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m INFO 06-13 05:30:50 selector.py:37] Using FlashAttention-2 backend.\n",
      "INFO 06-13 05:30:50 selector.py:37] Using FlashAttention-2 backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145] Error executing method init_device. This might cause deadlock in distributed execution.\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145] Traceback (most recent call last):\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145]   File \"/workspace/git/vllm_fork/vllm/worker/worker_base.py\", line 137, in execute_method\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145]     return executor(*args, **kwargs)\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145]   File \"/workspace/git/vllm_fork/vllm/worker/worker.py\", line 105, in init_device\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145]     torch.cuda.set_device(self.device)\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 399, in set_device\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145]     torch._C._cuda_setDevice(device)\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145] RuntimeError: CUDA error: invalid device ordinal\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145] For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[36m(RayWorkerWrapper pid=10366)\u001b[0m ERROR 06-13 05:30:51 worker_base.py:145] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 05:30:56,716\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RayWorkerWrapper.execute_method()\u001b[39m (pid=10366, ip=192.168.226.2, actor_id=7f3cd4cb789611ca5a2e2c1101000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7f0e84e65960>)\n",
      "  File \"/workspace/git/vllm_fork/vllm/worker/worker_base.py\", line 146, in execute_method\n",
      "    raise e\n",
      "  File \"/workspace/git/vllm_fork/vllm/worker/worker_base.py\", line 137, in execute_method\n",
      "    return executor(*args, **kwargs)\n",
      "  File \"/workspace/git/vllm_fork/vllm/worker/worker.py\", line 105, in init_device\n",
      "    torch.cuda.set_device(self.device)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 399, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-qdora-vllm/\"\n",
    "llm = LLM(model=model_dir, tokenizer=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "            dtype=\"bfloat16\", tensor_parallel_size=NUM_GPUS, enforce_eager=False,\n",
    "            quantization=\"torchao\", gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6800342f-3d03-4496-88db-39b09c0984af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2000/2000 [01:16<00:00, 25.98it/s, Generation Speed: 25.98 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, SamplingParams(temperature=0.0, max_tokens=1, stop=[\"<|eot_id|>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bccda1e-d385-4a86-9630-0fdbea880cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aa3d3be-73e5-44ad-a2d3-de6b19daa2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7085"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.mean([p==a for p,a in zip(preds, eval_ds['output'])])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f4c2148-cc4c-4c8b-b9d7-bdd488d41b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_text(tokenizer, user_prompt, system=\"You are a helpful assistant.\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return messages, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760b426-e3a1-467c-bf05-e16883b033a0",
   "metadata": {},
   "source": [
    "### benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1662e4fe-f40c-4a9d-b345-b4d009647f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b641e6e7-6858-4fb4-8e96-dc2017f5c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TOKENS = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6eccc58-d97a-4ea7-b065-8c0429f18fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps = \"hello world!\\n\" * 8000\n",
    "dummy_input = f\"Repeat this indefinitely: {reps}\"\n",
    "_,long_prompt = create_chat_text(tokenizer, dummy_input)\n",
    "truncated_tokens = tokenizer.encode(long_prompt, add_special_tokens=False)[:INPUT_TOKENS-1]\n",
    "prompt = tokenizer.decode(truncated_tokens)\n",
    "assert len(tokenizer.encode(prompt)) == INPUT_TOKENS\n",
    "len(tokenizer.encode(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bdf5c-083a-4b90-859a-3368052c85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.llm_engine.model_executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e7a72919-9f15-49de-9a1d-69b7d0eb757b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6144"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f3887e3e-f6b5-4ab7-8759-720706f7615e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 32\n",
      "1024 64\n",
      "1024 128\n",
      "1024 256\n",
      "1024 512\n",
      "1024 1024\n",
      "1024 2048\n",
      "1024 4096\n",
      "2048 32\n",
      "2048 64\n",
      "2048 128\n",
      "2048 256\n",
      "2048 512\n",
      "2048 1024\n",
      "2048 2048\n",
      "2048 4096\n",
      "4096 32\n",
      "4096 64\n",
      "4096 128\n",
      "4096 256\n",
      "4096 512\n",
      "4096 1024\n",
      "4096 2048\n",
      "6144 32\n",
      "6144 64\n",
      "6144 128\n",
      "6144 256\n",
      "6144 512\n",
      "6144 1024\n",
      "1024 32\n",
      "1024 64\n",
      "1024 128\n",
      "1024 256\n",
      "1024 512\n",
      "1024 1024\n",
      "1024 2048\n",
      "1024 4096\n",
      "2048 32\n",
      "2048 64\n",
      "2048 128\n",
      "2048 256\n",
      "2048 512\n",
      "2048 1024\n",
      "2048 2048\n",
      "2048 4096\n",
      "4096 32\n",
      "4096 64\n",
      "4096 128\n",
      "4096 256\n",
      "4096 512\n",
      "4096 1024\n",
      "4096 2048\n",
      "6144 32\n",
      "6144 64\n",
      "6144 128\n",
      "6144 256\n",
      "6144 512\n",
      "6144 1024\n",
      "1024 32\n",
      "1024 64\n",
      "1024 128\n",
      "1024 256\n",
      "1024 512\n",
      "1024 1024\n",
      "1024 2048\n",
      "1024 4096\n",
      "2048 32\n",
      "2048 64\n",
      "2048 128\n",
      "2048 256\n",
      "2048 512\n",
      "2048 1024\n",
      "2048 2048\n",
      "2048 4096\n",
      "4096 32\n",
      "4096 64\n",
      "4096 128\n",
      "4096 256\n",
      "4096 512\n",
      "4096 1024\n",
      "4096 2048\n",
      "6144 32\n",
      "6144 64\n",
      "6144 128\n",
      "6144 256\n",
      "6144 512\n",
      "6144 1024\n",
      "1024 32\n",
      "1024 64\n",
      "1024 128\n",
      "1024 256\n",
      "1024 512\n",
      "1024 1024\n",
      "1024 2048\n",
      "1024 4096\n",
      "2048 32\n",
      "2048 64\n",
      "2048 128\n",
      "2048 256\n",
      "2048 512\n",
      "2048 1024\n",
      "2048 2048\n",
      "2048 4096\n",
      "4096 32\n",
      "4096 64\n",
      "4096 128\n",
      "4096 256\n",
      "4096 512\n",
      "4096 1024\n",
      "4096 2048\n",
      "6144 32\n",
      "6144 64\n",
      "6144 128\n",
      "6144 256\n",
      "6144 512\n",
      "6144 1024\n"
     ]
    }
   ],
   "source": [
    "for BS in [1, 4, 8, 16]:\n",
    "    for INPUT_TOKENS in [1024, 2048, 4096, 6144]:\n",
    "        for MAX_TOKENS in [32, 64, 128, 256, 512, 1024, 2048, 4096]:\n",
    "            if INPUT_TOKENS + MAX_TOKENS < 8192:\n",
    "                print(INPUT_TOKENS, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59d06e93-341d-40df-90a3-a546c145260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ee72a46-89fc-4493-b851-8327043c6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b6027af2-669f-4648-b4e3-666651a99f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [00:15<00:00,  1.06it/s, Generation Speed: 1.06 toks/s]\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "output = llm.generate([prompt]*BS, SamplingParams(temperature=0.0, max_tokens=1))\n",
    "e = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d44e5df0-07c5-4cce-9ded-c78bf947cd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.223310470581055"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttft = e - s; ttft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d0737f00-7abf-48b1-a991-7ffa9bf641e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [00:58<00:00,  3.63s/it, Generation Speed: 281.83 toks/s]\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "output = llm.generate([prompt]*BS, SamplingParams(temperature=0.0, max_tokens=MAX_TOKENS))\n",
    "e = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "15be8381-c643-4739-978e-4c9319b7c9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.26063251495361"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_time = e - s; completion_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0713db4-f086-4e58-9e60-68f5754ebc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.793301984362092"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_tok_per_sec = MAX_TOKENS / (completion_time - ttft); tok_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "918019ea-2d55-4976-ad00-6ab764e04acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380.69283174979347"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tput_tok_per_sec = BS*MAX_TOKENS / (completion_time - ttft); tok_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0dde86-de99-4294-b27f-62691e3ae73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.read_csv(\"/workspace/git/kerem_research/inference_benchmarking/baseline.csv\")\n",
    "qdora_df    = pd.read_csv(\"/workspace/git/kerem_research/inference_benchmarking/qdora.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca76543d-a018-486b-b955-bf41d47af00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84, 7), (84, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df.shape, qdora_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "148483ce-b9d0-470d-9401-dfbc76bd934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BS</th>\n",
       "      <th>INPUT_TOKENS</th>\n",
       "      <th>MAX_TOKENS</th>\n",
       "      <th>time_to_first_token_baseline</th>\n",
       "      <th>completion_time_baseline</th>\n",
       "      <th>lat_tok_per_sec_baseline</th>\n",
       "      <th>tput_tok_per_sec_baseline</th>\n",
       "      <th>time_to_first_token_qdora</th>\n",
       "      <th>completion_time_qdora</th>\n",
       "      <th>lat_tok_per_sec_qdora</th>\n",
       "      <th>tput_tok_per_sec_qdora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.150467</td>\n",
       "      <td>3.934720</td>\n",
       "      <td>33.824378</td>\n",
       "      <td>33.824378</td>\n",
       "      <td>0.281821</td>\n",
       "      <td>1.981092</td>\n",
       "      <td>75.326379</td>\n",
       "      <td>75.326379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.150341</td>\n",
       "      <td>7.747869</td>\n",
       "      <td>33.695169</td>\n",
       "      <td>33.695169</td>\n",
       "      <td>0.282049</td>\n",
       "      <td>3.690753</td>\n",
       "      <td>75.101866</td>\n",
       "      <td>75.101866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>0.150956</td>\n",
       "      <td>15.378392</td>\n",
       "      <td>33.623521</td>\n",
       "      <td>33.623521</td>\n",
       "      <td>0.282219</td>\n",
       "      <td>7.106683</td>\n",
       "      <td>75.024201</td>\n",
       "      <td>75.024201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.152311</td>\n",
       "      <td>30.735919</td>\n",
       "      <td>33.481988</td>\n",
       "      <td>33.481988</td>\n",
       "      <td>0.283652</td>\n",
       "      <td>14.015289</td>\n",
       "      <td>74.572318</td>\n",
       "      <td>74.572318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.151235</td>\n",
       "      <td>61.438495</td>\n",
       "      <td>33.416406</td>\n",
       "      <td>33.416406</td>\n",
       "      <td>0.283724</td>\n",
       "      <td>27.714202</td>\n",
       "      <td>74.661475</td>\n",
       "      <td>74.661475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BS  INPUT_TOKENS  MAX_TOKENS  time_to_first_token_baseline  \\\n",
       "0   1          1024         128                      0.150467   \n",
       "1   1          1024         256                      0.150341   \n",
       "2   1          1024         512                      0.150956   \n",
       "3   1          1024        1024                      0.152311   \n",
       "4   1          1024        2048                      0.151235   \n",
       "\n",
       "   completion_time_baseline  lat_tok_per_sec_baseline  \\\n",
       "0                  3.934720                 33.824378   \n",
       "1                  7.747869                 33.695169   \n",
       "2                 15.378392                 33.623521   \n",
       "3                 30.735919                 33.481988   \n",
       "4                 61.438495                 33.416406   \n",
       "\n",
       "   tput_tok_per_sec_baseline  time_to_first_token_qdora  \\\n",
       "0                  33.824378                   0.281821   \n",
       "1                  33.695169                   0.282049   \n",
       "2                  33.623521                   0.282219   \n",
       "3                  33.481988                   0.283652   \n",
       "4                  33.416406                   0.283724   \n",
       "\n",
       "   completion_time_qdora  lat_tok_per_sec_qdora  tput_tok_per_sec_qdora  \n",
       "0               1.981092              75.326379               75.326379  \n",
       "1               3.690753              75.101866               75.101866  \n",
       "2               7.106683              75.024201               75.024201  \n",
       "3              14.015289              74.572318               74.572318  \n",
       "4              27.714202              74.661475               74.661475  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_df = baseline_df.merge(qdora_df, on=[\"BS\", \"INPUT_TOKENS\", \"MAX_TOKENS\"], how=\"inner\", suffixes=[\"_baseline\", \"_qdora\"]); bench_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86f0956b-ab7a-4339-b0b7-45614d069f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BS</th>\n",
       "      <th>INPUT_TOKENS</th>\n",
       "      <th>MAX_TOKENS</th>\n",
       "      <th>time_to_first_token_baseline</th>\n",
       "      <th>completion_time_baseline</th>\n",
       "      <th>lat_tok_per_sec_baseline</th>\n",
       "      <th>tput_tok_per_sec_baseline</th>\n",
       "      <th>time_to_first_token_qdora</th>\n",
       "      <th>completion_time_qdora</th>\n",
       "      <th>lat_tok_per_sec_qdora</th>\n",
       "      <th>tput_tok_per_sec_qdora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [BS, INPUT_TOKENS, MAX_TOKENS, time_to_first_token_baseline, completion_time_baseline, lat_tok_per_sec_baseline, tput_tok_per_sec_baseline, time_to_first_token_qdora, completion_time_qdora, lat_tok_per_sec_qdora, tput_tok_per_sec_qdora]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_df.query(\"lat_tok_per_sec_qdora < lat_tok_per_sec_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35202fae-6e98-4128-9d95-ed84486d9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_df[\"improvement\"] = bench_df[\"lat_tok_per_sec_qdora\"] / bench_df[\"lat_tok_per_sec_baseline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4d7aef6-7a21-48d0-ab65-af87d45996ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_df = bench_df.sort_values(\"improvement\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e383e865-f319-463a-a504-48e7ff064098",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62f68e8d-fe06-4955-ae63-e1f847ae216c",
   "metadata": {},
   "source": [
    "for BS in [1, 4, 8, 16]:\n",
    "    for INPUT_TOKENS in [1024, 2048, 4096, 6144]:\n",
    "        for MAX_TOKENS in [128, 256, 512, 1024, 2048, 4096]:\n",
    "            if INPUT_TOKENS + MAX_TOKENS < 8192:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2473d859-d826-44c8-beee-2d89f4b087f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7897560673956545, 1.7478800401891452)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(bench_df[\"improvement\"]), np.mean(bench_df[\"improvement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4284fac6-3e0e-4af1-aaa1-b1ff8bc3dd4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BS</th>\n",
       "      <th>INPUT_TOKENS</th>\n",
       "      <th>MAX_TOKENS</th>\n",
       "      <th>time_to_first_token_baseline</th>\n",
       "      <th>completion_time_baseline</th>\n",
       "      <th>lat_tok_per_sec_baseline</th>\n",
       "      <th>tput_tok_per_sec_baseline</th>\n",
       "      <th>time_to_first_token_qdora</th>\n",
       "      <th>completion_time_qdora</th>\n",
       "      <th>lat_tok_per_sec_qdora</th>\n",
       "      <th>tput_tok_per_sec_qdora</th>\n",
       "      <th>improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.151235</td>\n",
       "      <td>61.438495</td>\n",
       "      <td>33.416406</td>\n",
       "      <td>33.416406</td>\n",
       "      <td>0.283724</td>\n",
       "      <td>27.714202</td>\n",
       "      <td>74.661475</td>\n",
       "      <td>74.661475</td>\n",
       "      <td>2.234276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>0.150956</td>\n",
       "      <td>15.378392</td>\n",
       "      <td>33.623521</td>\n",
       "      <td>33.623521</td>\n",
       "      <td>0.282219</td>\n",
       "      <td>7.106683</td>\n",
       "      <td>75.024201</td>\n",
       "      <td>75.024201</td>\n",
       "      <td>2.231301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.150341</td>\n",
       "      <td>7.747869</td>\n",
       "      <td>33.695169</td>\n",
       "      <td>33.695169</td>\n",
       "      <td>0.282049</td>\n",
       "      <td>3.690753</td>\n",
       "      <td>75.101866</td>\n",
       "      <td>75.101866</td>\n",
       "      <td>2.228862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>0.294153</td>\n",
       "      <td>15.557348</td>\n",
       "      <td>33.544748</td>\n",
       "      <td>33.544748</td>\n",
       "      <td>0.532325</td>\n",
       "      <td>7.384617</td>\n",
       "      <td>74.719524</td>\n",
       "      <td>74.719524</td>\n",
       "      <td>2.227458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.152311</td>\n",
       "      <td>30.735919</td>\n",
       "      <td>33.481988</td>\n",
       "      <td>33.481988</td>\n",
       "      <td>0.283652</td>\n",
       "      <td>14.015289</td>\n",
       "      <td>74.572318</td>\n",
       "      <td>74.572318</td>\n",
       "      <td>2.227237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.150467</td>\n",
       "      <td>3.934720</td>\n",
       "      <td>33.824378</td>\n",
       "      <td>33.824378</td>\n",
       "      <td>0.281821</td>\n",
       "      <td>1.981092</td>\n",
       "      <td>75.326379</td>\n",
       "      <td>75.326379</td>\n",
       "      <td>2.226985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>0.294467</td>\n",
       "      <td>7.909505</td>\n",
       "      <td>33.617691</td>\n",
       "      <td>33.617691</td>\n",
       "      <td>0.533354</td>\n",
       "      <td>3.953269</td>\n",
       "      <td>74.855647</td>\n",
       "      <td>74.855647</td>\n",
       "      <td>2.226674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>128</td>\n",
       "      <td>0.296854</td>\n",
       "      <td>4.089521</td>\n",
       "      <td>33.749343</td>\n",
       "      <td>33.749343</td>\n",
       "      <td>0.532804</td>\n",
       "      <td>2.237246</td>\n",
       "      <td>75.097869</td>\n",
       "      <td>75.097869</td>\n",
       "      <td>2.225165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.295098</td>\n",
       "      <td>30.906995</td>\n",
       "      <td>33.451047</td>\n",
       "      <td>33.451047</td>\n",
       "      <td>0.532820</td>\n",
       "      <td>14.298145</td>\n",
       "      <td>74.389817</td>\n",
       "      <td>74.389817</td>\n",
       "      <td>2.223841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.155924</td>\n",
       "      <td>123.292990</td>\n",
       "      <td>33.263745</td>\n",
       "      <td>33.263745</td>\n",
       "      <td>0.283646</td>\n",
       "      <td>55.872633</td>\n",
       "      <td>73.683659</td>\n",
       "      <td>73.683659</td>\n",
       "      <td>2.215134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.294306</td>\n",
       "      <td>61.710004</td>\n",
       "      <td>33.346523</td>\n",
       "      <td>33.346523</td>\n",
       "      <td>0.533553</td>\n",
       "      <td>28.259704</td>\n",
       "      <td>73.865284</td>\n",
       "      <td>73.865284</td>\n",
       "      <td>2.215082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>128</td>\n",
       "      <td>0.600184</td>\n",
       "      <td>4.438230</td>\n",
       "      <td>33.350303</td>\n",
       "      <td>33.350303</td>\n",
       "      <td>1.020622</td>\n",
       "      <td>2.764956</td>\n",
       "      <td>73.380453</td>\n",
       "      <td>73.380453</td>\n",
       "      <td>2.200293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>0.599594</td>\n",
       "      <td>8.302945</td>\n",
       "      <td>33.232292</td>\n",
       "      <td>33.232292</td>\n",
       "      <td>1.020459</td>\n",
       "      <td>4.523858</td>\n",
       "      <td>73.071886</td>\n",
       "      <td>73.071886</td>\n",
       "      <td>2.198822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.295717</td>\n",
       "      <td>123.958034</td>\n",
       "      <td>33.122459</td>\n",
       "      <td>33.122459</td>\n",
       "      <td>0.533522</td>\n",
       "      <td>56.803754</td>\n",
       "      <td>72.791597</td>\n",
       "      <td>72.791597</td>\n",
       "      <td>2.197651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>0.598475</td>\n",
       "      <td>16.038460</td>\n",
       "      <td>33.160652</td>\n",
       "      <td>33.160652</td>\n",
       "      <td>1.021476</td>\n",
       "      <td>8.049688</td>\n",
       "      <td>72.849253</td>\n",
       "      <td>72.849253</td>\n",
       "      <td>2.196858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.600451</td>\n",
       "      <td>31.558232</td>\n",
       "      <td>33.077305</td>\n",
       "      <td>33.077305</td>\n",
       "      <td>1.021129</td>\n",
       "      <td>15.147839</td>\n",
       "      <td>72.486800</td>\n",
       "      <td>72.486800</td>\n",
       "      <td>2.191436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.598568</td>\n",
       "      <td>63.054200</td>\n",
       "      <td>32.791278</td>\n",
       "      <td>32.791278</td>\n",
       "      <td>1.022439</td>\n",
       "      <td>29.549512</td>\n",
       "      <td>71.791454</td>\n",
       "      <td>71.791454</td>\n",
       "      <td>2.189346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>6144</td>\n",
       "      <td>256</td>\n",
       "      <td>0.927650</td>\n",
       "      <td>8.748637</td>\n",
       "      <td>32.732443</td>\n",
       "      <td>32.732443</td>\n",
       "      <td>1.520687</td>\n",
       "      <td>5.135771</td>\n",
       "      <td>70.814391</td>\n",
       "      <td>70.814391</td>\n",
       "      <td>2.163431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>6144</td>\n",
       "      <td>128</td>\n",
       "      <td>0.927250</td>\n",
       "      <td>4.821047</td>\n",
       "      <td>32.872796</td>\n",
       "      <td>32.872796</td>\n",
       "      <td>1.521368</td>\n",
       "      <td>3.321487</td>\n",
       "      <td>71.106404</td>\n",
       "      <td>71.106404</td>\n",
       "      <td>2.163077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>6144</td>\n",
       "      <td>512</td>\n",
       "      <td>0.927906</td>\n",
       "      <td>16.606921</td>\n",
       "      <td>32.655111</td>\n",
       "      <td>32.655111</td>\n",
       "      <td>1.521712</td>\n",
       "      <td>8.778147</td>\n",
       "      <td>70.558069</td>\n",
       "      <td>70.558069</td>\n",
       "      <td>2.160705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>6144</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.927576</td>\n",
       "      <td>32.524577</td>\n",
       "      <td>32.408139</td>\n",
       "      <td>32.408139</td>\n",
       "      <td>1.520655</td>\n",
       "      <td>16.279424</td>\n",
       "      <td>69.382481</td>\n",
       "      <td>69.382481</td>\n",
       "      <td>2.140897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.567265</td>\n",
       "      <td>4.631953</td>\n",
       "      <td>31.490732</td>\n",
       "      <td>125.962929</td>\n",
       "      <td>0.989104</td>\n",
       "      <td>3.066840</td>\n",
       "      <td>61.605513</td>\n",
       "      <td>246.422051</td>\n",
       "      <td>1.956306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.568095</td>\n",
       "      <td>8.730037</td>\n",
       "      <td>31.365084</td>\n",
       "      <td>125.460338</td>\n",
       "      <td>0.988858</td>\n",
       "      <td>5.176135</td>\n",
       "      <td>61.137593</td>\n",
       "      <td>244.550374</td>\n",
       "      <td>1.949225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>0.567570</td>\n",
       "      <td>16.980463</td>\n",
       "      <td>31.194988</td>\n",
       "      <td>124.779952</td>\n",
       "      <td>0.988290</td>\n",
       "      <td>9.436454</td>\n",
       "      <td>60.604888</td>\n",
       "      <td>242.419550</td>\n",
       "      <td>1.942776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>1.128719</td>\n",
       "      <td>5.429803</td>\n",
       "      <td>29.759939</td>\n",
       "      <td>238.079514</td>\n",
       "      <td>1.897721</td>\n",
       "      <td>4.133304</td>\n",
       "      <td>57.255763</td>\n",
       "      <td>458.046102</td>\n",
       "      <td>1.923921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>1.128871</td>\n",
       "      <td>9.790628</td>\n",
       "      <td>29.555205</td>\n",
       "      <td>236.441637</td>\n",
       "      <td>1.899848</td>\n",
       "      <td>6.417923</td>\n",
       "      <td>56.661308</td>\n",
       "      <td>453.290468</td>\n",
       "      <td>1.917135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>1.126603</td>\n",
       "      <td>18.553968</td>\n",
       "      <td>29.379083</td>\n",
       "      <td>235.032662</td>\n",
       "      <td>1.898502</td>\n",
       "      <td>11.020588</td>\n",
       "      <td>56.127511</td>\n",
       "      <td>449.020089</td>\n",
       "      <td>1.910458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.567969</td>\n",
       "      <td>68.144612</td>\n",
       "      <td>30.306329</td>\n",
       "      <td>121.225317</td>\n",
       "      <td>0.985778</td>\n",
       "      <td>36.690979</td>\n",
       "      <td>57.358589</td>\n",
       "      <td>229.434356</td>\n",
       "      <td>1.892627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.568206</td>\n",
       "      <td>33.792349</td>\n",
       "      <td>30.820960</td>\n",
       "      <td>123.283840</td>\n",
       "      <td>0.987648</td>\n",
       "      <td>18.642093</td>\n",
       "      <td>58.002391</td>\n",
       "      <td>232.009563</td>\n",
       "      <td>1.881914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>128</td>\n",
       "      <td>1.143729</td>\n",
       "      <td>5.360945</td>\n",
       "      <td>30.351780</td>\n",
       "      <td>121.407122</td>\n",
       "      <td>1.913566</td>\n",
       "      <td>4.157911</td>\n",
       "      <td>57.032231</td>\n",
       "      <td>228.128925</td>\n",
       "      <td>1.879041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>1.142349</td>\n",
       "      <td>9.613678</td>\n",
       "      <td>30.219581</td>\n",
       "      <td>120.878325</td>\n",
       "      <td>1.910499</td>\n",
       "      <td>6.427238</td>\n",
       "      <td>56.678066</td>\n",
       "      <td>226.712266</td>\n",
       "      <td>1.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>1.142193</td>\n",
       "      <td>18.153991</td>\n",
       "      <td>30.096760</td>\n",
       "      <td>120.387039</td>\n",
       "      <td>1.914088</td>\n",
       "      <td>10.988255</td>\n",
       "      <td>56.423910</td>\n",
       "      <td>225.695642</td>\n",
       "      <td>1.874750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.142301</td>\n",
       "      <td>35.466746</td>\n",
       "      <td>29.832966</td>\n",
       "      <td>119.331862</td>\n",
       "      <td>1.912499</td>\n",
       "      <td>20.261988</td>\n",
       "      <td>55.805370</td>\n",
       "      <td>223.221478</td>\n",
       "      <td>1.870594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.126142</td>\n",
       "      <td>36.633079</td>\n",
       "      <td>28.839435</td>\n",
       "      <td>230.715483</td>\n",
       "      <td>1.898799</td>\n",
       "      <td>21.015105</td>\n",
       "      <td>53.566834</td>\n",
       "      <td>428.534675</td>\n",
       "      <td>1.857416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.143042</td>\n",
       "      <td>70.944614</td>\n",
       "      <td>29.340314</td>\n",
       "      <td>117.361254</td>\n",
       "      <td>1.910837</td>\n",
       "      <td>39.749205</td>\n",
       "      <td>54.124956</td>\n",
       "      <td>216.499823</td>\n",
       "      <td>1.844730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>140.183831</td>\n",
       "      <td>29.337817</td>\n",
       "      <td>117.351267</td>\n",
       "      <td>0.989147</td>\n",
       "      <td>76.718642</td>\n",
       "      <td>54.087248</td>\n",
       "      <td>216.348994</td>\n",
       "      <td>1.843602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>8</td>\n",
       "      <td>2048</td>\n",
       "      <td>128</td>\n",
       "      <td>2.299330</td>\n",
       "      <td>6.832695</td>\n",
       "      <td>28.235094</td>\n",
       "      <td>225.880751</td>\n",
       "      <td>3.830372</td>\n",
       "      <td>6.309078</td>\n",
       "      <td>51.639865</td>\n",
       "      <td>413.118919</td>\n",
       "      <td>1.828925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>8</td>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>2.298381</td>\n",
       "      <td>11.424478</td>\n",
       "      <td>28.051421</td>\n",
       "      <td>224.411367</td>\n",
       "      <td>3.831724</td>\n",
       "      <td>8.825861</td>\n",
       "      <td>51.260110</td>\n",
       "      <td>410.080876</td>\n",
       "      <td>1.827362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.126195</td>\n",
       "      <td>74.151658</td>\n",
       "      <td>28.045012</td>\n",
       "      <td>224.360099</td>\n",
       "      <td>1.898573</td>\n",
       "      <td>42.197903</td>\n",
       "      <td>50.819703</td>\n",
       "      <td>406.557620</td>\n",
       "      <td>1.812076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>8</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>2.297208</td>\n",
       "      <td>20.780297</td>\n",
       "      <td>27.700997</td>\n",
       "      <td>221.607977</td>\n",
       "      <td>3.833794</td>\n",
       "      <td>14.039262</td>\n",
       "      <td>50.169183</td>\n",
       "      <td>401.353464</td>\n",
       "      <td>1.811097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>2.378480</td>\n",
       "      <td>11.372052</td>\n",
       "      <td>28.464775</td>\n",
       "      <td>113.859102</td>\n",
       "      <td>3.908746</td>\n",
       "      <td>8.912178</td>\n",
       "      <td>51.164878</td>\n",
       "      <td>204.659511</td>\n",
       "      <td>1.797480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>4096</td>\n",
       "      <td>1.144094</td>\n",
       "      <td>145.027761</td>\n",
       "      <td>28.467443</td>\n",
       "      <td>113.869770</td>\n",
       "      <td>1.917424</td>\n",
       "      <td>82.288971</td>\n",
       "      <td>50.963309</td>\n",
       "      <td>203.853235</td>\n",
       "      <td>1.790231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8</td>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.295995</td>\n",
       "      <td>39.823081</td>\n",
       "      <td>27.286957</td>\n",
       "      <td>218.295656</td>\n",
       "      <td>3.835872</td>\n",
       "      <td>24.809152</td>\n",
       "      <td>48.824027</td>\n",
       "      <td>390.592217</td>\n",
       "      <td>1.789281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>2.375986</td>\n",
       "      <td>20.470209</td>\n",
       "      <td>28.296325</td>\n",
       "      <td>113.185301</td>\n",
       "      <td>3.908420</td>\n",
       "      <td>14.256619</td>\n",
       "      <td>49.477209</td>\n",
       "      <td>197.908836</td>\n",
       "      <td>1.748538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>1.130404</td>\n",
       "      <td>155.709155</td>\n",
       "      <td>26.497821</td>\n",
       "      <td>211.982564</td>\n",
       "      <td>1.898295</td>\n",
       "      <td>90.346413</td>\n",
       "      <td>46.309635</td>\n",
       "      <td>370.477077</td>\n",
       "      <td>1.747677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>2.293705</td>\n",
       "      <td>79.442334</td>\n",
       "      <td>26.546162</td>\n",
       "      <td>212.369294</td>\n",
       "      <td>3.836867</td>\n",
       "      <td>48.200913</td>\n",
       "      <td>46.163508</td>\n",
       "      <td>369.308063</td>\n",
       "      <td>1.738990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.379347</td>\n",
       "      <td>38.912859</td>\n",
       "      <td>28.029060</td>\n",
       "      <td>112.116238</td>\n",
       "      <td>3.920191</td>\n",
       "      <td>25.167717</td>\n",
       "      <td>48.193845</td>\n",
       "      <td>192.775379</td>\n",
       "      <td>1.719424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>6144</td>\n",
       "      <td>128</td>\n",
       "      <td>3.737674</td>\n",
       "      <td>8.443079</td>\n",
       "      <td>27.202761</td>\n",
       "      <td>108.811043</td>\n",
       "      <td>6.138360</td>\n",
       "      <td>8.890021</td>\n",
       "      <td>46.517357</td>\n",
       "      <td>186.069428</td>\n",
       "      <td>1.710023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>6144</td>\n",
       "      <td>256</td>\n",
       "      <td>3.742183</td>\n",
       "      <td>13.206501</td>\n",
       "      <td>27.048964</td>\n",
       "      <td>108.195857</td>\n",
       "      <td>6.099636</td>\n",
       "      <td>11.639060</td>\n",
       "      <td>46.214192</td>\n",
       "      <td>184.856767</td>\n",
       "      <td>1.708538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>4096</td>\n",
       "      <td>2048</td>\n",
       "      <td>2.375921</td>\n",
       "      <td>76.342976</td>\n",
       "      <td>27.688002</td>\n",
       "      <td>110.752010</td>\n",
       "      <td>3.922945</td>\n",
       "      <td>47.251142</td>\n",
       "      <td>47.267142</td>\n",
       "      <td>189.068566</td>\n",
       "      <td>1.707134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>4096</td>\n",
       "      <td>128</td>\n",
       "      <td>2.626781</td>\n",
       "      <td>6.860531</td>\n",
       "      <td>30.233241</td>\n",
       "      <td>120.932962</td>\n",
       "      <td>3.909913</td>\n",
       "      <td>6.392488</td>\n",
       "      <td>51.559380</td>\n",
       "      <td>206.237520</td>\n",
       "      <td>1.705387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>6144</td>\n",
       "      <td>512</td>\n",
       "      <td>3.743457</td>\n",
       "      <td>22.747483</td>\n",
       "      <td>26.941659</td>\n",
       "      <td>107.766638</td>\n",
       "      <td>6.097214</td>\n",
       "      <td>17.241894</td>\n",
       "      <td>45.941201</td>\n",
       "      <td>183.764805</td>\n",
       "      <td>1.705211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>6144</td>\n",
       "      <td>1024</td>\n",
       "      <td>3.745980</td>\n",
       "      <td>42.118006</td>\n",
       "      <td>26.686107</td>\n",
       "      <td>106.744428</td>\n",
       "      <td>6.096725</td>\n",
       "      <td>28.745638</td>\n",
       "      <td>45.211881</td>\n",
       "      <td>180.847524</td>\n",
       "      <td>1.694210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8</td>\n",
       "      <td>4096</td>\n",
       "      <td>128</td>\n",
       "      <td>4.774659</td>\n",
       "      <td>9.817583</td>\n",
       "      <td>25.382101</td>\n",
       "      <td>203.056811</td>\n",
       "      <td>7.820500</td>\n",
       "      <td>10.836103</td>\n",
       "      <td>42.445908</td>\n",
       "      <td>339.567264</td>\n",
       "      <td>1.672277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8</td>\n",
       "      <td>2048</td>\n",
       "      <td>4096</td>\n",
       "      <td>2.298978</td>\n",
       "      <td>165.570091</td>\n",
       "      <td>25.087108</td>\n",
       "      <td>200.696862</td>\n",
       "      <td>3.834827</td>\n",
       "      <td>101.614325</td>\n",
       "      <td>41.890172</td>\n",
       "      <td>335.121377</td>\n",
       "      <td>1.669789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8</td>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>4.774695</td>\n",
       "      <td>25.283352</td>\n",
       "      <td>24.965068</td>\n",
       "      <td>199.720540</td>\n",
       "      <td>7.808203</td>\n",
       "      <td>20.206044</td>\n",
       "      <td>41.297511</td>\n",
       "      <td>330.380086</td>\n",
       "      <td>1.654212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>8</td>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.779639</td>\n",
       "      <td>46.583019</td>\n",
       "      <td>24.495627</td>\n",
       "      <td>195.965015</td>\n",
       "      <td>7.820111</td>\n",
       "      <td>33.222277</td>\n",
       "      <td>40.311523</td>\n",
       "      <td>322.492183</td>\n",
       "      <td>1.645662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>8</td>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>4.771705</td>\n",
       "      <td>14.930486</td>\n",
       "      <td>25.199873</td>\n",
       "      <td>201.598985</td>\n",
       "      <td>7.820940</td>\n",
       "      <td>14.055291</td>\n",
       "      <td>41.062811</td>\n",
       "      <td>328.502484</td>\n",
       "      <td>1.629485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>8</td>\n",
       "      <td>4096</td>\n",
       "      <td>2048</td>\n",
       "      <td>4.776110</td>\n",
       "      <td>90.251816</td>\n",
       "      <td>23.960025</td>\n",
       "      <td>191.680197</td>\n",
       "      <td>7.822069</td>\n",
       "      <td>61.106444</td>\n",
       "      <td>38.435283</td>\n",
       "      <td>307.482264</td>\n",
       "      <td>1.604142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>8</td>\n",
       "      <td>6144</td>\n",
       "      <td>256</td>\n",
       "      <td>7.526672</td>\n",
       "      <td>18.725376</td>\n",
       "      <td>22.859789</td>\n",
       "      <td>182.878308</td>\n",
       "      <td>12.318409</td>\n",
       "      <td>19.405489</td>\n",
       "      <td>36.122071</td>\n",
       "      <td>288.976570</td>\n",
       "      <td>1.580158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>8</td>\n",
       "      <td>6144</td>\n",
       "      <td>128</td>\n",
       "      <td>7.522228</td>\n",
       "      <td>13.099854</td>\n",
       "      <td>22.948832</td>\n",
       "      <td>183.590653</td>\n",
       "      <td>12.215244</td>\n",
       "      <td>15.793805</td>\n",
       "      <td>35.768571</td>\n",
       "      <td>286.148571</td>\n",
       "      <td>1.558623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>8</td>\n",
       "      <td>6144</td>\n",
       "      <td>512</td>\n",
       "      <td>7.528436</td>\n",
       "      <td>30.225390</td>\n",
       "      <td>22.558092</td>\n",
       "      <td>180.464739</td>\n",
       "      <td>12.188937</td>\n",
       "      <td>26.826892</td>\n",
       "      <td>34.977564</td>\n",
       "      <td>279.820509</td>\n",
       "      <td>1.550555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>8</td>\n",
       "      <td>6144</td>\n",
       "      <td>1024</td>\n",
       "      <td>7.521700</td>\n",
       "      <td>53.243452</td>\n",
       "      <td>22.396342</td>\n",
       "      <td>179.170737</td>\n",
       "      <td>12.183776</td>\n",
       "      <td>42.059207</td>\n",
       "      <td>34.275657</td>\n",
       "      <td>274.205255</td>\n",
       "      <td>1.530413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>2.267591</td>\n",
       "      <td>7.011758</td>\n",
       "      <td>26.980500</td>\n",
       "      <td>431.687999</td>\n",
       "      <td>3.924520</td>\n",
       "      <td>7.316688</td>\n",
       "      <td>37.733974</td>\n",
       "      <td>603.743590</td>\n",
       "      <td>1.398565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>2.263727</td>\n",
       "      <td>11.868795</td>\n",
       "      <td>26.652596</td>\n",
       "      <td>426.441542</td>\n",
       "      <td>3.796457</td>\n",
       "      <td>10.935636</td>\n",
       "      <td>35.858463</td>\n",
       "      <td>573.735402</td>\n",
       "      <td>1.345402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.259459</td>\n",
       "      <td>42.677385</td>\n",
       "      <td>25.335293</td>\n",
       "      <td>405.364688</td>\n",
       "      <td>3.798591</td>\n",
       "      <td>34.118998</td>\n",
       "      <td>33.772634</td>\n",
       "      <td>540.362141</td>\n",
       "      <td>1.333027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>2.261966</td>\n",
       "      <td>21.761587</td>\n",
       "      <td>26.256922</td>\n",
       "      <td>420.110747</td>\n",
       "      <td>3.796732</td>\n",
       "      <td>18.440409</td>\n",
       "      <td>34.963897</td>\n",
       "      <td>559.422355</td>\n",
       "      <td>1.331607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>2.260545</td>\n",
       "      <td>87.143561</td>\n",
       "      <td>24.127324</td>\n",
       "      <td>386.037177</td>\n",
       "      <td>3.796203</td>\n",
       "      <td>69.326061</td>\n",
       "      <td>31.252929</td>\n",
       "      <td>500.046865</td>\n",
       "      <td>1.295333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>128</td>\n",
       "      <td>4.615591</td>\n",
       "      <td>9.901035</td>\n",
       "      <td>24.217453</td>\n",
       "      <td>387.479254</td>\n",
       "      <td>7.679067</td>\n",
       "      <td>11.789525</td>\n",
       "      <td>31.140081</td>\n",
       "      <td>498.241299</td>\n",
       "      <td>1.285853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>4.610428</td>\n",
       "      <td>26.315743</td>\n",
       "      <td>23.588693</td>\n",
       "      <td>377.419081</td>\n",
       "      <td>7.677866</td>\n",
       "      <td>24.593586</td>\n",
       "      <td>30.267704</td>\n",
       "      <td>484.283259</td>\n",
       "      <td>1.283145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>4.610674</td>\n",
       "      <td>15.284896</td>\n",
       "      <td>23.983013</td>\n",
       "      <td>383.728201</td>\n",
       "      <td>7.670625</td>\n",
       "      <td>15.993818</td>\n",
       "      <td>30.757425</td>\n",
       "      <td>492.118797</td>\n",
       "      <td>1.282467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.610234</td>\n",
       "      <td>49.509676</td>\n",
       "      <td>22.806519</td>\n",
       "      <td>364.904305</td>\n",
       "      <td>7.666734</td>\n",
       "      <td>42.698757</td>\n",
       "      <td>29.230399</td>\n",
       "      <td>467.686383</td>\n",
       "      <td>1.281669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>4.604301</td>\n",
       "      <td>97.589847</td>\n",
       "      <td>22.024929</td>\n",
       "      <td>352.398856</td>\n",
       "      <td>7.683526</td>\n",
       "      <td>82.607861</td>\n",
       "      <td>27.334243</td>\n",
       "      <td>437.347892</td>\n",
       "      <td>1.241059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>16</td>\n",
       "      <td>4096</td>\n",
       "      <td>128</td>\n",
       "      <td>9.556051</td>\n",
       "      <td>15.853818</td>\n",
       "      <td>20.324662</td>\n",
       "      <td>325.194596</td>\n",
       "      <td>15.749274</td>\n",
       "      <td>20.852940</td>\n",
       "      <td>25.080013</td>\n",
       "      <td>401.280210</td>\n",
       "      <td>1.233969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>2.265064</td>\n",
       "      <td>189.931578</td>\n",
       "      <td>21.825950</td>\n",
       "      <td>349.215205</td>\n",
       "      <td>3.794095</td>\n",
       "      <td>157.368099</td>\n",
       "      <td>26.671181</td>\n",
       "      <td>426.738888</td>\n",
       "      <td>1.221994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>4096</td>\n",
       "      <td>4.606565</td>\n",
       "      <td>208.218909</td>\n",
       "      <td>20.116659</td>\n",
       "      <td>321.866538</td>\n",
       "      <td>7.668616</td>\n",
       "      <td>175.929594</td>\n",
       "      <td>24.343137</td>\n",
       "      <td>389.490188</td>\n",
       "      <td>1.210098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>16</td>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>9.557682</td>\n",
       "      <td>22.269969</td>\n",
       "      <td>20.137997</td>\n",
       "      <td>322.207944</td>\n",
       "      <td>15.631569</td>\n",
       "      <td>26.162451</td>\n",
       "      <td>24.309455</td>\n",
       "      <td>388.951281</td>\n",
       "      <td>1.207144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>16</td>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>9.560567</td>\n",
       "      <td>35.429318</td>\n",
       "      <td>19.792219</td>\n",
       "      <td>316.675504</td>\n",
       "      <td>15.624200</td>\n",
       "      <td>37.148754</td>\n",
       "      <td>23.786788</td>\n",
       "      <td>380.588614</td>\n",
       "      <td>1.201825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>16</td>\n",
       "      <td>4096</td>\n",
       "      <td>2048</td>\n",
       "      <td>9.560809</td>\n",
       "      <td>119.510694</td>\n",
       "      <td>18.626668</td>\n",
       "      <td>298.026686</td>\n",
       "      <td>15.761344</td>\n",
       "      <td>108.245550</td>\n",
       "      <td>22.144322</td>\n",
       "      <td>354.309147</td>\n",
       "      <td>1.188850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>16</td>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>9.563496</td>\n",
       "      <td>62.212275</td>\n",
       "      <td>19.449644</td>\n",
       "      <td>311.194302</td>\n",
       "      <td>15.635339</td>\n",
       "      <td>59.925424</td>\n",
       "      <td>23.120299</td>\n",
       "      <td>369.924784</td>\n",
       "      <td>1.188726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>16</td>\n",
       "      <td>6144</td>\n",
       "      <td>128</td>\n",
       "      <td>15.061973</td>\n",
       "      <td>22.547283</td>\n",
       "      <td>17.100160</td>\n",
       "      <td>273.602560</td>\n",
       "      <td>24.367623</td>\n",
       "      <td>30.701054</td>\n",
       "      <td>20.210215</td>\n",
       "      <td>323.363434</td>\n",
       "      <td>1.181873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>16</td>\n",
       "      <td>6144</td>\n",
       "      <td>512</td>\n",
       "      <td>15.052053</td>\n",
       "      <td>45.290788</td>\n",
       "      <td>16.931925</td>\n",
       "      <td>270.910804</td>\n",
       "      <td>24.350447</td>\n",
       "      <td>50.378435</td>\n",
       "      <td>19.671133</td>\n",
       "      <td>314.738120</td>\n",
       "      <td>1.161778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>16</td>\n",
       "      <td>6144</td>\n",
       "      <td>1024</td>\n",
       "      <td>15.054102</td>\n",
       "      <td>76.463945</td>\n",
       "      <td>16.674851</td>\n",
       "      <td>266.797618</td>\n",
       "      <td>24.366424</td>\n",
       "      <td>77.552583</td>\n",
       "      <td>19.253129</td>\n",
       "      <td>308.050071</td>\n",
       "      <td>1.154621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>16</td>\n",
       "      <td>6144</td>\n",
       "      <td>256</td>\n",
       "      <td>15.048780</td>\n",
       "      <td>29.914904</td>\n",
       "      <td>17.220360</td>\n",
       "      <td>275.525767</td>\n",
       "      <td>24.364633</td>\n",
       "      <td>37.275451</td>\n",
       "      <td>19.828333</td>\n",
       "      <td>317.253333</td>\n",
       "      <td>1.151447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BS  INPUT_TOKENS  MAX_TOKENS  time_to_first_token_baseline  \\\n",
       "4    1          1024        2048                      0.151235   \n",
       "2    1          1024         512                      0.150956   \n",
       "1    1          1024         256                      0.150341   \n",
       "8    1          2048         512                      0.294153   \n",
       "3    1          1024        1024                      0.152311   \n",
       "0    1          1024         128                      0.150467   \n",
       "7    1          2048         256                      0.294467   \n",
       "6    1          2048         128                      0.296854   \n",
       "9    1          2048        1024                      0.295098   \n",
       "5    1          1024        4096                      0.155924   \n",
       "10   1          2048        2048                      0.294306   \n",
       "12   1          4096         128                      0.600184   \n",
       "13   1          4096         256                      0.599594   \n",
       "11   1          2048        4096                      0.295717   \n",
       "14   1          4096         512                      0.598475   \n",
       "15   1          4096        1024                      0.600451   \n",
       "16   1          4096        2048                      0.598568   \n",
       "18   1          6144         256                      0.927650   \n",
       "17   1          6144         128                      0.927250   \n",
       "19   1          6144         512                      0.927906   \n",
       "20   1          6144        1024                      0.927576   \n",
       "21   4          1024         128                      0.567265   \n",
       "22   4          1024         256                      0.568095   \n",
       "23   4          1024         512                      0.567570   \n",
       "42   8          1024         128                      1.128719   \n",
       "43   8          1024         256                      1.128871   \n",
       "44   8          1024         512                      1.126603   \n",
       "25   4          1024        2048                      0.567969   \n",
       "24   4          1024        1024                      0.568206   \n",
       "27   4          2048         128                      1.143729   \n",
       "28   4          2048         256                      1.142349   \n",
       "29   4          2048         512                      1.142193   \n",
       "30   4          2048        1024                      1.142301   \n",
       "45   8          1024        1024                      1.126142   \n",
       "31   4          2048        2048                      1.143042   \n",
       "26   4          1024        4096                      0.568807   \n",
       "48   8          2048         128                      2.299330   \n",
       "49   8          2048         256                      2.298381   \n",
       "46   8          1024        2048                      1.126195   \n",
       "50   8          2048         512                      2.297208   \n",
       "34   4          4096         256                      2.378480   \n",
       "32   4          2048        4096                      1.144094   \n",
       "51   8          2048        1024                      2.295995   \n",
       "35   4          4096         512                      2.375986   \n",
       "47   8          1024        4096                      1.130404   \n",
       "52   8          2048        2048                      2.293705   \n",
       "36   4          4096        1024                      2.379347   \n",
       "38   4          6144         128                      3.737674   \n",
       "39   4          6144         256                      3.742183   \n",
       "37   4          4096        2048                      2.375921   \n",
       "33   4          4096         128                      2.626781   \n",
       "40   4          6144         512                      3.743457   \n",
       "41   4          6144        1024                      3.745980   \n",
       "54   8          4096         128                      4.774659   \n",
       "53   8          2048        4096                      2.298978   \n",
       "56   8          4096         512                      4.774695   \n",
       "57   8          4096        1024                      4.779639   \n",
       "55   8          4096         256                      4.771705   \n",
       "58   8          4096        2048                      4.776110   \n",
       "60   8          6144         256                      7.526672   \n",
       "59   8          6144         128                      7.522228   \n",
       "61   8          6144         512                      7.528436   \n",
       "62   8          6144        1024                      7.521700   \n",
       "63  16          1024         128                      2.267591   \n",
       "64  16          1024         256                      2.263727   \n",
       "66  16          1024        1024                      2.259459   \n",
       "65  16          1024         512                      2.261966   \n",
       "67  16          1024        2048                      2.260545   \n",
       "69  16          2048         128                      4.615591   \n",
       "71  16          2048         512                      4.610428   \n",
       "70  16          2048         256                      4.610674   \n",
       "72  16          2048        1024                      4.610234   \n",
       "73  16          2048        2048                      4.604301   \n",
       "75  16          4096         128                      9.556051   \n",
       "68  16          1024        4096                      2.265064   \n",
       "74  16          2048        4096                      4.606565   \n",
       "76  16          4096         256                      9.557682   \n",
       "77  16          4096         512                      9.560567   \n",
       "79  16          4096        2048                      9.560809   \n",
       "78  16          4096        1024                      9.563496   \n",
       "80  16          6144         128                     15.061973   \n",
       "82  16          6144         512                     15.052053   \n",
       "83  16          6144        1024                     15.054102   \n",
       "81  16          6144         256                     15.048780   \n",
       "\n",
       "    completion_time_baseline  lat_tok_per_sec_baseline  \\\n",
       "4                  61.438495                 33.416406   \n",
       "2                  15.378392                 33.623521   \n",
       "1                   7.747869                 33.695169   \n",
       "8                  15.557348                 33.544748   \n",
       "3                  30.735919                 33.481988   \n",
       "0                   3.934720                 33.824378   \n",
       "7                   7.909505                 33.617691   \n",
       "6                   4.089521                 33.749343   \n",
       "9                  30.906995                 33.451047   \n",
       "5                 123.292990                 33.263745   \n",
       "10                 61.710004                 33.346523   \n",
       "12                  4.438230                 33.350303   \n",
       "13                  8.302945                 33.232292   \n",
       "11                123.958034                 33.122459   \n",
       "14                 16.038460                 33.160652   \n",
       "15                 31.558232                 33.077305   \n",
       "16                 63.054200                 32.791278   \n",
       "18                  8.748637                 32.732443   \n",
       "17                  4.821047                 32.872796   \n",
       "19                 16.606921                 32.655111   \n",
       "20                 32.524577                 32.408139   \n",
       "21                  4.631953                 31.490732   \n",
       "22                  8.730037                 31.365084   \n",
       "23                 16.980463                 31.194988   \n",
       "42                  5.429803                 29.759939   \n",
       "43                  9.790628                 29.555205   \n",
       "44                 18.553968                 29.379083   \n",
       "25                 68.144612                 30.306329   \n",
       "24                 33.792349                 30.820960   \n",
       "27                  5.360945                 30.351780   \n",
       "28                  9.613678                 30.219581   \n",
       "29                 18.153991                 30.096760   \n",
       "30                 35.466746                 29.832966   \n",
       "45                 36.633079                 28.839435   \n",
       "31                 70.944614                 29.340314   \n",
       "26                140.183831                 29.337817   \n",
       "48                  6.832695                 28.235094   \n",
       "49                 11.424478                 28.051421   \n",
       "46                 74.151658                 28.045012   \n",
       "50                 20.780297                 27.700997   \n",
       "34                 11.372052                 28.464775   \n",
       "32                145.027761                 28.467443   \n",
       "51                 39.823081                 27.286957   \n",
       "35                 20.470209                 28.296325   \n",
       "47                155.709155                 26.497821   \n",
       "52                 79.442334                 26.546162   \n",
       "36                 38.912859                 28.029060   \n",
       "38                  8.443079                 27.202761   \n",
       "39                 13.206501                 27.048964   \n",
       "37                 76.342976                 27.688002   \n",
       "33                  6.860531                 30.233241   \n",
       "40                 22.747483                 26.941659   \n",
       "41                 42.118006                 26.686107   \n",
       "54                  9.817583                 25.382101   \n",
       "53                165.570091                 25.087108   \n",
       "56                 25.283352                 24.965068   \n",
       "57                 46.583019                 24.495627   \n",
       "55                 14.930486                 25.199873   \n",
       "58                 90.251816                 23.960025   \n",
       "60                 18.725376                 22.859789   \n",
       "59                 13.099854                 22.948832   \n",
       "61                 30.225390                 22.558092   \n",
       "62                 53.243452                 22.396342   \n",
       "63                  7.011758                 26.980500   \n",
       "64                 11.868795                 26.652596   \n",
       "66                 42.677385                 25.335293   \n",
       "65                 21.761587                 26.256922   \n",
       "67                 87.143561                 24.127324   \n",
       "69                  9.901035                 24.217453   \n",
       "71                 26.315743                 23.588693   \n",
       "70                 15.284896                 23.983013   \n",
       "72                 49.509676                 22.806519   \n",
       "73                 97.589847                 22.024929   \n",
       "75                 15.853818                 20.324662   \n",
       "68                189.931578                 21.825950   \n",
       "74                208.218909                 20.116659   \n",
       "76                 22.269969                 20.137997   \n",
       "77                 35.429318                 19.792219   \n",
       "79                119.510694                 18.626668   \n",
       "78                 62.212275                 19.449644   \n",
       "80                 22.547283                 17.100160   \n",
       "82                 45.290788                 16.931925   \n",
       "83                 76.463945                 16.674851   \n",
       "81                 29.914904                 17.220360   \n",
       "\n",
       "    tput_tok_per_sec_baseline  time_to_first_token_qdora  \\\n",
       "4                   33.416406                   0.283724   \n",
       "2                   33.623521                   0.282219   \n",
       "1                   33.695169                   0.282049   \n",
       "8                   33.544748                   0.532325   \n",
       "3                   33.481988                   0.283652   \n",
       "0                   33.824378                   0.281821   \n",
       "7                   33.617691                   0.533354   \n",
       "6                   33.749343                   0.532804   \n",
       "9                   33.451047                   0.532820   \n",
       "5                   33.263745                   0.283646   \n",
       "10                  33.346523                   0.533553   \n",
       "12                  33.350303                   1.020622   \n",
       "13                  33.232292                   1.020459   \n",
       "11                  33.122459                   0.533522   \n",
       "14                  33.160652                   1.021476   \n",
       "15                  33.077305                   1.021129   \n",
       "16                  32.791278                   1.022439   \n",
       "18                  32.732443                   1.520687   \n",
       "17                  32.872796                   1.521368   \n",
       "19                  32.655111                   1.521712   \n",
       "20                  32.408139                   1.520655   \n",
       "21                 125.962929                   0.989104   \n",
       "22                 125.460338                   0.988858   \n",
       "23                 124.779952                   0.988290   \n",
       "42                 238.079514                   1.897721   \n",
       "43                 236.441637                   1.899848   \n",
       "44                 235.032662                   1.898502   \n",
       "25                 121.225317                   0.985778   \n",
       "24                 123.283840                   0.987648   \n",
       "27                 121.407122                   1.913566   \n",
       "28                 120.878325                   1.910499   \n",
       "29                 120.387039                   1.914088   \n",
       "30                 119.331862                   1.912499   \n",
       "45                 230.715483                   1.898799   \n",
       "31                 117.361254                   1.910837   \n",
       "26                 117.351267                   0.989147   \n",
       "48                 225.880751                   3.830372   \n",
       "49                 224.411367                   3.831724   \n",
       "46                 224.360099                   1.898573   \n",
       "50                 221.607977                   3.833794   \n",
       "34                 113.859102                   3.908746   \n",
       "32                 113.869770                   1.917424   \n",
       "51                 218.295656                   3.835872   \n",
       "35                 113.185301                   3.908420   \n",
       "47                 211.982564                   1.898295   \n",
       "52                 212.369294                   3.836867   \n",
       "36                 112.116238                   3.920191   \n",
       "38                 108.811043                   6.138360   \n",
       "39                 108.195857                   6.099636   \n",
       "37                 110.752010                   3.922945   \n",
       "33                 120.932962                   3.909913   \n",
       "40                 107.766638                   6.097214   \n",
       "41                 106.744428                   6.096725   \n",
       "54                 203.056811                   7.820500   \n",
       "53                 200.696862                   3.834827   \n",
       "56                 199.720540                   7.808203   \n",
       "57                 195.965015                   7.820111   \n",
       "55                 201.598985                   7.820940   \n",
       "58                 191.680197                   7.822069   \n",
       "60                 182.878308                  12.318409   \n",
       "59                 183.590653                  12.215244   \n",
       "61                 180.464739                  12.188937   \n",
       "62                 179.170737                  12.183776   \n",
       "63                 431.687999                   3.924520   \n",
       "64                 426.441542                   3.796457   \n",
       "66                 405.364688                   3.798591   \n",
       "65                 420.110747                   3.796732   \n",
       "67                 386.037177                   3.796203   \n",
       "69                 387.479254                   7.679067   \n",
       "71                 377.419081                   7.677866   \n",
       "70                 383.728201                   7.670625   \n",
       "72                 364.904305                   7.666734   \n",
       "73                 352.398856                   7.683526   \n",
       "75                 325.194596                  15.749274   \n",
       "68                 349.215205                   3.794095   \n",
       "74                 321.866538                   7.668616   \n",
       "76                 322.207944                  15.631569   \n",
       "77                 316.675504                  15.624200   \n",
       "79                 298.026686                  15.761344   \n",
       "78                 311.194302                  15.635339   \n",
       "80                 273.602560                  24.367623   \n",
       "82                 270.910804                  24.350447   \n",
       "83                 266.797618                  24.366424   \n",
       "81                 275.525767                  24.364633   \n",
       "\n",
       "    completion_time_qdora  lat_tok_per_sec_qdora  tput_tok_per_sec_qdora  \\\n",
       "4               27.714202              74.661475               74.661475   \n",
       "2                7.106683              75.024201               75.024201   \n",
       "1                3.690753              75.101866               75.101866   \n",
       "8                7.384617              74.719524               74.719524   \n",
       "3               14.015289              74.572318               74.572318   \n",
       "0                1.981092              75.326379               75.326379   \n",
       "7                3.953269              74.855647               74.855647   \n",
       "6                2.237246              75.097869               75.097869   \n",
       "9               14.298145              74.389817               74.389817   \n",
       "5               55.872633              73.683659               73.683659   \n",
       "10              28.259704              73.865284               73.865284   \n",
       "12               2.764956              73.380453               73.380453   \n",
       "13               4.523858              73.071886               73.071886   \n",
       "11              56.803754              72.791597               72.791597   \n",
       "14               8.049688              72.849253               72.849253   \n",
       "15              15.147839              72.486800               72.486800   \n",
       "16              29.549512              71.791454               71.791454   \n",
       "18               5.135771              70.814391               70.814391   \n",
       "17               3.321487              71.106404               71.106404   \n",
       "19               8.778147              70.558069               70.558069   \n",
       "20              16.279424              69.382481               69.382481   \n",
       "21               3.066840              61.605513              246.422051   \n",
       "22               5.176135              61.137593              244.550374   \n",
       "23               9.436454              60.604888              242.419550   \n",
       "42               4.133304              57.255763              458.046102   \n",
       "43               6.417923              56.661308              453.290468   \n",
       "44              11.020588              56.127511              449.020089   \n",
       "25              36.690979              57.358589              229.434356   \n",
       "24              18.642093              58.002391              232.009563   \n",
       "27               4.157911              57.032231              228.128925   \n",
       "28               6.427238              56.678066              226.712266   \n",
       "29              10.988255              56.423910              225.695642   \n",
       "30              20.261988              55.805370              223.221478   \n",
       "45              21.015105              53.566834              428.534675   \n",
       "31              39.749205              54.124956              216.499823   \n",
       "26              76.718642              54.087248              216.348994   \n",
       "48               6.309078              51.639865              413.118919   \n",
       "49               8.825861              51.260110              410.080876   \n",
       "46              42.197903              50.819703              406.557620   \n",
       "50              14.039262              50.169183              401.353464   \n",
       "34               8.912178              51.164878              204.659511   \n",
       "32              82.288971              50.963309              203.853235   \n",
       "51              24.809152              48.824027              390.592217   \n",
       "35              14.256619              49.477209              197.908836   \n",
       "47              90.346413              46.309635              370.477077   \n",
       "52              48.200913              46.163508              369.308063   \n",
       "36              25.167717              48.193845              192.775379   \n",
       "38               8.890021              46.517357              186.069428   \n",
       "39              11.639060              46.214192              184.856767   \n",
       "37              47.251142              47.267142              189.068566   \n",
       "33               6.392488              51.559380              206.237520   \n",
       "40              17.241894              45.941201              183.764805   \n",
       "41              28.745638              45.211881              180.847524   \n",
       "54              10.836103              42.445908              339.567264   \n",
       "53             101.614325              41.890172              335.121377   \n",
       "56              20.206044              41.297511              330.380086   \n",
       "57              33.222277              40.311523              322.492183   \n",
       "55              14.055291              41.062811              328.502484   \n",
       "58              61.106444              38.435283              307.482264   \n",
       "60              19.405489              36.122071              288.976570   \n",
       "59              15.793805              35.768571              286.148571   \n",
       "61              26.826892              34.977564              279.820509   \n",
       "62              42.059207              34.275657              274.205255   \n",
       "63               7.316688              37.733974              603.743590   \n",
       "64              10.935636              35.858463              573.735402   \n",
       "66              34.118998              33.772634              540.362141   \n",
       "65              18.440409              34.963897              559.422355   \n",
       "67              69.326061              31.252929              500.046865   \n",
       "69              11.789525              31.140081              498.241299   \n",
       "71              24.593586              30.267704              484.283259   \n",
       "70              15.993818              30.757425              492.118797   \n",
       "72              42.698757              29.230399              467.686383   \n",
       "73              82.607861              27.334243              437.347892   \n",
       "75              20.852940              25.080013              401.280210   \n",
       "68             157.368099              26.671181              426.738888   \n",
       "74             175.929594              24.343137              389.490188   \n",
       "76              26.162451              24.309455              388.951281   \n",
       "77              37.148754              23.786788              380.588614   \n",
       "79             108.245550              22.144322              354.309147   \n",
       "78              59.925424              23.120299              369.924784   \n",
       "80              30.701054              20.210215              323.363434   \n",
       "82              50.378435              19.671133              314.738120   \n",
       "83              77.552583              19.253129              308.050071   \n",
       "81              37.275451              19.828333              317.253333   \n",
       "\n",
       "    improvement  \n",
       "4      2.234276  \n",
       "2      2.231301  \n",
       "1      2.228862  \n",
       "8      2.227458  \n",
       "3      2.227237  \n",
       "0      2.226985  \n",
       "7      2.226674  \n",
       "6      2.225165  \n",
       "9      2.223841  \n",
       "5      2.215134  \n",
       "10     2.215082  \n",
       "12     2.200293  \n",
       "13     2.198822  \n",
       "11     2.197651  \n",
       "14     2.196858  \n",
       "15     2.191436  \n",
       "16     2.189346  \n",
       "18     2.163431  \n",
       "17     2.163077  \n",
       "19     2.160705  \n",
       "20     2.140897  \n",
       "21     1.956306  \n",
       "22     1.949225  \n",
       "23     1.942776  \n",
       "42     1.923921  \n",
       "43     1.917135  \n",
       "44     1.910458  \n",
       "25     1.892627  \n",
       "24     1.881914  \n",
       "27     1.879041  \n",
       "28     1.875541  \n",
       "29     1.874750  \n",
       "30     1.870594  \n",
       "45     1.857416  \n",
       "31     1.844730  \n",
       "26     1.843602  \n",
       "48     1.828925  \n",
       "49     1.827362  \n",
       "46     1.812076  \n",
       "50     1.811097  \n",
       "34     1.797480  \n",
       "32     1.790231  \n",
       "51     1.789281  \n",
       "35     1.748538  \n",
       "47     1.747677  \n",
       "52     1.738990  \n",
       "36     1.719424  \n",
       "38     1.710023  \n",
       "39     1.708538  \n",
       "37     1.707134  \n",
       "33     1.705387  \n",
       "40     1.705211  \n",
       "41     1.694210  \n",
       "54     1.672277  \n",
       "53     1.669789  \n",
       "56     1.654212  \n",
       "57     1.645662  \n",
       "55     1.629485  \n",
       "58     1.604142  \n",
       "60     1.580158  \n",
       "59     1.558623  \n",
       "61     1.550555  \n",
       "62     1.530413  \n",
       "63     1.398565  \n",
       "64     1.345402  \n",
       "66     1.333027  \n",
       "65     1.331607  \n",
       "67     1.295333  \n",
       "69     1.285853  \n",
       "71     1.283145  \n",
       "70     1.282467  \n",
       "72     1.281669  \n",
       "73     1.241059  \n",
       "75     1.233969  \n",
       "68     1.221994  \n",
       "74     1.210098  \n",
       "76     1.207144  \n",
       "77     1.201825  \n",
       "79     1.188850  \n",
       "78     1.188726  \n",
       "80     1.181873  \n",
       "82     1.161778  \n",
       "83     1.154621  \n",
       "81     1.151447  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da203e2a-9f00-404b-9203-ee5cfaf073e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14151e1d-82da-4701-98be-077c0f86f640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea0a28-eec5-4ae4-af29-f5a4169a8c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502fa69-0fe8-4307-9959-d4d036dd71f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeec635-64a7-4c3e-86ca-d1c016061db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
