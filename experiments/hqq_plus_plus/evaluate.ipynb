{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9eb91b9-f5fc-44ae-8cca-90d243a73884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from fastcore.parallel import parallel\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0917d6f5-a6d3-4e34-842e-64931b91b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chat_input(question):\n",
    "\n",
    "    system =  \"\"\"You are an AI assistant, answering multiple choice questions. \n",
    "Only output the letter (A,B,C,D,E,etc..) of the answer and nothing else.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\":system},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c06918-441b-4059-b0b4-7327f7a47ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137ef2d9-d8d9-40d6-853f-b3c796dd738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUS = torch.cuda.device_count(); NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715c6b32-b6e3-4a6e-a07a-4fd5cb9f2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = load_dataset(\"pharaouk/dharma-2\")['dharma_g1i5_shuffled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0717af3e-d569-4e08-9855-aaba3924a27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(eval_ds['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae5db57-25fd-4770-846b-fe00e248c0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'The following are multiple choice questions (with answers) about  conceptual physics.\\n\\nA magnetic force can act on an electron even when it\\nA. is at rest\\nB. moves parallel to magnetic field lines\\nC. Both of these\\nD. Neither of these\\nAnswer:',\n",
       " 'output': 'D',\n",
       " 'subject': 'MMLU'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fb045c-5d7c-4c54-afc5-d3c259abe5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about  conceptual physics.\n",
      "\n",
      "A magnetic force can act on an electron even when it\n",
      "A. is at rest\n",
      "B. moves parallel to magnetic field lines\n",
      "C. Both of these\n",
      "D. Neither of these\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(eval_ds[11]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d54579-b826-4491-9cce-062571cc540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [convert_to_chat_input(t) for t in eval_ds['input']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e63735-095a-43c9-a947-ba34a98c5803",
   "metadata": {},
   "source": [
    "### llama-3-8b-instruct\n",
    "\n",
    "This is the baseline llama chat model.\n",
    "\n",
    "`acc:0.58`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a0c132-b998-49d1-b868-56c73f1a45fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-10 16:10:48 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-10 16:10:49 selector.py:37] Using FlashAttention-2 backend.\n",
      "INFO 06-10 16:10:50 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-10 16:10:54 model_runner.py:145] Loading model weights took 14.9595 GB\n",
      "INFO 06-10 16:10:56 gpu_executor.py:83] # GPU blocks: 11780, # CPU blocks: 2048\n",
      "INFO 06-10 16:10:58 model_runner.py:818] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-10 16:10:58 model_runner.py:822] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-10 16:11:05 model_runner.py:888] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm = LLM(model=MODEL_NAME, tensor_parallel_size=NUM_GPUS, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24eb59a6-6e2b-4d6f-9d52-36333f725ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 300/300 [00:06<00:00, 44.27it/s, Generation Speed: 44.27 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, SamplingParams(temperature=0.0, max_tokens=1, stop=[\"<|eot_id|>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f33b1a-52b6-4f84-9ba4-506521a093f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad86cf5-1e6f-4c01-b57c-ac0a22478cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.mean([p==a for p,a in zip(preds, eval_ds['output'])])\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709b3d3-d08c-44df-9fa9-6d37c037d3d6",
   "metadata": {},
   "source": [
    "### llama-3-8b-instruct-hqq\n",
    "\n",
    "This is the model with just HQQ quantization.\n",
    "\n",
    "`acc:0.6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b1b1ec1-6be4-45e0-8418-bd9c11d51522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-10 16:00:06 config.py:187] torchao quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 06-10 16:00:06 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-only-hqq-vllm', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=torchao, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-only-hqq-vllm)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-10 16:00:07 selector.py:37] Using FlashAttention-2 backend.\n",
      "INFO 06-10 16:00:12 model_runner.py:145] Loading model weights took 9.3189 GB\n",
      "INFO 06-10 16:00:44 gpu_executor.py:83] # GPU blocks: 14326, # CPU blocks: 2048\n",
      "INFO 06-10 16:00:46 model_runner.py:818] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-10 16:00:46 model_runner.py:822] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-10 16:01:58 model_runner.py:888] Graph capturing finished in 72 secs.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-only-hqq-vllm\"\n",
    "llm = LLM(model=model_dir, tokenizer=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "            dtype=\"bfloat16\", tensor_parallel_size=NUM_GPUS, enforce_eager=False,\n",
    "            quantization=\"torchao\", gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ba0f50-f897-4ef5-99f0-5c849b6067ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 300/300 [00:06<00:00, 44.42it/s, Generation Speed: 44.42 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, SamplingParams(temperature=0.0, max_tokens=1, stop=[\"<|eot_id|>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0817e042-a228-400d-be93-60c763532464",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9f12ef-20db-44d0-9112-d3e8f7ad9ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.mean([p==a for p,a in zip(preds, eval_ds['output'])])\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81db1d-42a9-4414-a380-e7eafa99afdb",
   "metadata": {},
   "source": [
    "### llama-3-8b-instruct-hqq-dora\n",
    "\n",
    "This is the model with HQQ quantization and HQQ++ dataset dora finetuning.\n",
    "\n",
    "`acc:0.6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029570a4-4035-4d6b-a177-b53224be6af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-10 16:05:17 config.py:187] torchao quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 06-10 16:05:18 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-qdora-vllm/', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=torchao, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-qdora-vllm/)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-10 16:05:19 selector.py:37] Using FlashAttention-2 backend.\n",
      "WARNING 06-10 16:05:22 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:23 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:24 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:25 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:26 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:27 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:28 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:29 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:30 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:31 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:32 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:32 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:32 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:32 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:32 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:32 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:32 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:32 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:33 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:34 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:35 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:36 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:414] Loading a weight without `output_dim` attribute in MergedColumnParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:37 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "WARNING 06-10 16:05:38 linear.py:577] Loading a weight without `output_dim` attribute in QKVParallelLinear, assume the weight is the same for all partitions.\n",
      "INFO 06-10 16:05:38 model_runner.py:145] Loading model weights took 9.5822 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] WON'T CONVERT dora_layer /workspace/git/vllm_fork/vllm/model_executor/layers/quantization/torchao.py line 474 \n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] due to: \n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 786, in _convert_frame\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     result = inner_convert(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 400, in _convert_frame_assert\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return _compile(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 676, in _compile\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 535, in compile_inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 165, in _fn\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 500, in transform\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     tracer.run()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2149, in run\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     super().run()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     and self.step()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 971, in compile_subgraph\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1241, in call_user_compiler\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1222, in call_user_compiler\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 117, in debug_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1729, in __call__\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1330, in compile_fx\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return aot_autograd(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 58, in compiler_fn\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 903, in aot_module_simplified\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 119, in aot_dispatch_base\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1257, in fw_compiler_base\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return inner_compile(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 83, in debug_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py\", line 304, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 438, in compile_fx_inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1307, in compile_to_fn\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1254, in compile_to_module\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2160, in load_by_key_path\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/tmp/torchinductor_root/sl/csl4rmmbjvtfi3wrjvnruosynk3jzeufrsuhtx3ude3ixsnxl4yi.py\", line 72, in <module>\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2715, in wait\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2522, in result\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     self.future.result()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return self.__get_result()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     raise self._exception\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] AssertionError: target must be of GPUTarget type\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] \n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] \n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 786, in _convert_frame\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     result = inner_convert(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 400, in _convert_frame_assert\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return _compile(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 676, in _compile\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 535, in compile_inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 165, in _fn\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 500, in transform\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     tracer.run()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2149, in run\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     super().run()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     and self.step()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 971, in compile_subgraph\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1241, in call_user_compiler\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1222, in call_user_compiler\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 117, in debug_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1729, in __call__\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1330, in compile_fx\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return aot_autograd(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 58, in compiler_fn\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 903, in aot_module_simplified\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 119, in aot_dispatch_base\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1257, in fw_compiler_base\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return inner_compile(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 83, in debug_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py\", line 304, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 438, in compile_fx_inner\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1307, in compile_to_fn\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1254, in compile_to_module\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2160, in load_by_key_path\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/tmp/torchinductor_root/sl/csl4rmmbjvtfi3wrjvnruosynk3jzeufrsuhtx3ude3ixsnxl4yi.py\", line 72, in <module>\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2715, in wait\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2522, in result\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     self.future.result()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     return self.__get_result()\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824]     raise self._exception\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] AssertionError: target must be of GPUTarget type\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] \n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "[rank0]:W0610 16:05:43.355000 139982587503744 torch/_dynamo/convert_frame.py:824] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-10 16:05:58 gpu_executor.py:83] # GPU blocks: 13866, # CPU blocks: 2048\n",
      "INFO 06-10 16:05:59 model_runner.py:818] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-10 16:05:59 model_runner.py:822] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-10 16:07:14 model_runner.py:888] Graph capturing finished in 75 secs.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/workspace/models/llama-3-8b-instruct-hqq-dora-plus-plus-qdora-vllm/\"\n",
    "llm = LLM(model=model_dir, tokenizer=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "            dtype=\"bfloat16\", tensor_parallel_size=NUM_GPUS, enforce_eager=False,\n",
    "            quantization=\"torchao\", gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6800342f-3d03-4496-88db-39b09c0984af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 300/300 [00:11<00:00, 26.32it/s, Generation Speed: 26.32 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, SamplingParams(temperature=0.0, max_tokens=1, stop=[\"<|eot_id|>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bccda1e-d385-4a86-9630-0fdbea880cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa3d3be-73e5-44ad-a2d3-de6b19daa2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6033333333333334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.mean([p==a for p,a in zip(preds, eval_ds['output'])])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce19d2a-e41c-4e19-9407-cc9640152f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eccc58-d97a-4ea7-b065-8c0429f18fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3ecce-0578-40bf-b83e-6b364c5ff137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0737f00-7abf-48b1-a991-7ffa9bf641e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502fa69-0fe8-4307-9959-d4d036dd71f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
